<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Federated Inference Resource Scheduling Toolkit - Documentation for deploying and using LLM inference as a service"><meta name=author content="Argonne National Laboratory"><link href=https://auroragpt-anl.github.io/inference-gateway/admin-guide/inference-setup/local-vllm/ rel=canonical><link href=../globus-compute/ rel=prev><link href=../direct-api/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>Local vLLM Setup - FIRST Inference Gateway Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#local-vllm-setup class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="FIRST Inference Gateway Documentation" class="md-header__button md-logo" aria-label="FIRST Inference Gateway Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> FIRST Inference Gateway Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Local vLLM Setup </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/auroraGPT-ANL/inference-gateway title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> auroraGPT-ANL/inference-gateway </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Administrator Guide </a> </li> <li class=md-tabs__item> <a href=../../../user-guide/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../../../reference/citation/ class=md-tabs__link> Reference </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="FIRST Inference Gateway Documentation" class="md-nav__button md-logo" aria-label="FIRST Inference Gateway Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> FIRST Inference Gateway Documentation </label> <div class=md-nav__source> <a href=https://github.com/auroraGPT-ANL/inference-gateway title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> auroraGPT-ANL/inference-gateway </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Administrator Guide </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Administrator Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Gateway Setup </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Gateway Setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../gateway-setup/docker/ class=md-nav__link> <span class=md-ellipsis> Docker Deployment </span> </a> </li> <li class=md-nav__item> <a href=../../gateway-setup/bare-metal/ class=md-nav__link> <span class=md-ellipsis> Bare Metal Setup </span> </a> </li> <li class=md-nav__item> <a href=../../gateway-setup/configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration Reference </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Inference Backend Setup </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=true> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Inference Backend Setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../globus-compute/ class=md-nav__link> <span class=md-ellipsis> Globus Compute + vLLM </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Local vLLM Setup </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Local vLLM Setup </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#architecture class=md-nav__link> <span class=md-ellipsis> Architecture </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#step-1-install-vllm class=md-nav__link> <span class=md-ellipsis> Step 1: Install vLLM </span> </a> <nav class=md-nav aria-label="Step 1: Install vLLM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#option-a-install-from-source-recommended class=md-nav__link> <span class=md-ellipsis> Option A: Install from Source (Recommended) </span> </a> </li> <li class=md-nav__item> <a href=#option-b-install-via-pip class=md-nav__link> <span class=md-ellipsis> Option B: Install via pip </span> </a> </li> <li class=md-nav__item> <a href=#option-c-use-docker class=md-nav__link> <span class=md-ellipsis> Option C: Use Docker </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-2-download-a-model class=md-nav__link> <span class=md-ellipsis> Step 2: Download a Model </span> </a> <nav class=md-nav aria-label="Step 2: Download a Model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#using-hugging-face class=md-nav__link> <span class=md-ellipsis> Using Hugging Face </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-3-start-vllm-server class=md-nav__link> <span class=md-ellipsis> Step 3: Start vLLM Server </span> </a> <nav class=md-nav aria-label="Step 3: Start vLLM Server"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#basic-start class=md-nav__link> <span class=md-ellipsis> Basic Start </span> </a> </li> <li class=md-nav__item> <a href=#production-configuration class=md-nav__link> <span class=md-ellipsis> Production Configuration </span> </a> </li> <li class=md-nav__item> <a href=#multi-gpu-configuration class=md-nav__link> <span class=md-ellipsis> Multi-GPU Configuration </span> </a> </li> <li class=md-nav__item> <a href=#using-docker class=md-nav__link> <span class=md-ellipsis> Using Docker </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-4-test-vllm-server class=md-nav__link> <span class=md-ellipsis> Step 4: Test vLLM Server </span> </a> </li> <li class=md-nav__item> <a href=#step-5-create-systemd-service-optional class=md-nav__link> <span class=md-ellipsis> Step 5: Create systemd Service (Optional) </span> </a> </li> <li class=md-nav__item> <a href=#step-6-configure-gateway class=md-nav__link> <span class=md-ellipsis> Step 6: Configure Gateway </span> </a> <nav class=md-nav aria-label="Step 6: Configure Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#update-gateway-environment class=md-nav__link> <span class=md-ellipsis> Update Gateway Environment </span> </a> </li> <li class=md-nav__item> <a href=#create-endpoint-fixture class=md-nav__link> <span class=md-ellipsis> Create Endpoint Fixture </span> </a> </li> <li class=md-nav__item> <a href=#load-fixture class=md-nav__link> <span class=md-ellipsis> Load Fixture </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-7-test-end-to-end class=md-nav__link> <span class=md-ellipsis> Step 7: Test End-to-End </span> </a> </li> <li class=md-nav__item> <a href=#performance-tuning class=md-nav__link> <span class=md-ellipsis> Performance Tuning </span> </a> <nav class=md-nav aria-label="Performance Tuning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-memory-optimization class=md-nav__link> <span class=md-ellipsis> GPU Memory Optimization </span> </a> </li> <li class=md-nav__item> <a href=#batch-processing class=md-nav__link> <span class=md-ellipsis> Batch Processing </span> </a> </li> <li class=md-nav__item> <a href=#context-length class=md-nav__link> <span class=md-ellipsis> Context Length </span> </a> </li> <li class=md-nav__item> <a href=#quantization class=md-nav__link> <span class=md-ellipsis> Quantization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitoring class=md-nav__link> <span class=md-ellipsis> Monitoring </span> </a> <nav class=md-nav aria-label=Monitoring> <ul class=md-nav__list> <li class=md-nav__item> <a href=#vllm-metrics class=md-nav__link> <span class=md-ellipsis> vLLM Metrics </span> </a> </li> <li class=md-nav__item> <a href=#gpu-monitoring class=md-nav__link> <span class=md-ellipsis> GPU Monitoring </span> </a> </li> <li class=md-nav__item> <a href=#log-monitoring class=md-nav__link> <span class=md-ellipsis> Log Monitoring </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#troubleshooting class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> <nav class=md-nav aria-label=Troubleshooting> <ul class=md-nav__list> <li class=md-nav__item> <a href=#out-of-memory-oom-errors class=md-nav__link> <span class=md-ellipsis> Out of Memory (OOM) Errors </span> </a> </li> <li class=md-nav__item> <a href=#slow-response-times class=md-nav__link> <span class=md-ellipsis> Slow Response Times </span> </a> </li> <li class=md-nav__item> <a href=#model-not-found class=md-nav__link> <span class=md-ellipsis> Model Not Found </span> </a> </li> <li class=md-nav__item> <a href=#cuda-errors class=md-nav__link> <span class=md-ellipsis> CUDA Errors </span> </a> </li> <li class=md-nav__item> <a href=#connection-refused-from-gateway class=md-nav__link> <span class=md-ellipsis> Connection Refused from Gateway </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#running-multiple-models class=md-nav__link> <span class=md-ellipsis> Running Multiple Models </span> </a> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> <li class=md-nav__item> <a href=#additional-resources class=md-nav__link> <span class=md-ellipsis> Additional Resources </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../direct-api/ class=md-nav__link> <span class=md-ellipsis> Direct API Connection </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../deployment/kubernetes/ class=md-nav__link> <span class=md-ellipsis> Kubernetes </span> </a> </li> <li class=md-nav__item> <a href=../../deployment/production/ class=md-nav__link> <span class=md-ellipsis> Production Best Practices </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../monitoring/ class=md-nav__link> <span class=md-ellipsis> Monitoring & Troubleshooting </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../user-guide/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../reference/citation/ class=md-nav__link> <span class=md-ellipsis> Citation </span> </a> </li> <li class=md-nav__item> <a href=../../../reference/api/ class=md-nav__link> <span class=md-ellipsis> API Endpoints </span> </a> </li> <li class=md-nav__item> <a href=../../../reference/config/ class=md-nav__link> <span class=md-ellipsis> Configuration Options </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#architecture class=md-nav__link> <span class=md-ellipsis> Architecture </span> </a> </li> <li class=md-nav__item> <a href=#prerequisites class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#step-1-install-vllm class=md-nav__link> <span class=md-ellipsis> Step 1: Install vLLM </span> </a> <nav class=md-nav aria-label="Step 1: Install vLLM"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#option-a-install-from-source-recommended class=md-nav__link> <span class=md-ellipsis> Option A: Install from Source (Recommended) </span> </a> </li> <li class=md-nav__item> <a href=#option-b-install-via-pip class=md-nav__link> <span class=md-ellipsis> Option B: Install via pip </span> </a> </li> <li class=md-nav__item> <a href=#option-c-use-docker class=md-nav__link> <span class=md-ellipsis> Option C: Use Docker </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-2-download-a-model class=md-nav__link> <span class=md-ellipsis> Step 2: Download a Model </span> </a> <nav class=md-nav aria-label="Step 2: Download a Model"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#using-hugging-face class=md-nav__link> <span class=md-ellipsis> Using Hugging Face </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-3-start-vllm-server class=md-nav__link> <span class=md-ellipsis> Step 3: Start vLLM Server </span> </a> <nav class=md-nav aria-label="Step 3: Start vLLM Server"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#basic-start class=md-nav__link> <span class=md-ellipsis> Basic Start </span> </a> </li> <li class=md-nav__item> <a href=#production-configuration class=md-nav__link> <span class=md-ellipsis> Production Configuration </span> </a> </li> <li class=md-nav__item> <a href=#multi-gpu-configuration class=md-nav__link> <span class=md-ellipsis> Multi-GPU Configuration </span> </a> </li> <li class=md-nav__item> <a href=#using-docker class=md-nav__link> <span class=md-ellipsis> Using Docker </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-4-test-vllm-server class=md-nav__link> <span class=md-ellipsis> Step 4: Test vLLM Server </span> </a> </li> <li class=md-nav__item> <a href=#step-5-create-systemd-service-optional class=md-nav__link> <span class=md-ellipsis> Step 5: Create systemd Service (Optional) </span> </a> </li> <li class=md-nav__item> <a href=#step-6-configure-gateway class=md-nav__link> <span class=md-ellipsis> Step 6: Configure Gateway </span> </a> <nav class=md-nav aria-label="Step 6: Configure Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#update-gateway-environment class=md-nav__link> <span class=md-ellipsis> Update Gateway Environment </span> </a> </li> <li class=md-nav__item> <a href=#create-endpoint-fixture class=md-nav__link> <span class=md-ellipsis> Create Endpoint Fixture </span> </a> </li> <li class=md-nav__item> <a href=#load-fixture class=md-nav__link> <span class=md-ellipsis> Load Fixture </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-7-test-end-to-end class=md-nav__link> <span class=md-ellipsis> Step 7: Test End-to-End </span> </a> </li> <li class=md-nav__item> <a href=#performance-tuning class=md-nav__link> <span class=md-ellipsis> Performance Tuning </span> </a> <nav class=md-nav aria-label="Performance Tuning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-memory-optimization class=md-nav__link> <span class=md-ellipsis> GPU Memory Optimization </span> </a> </li> <li class=md-nav__item> <a href=#batch-processing class=md-nav__link> <span class=md-ellipsis> Batch Processing </span> </a> </li> <li class=md-nav__item> <a href=#context-length class=md-nav__link> <span class=md-ellipsis> Context Length </span> </a> </li> <li class=md-nav__item> <a href=#quantization class=md-nav__link> <span class=md-ellipsis> Quantization </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#monitoring class=md-nav__link> <span class=md-ellipsis> Monitoring </span> </a> <nav class=md-nav aria-label=Monitoring> <ul class=md-nav__list> <li class=md-nav__item> <a href=#vllm-metrics class=md-nav__link> <span class=md-ellipsis> vLLM Metrics </span> </a> </li> <li class=md-nav__item> <a href=#gpu-monitoring class=md-nav__link> <span class=md-ellipsis> GPU Monitoring </span> </a> </li> <li class=md-nav__item> <a href=#log-monitoring class=md-nav__link> <span class=md-ellipsis> Log Monitoring </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#troubleshooting class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> <nav class=md-nav aria-label=Troubleshooting> <ul class=md-nav__list> <li class=md-nav__item> <a href=#out-of-memory-oom-errors class=md-nav__link> <span class=md-ellipsis> Out of Memory (OOM) Errors </span> </a> </li> <li class=md-nav__item> <a href=#slow-response-times class=md-nav__link> <span class=md-ellipsis> Slow Response Times </span> </a> </li> <li class=md-nav__item> <a href=#model-not-found class=md-nav__link> <span class=md-ellipsis> Model Not Found </span> </a> </li> <li class=md-nav__item> <a href=#cuda-errors class=md-nav__link> <span class=md-ellipsis> CUDA Errors </span> </a> </li> <li class=md-nav__item> <a href=#connection-refused-from-gateway class=md-nav__link> <span class=md-ellipsis> Connection Refused from Gateway </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#running-multiple-models class=md-nav__link> <span class=md-ellipsis> Running Multiple Models </span> </a> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> <li class=md-nav__item> <a href=#additional-resources class=md-nav__link> <span class=md-ellipsis> Additional Resources </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=local-vllm-setup>Local vLLM Setup<a class=headerlink href=#local-vllm-setup title="Permanent link">&para;</a></h1> <p>This guide shows you how to run vLLM inference server locally and connect it to the FIRST Gateway <strong>without</strong> Globus Compute.</p> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">&para;</a></h2> <p>This setup is ideal for:</p> <ul> <li>Single-server deployments</li> <li>Development and testing</li> <li>Scenarios where Globus Compute overhead isn't needed</li> <li>Direct control over the inference process</li> </ul> <h2 id=architecture>Architecture<a class=headerlink href=#architecture title="Permanent link">&para;</a></h2> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>graph LR
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>    A[User] --&gt;|Globus Token| B[FIRST Gateway]
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>    B --&gt;|HTTP| C[vLLM Server]
<a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>    C --&gt;|GPU| D[Model]
</code></pre></div> <h2 id=prerequisites>Prerequisites<a class=headerlink href=#prerequisites title="Permanent link">&para;</a></h2> <ul> <li>NVIDIA GPU with sufficient VRAM for your model</li> <li>CUDA 11.8 or later</li> <li>Python 3.12+</li> <li>Docker (optional, for containerized vLLM)</li> </ul> <h2 id=step-1-install-vllm>Step 1: Install vLLM<a class=headerlink href=#step-1-install-vllm title="Permanent link">&para;</a></h2> <h3 id=option-a-install-from-source-recommended>Option A: Install from Source (Recommended)<a class=headerlink href=#option-a-install-from-source-recommended title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># Create virtual environment</span>
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>python3.12<span class=w> </span>-m<span class=w> </span>venv<span class=w> </span>vllm-env
<a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=nb>source</span><span class=w> </span>vllm-env/bin/activate
<a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
<a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=c1># Clone and install vLLM</span>
<a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>git<span class=w> </span>clone<span class=w> </span>https://github.com/vllm-project/vllm.git
<a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=nb>cd</span><span class=w> </span>vllm
<a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>pip<span class=w> </span>install<span class=w> </span>-e<span class=w> </span>.
<a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
<a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=c1># Install additional dependencies</span>
<a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>pip<span class=w> </span>install<span class=w> </span>openai<span class=w>  </span><span class=c1># For testing</span>
</code></pre></div> <h3 id=option-b-install-via-pip>Option B: Install via pip<a class=headerlink href=#option-b-install-via-pip title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>python3.12<span class=w> </span>-m<span class=w> </span>venv<span class=w> </span>vllm-env
<a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=nb>source</span><span class=w> </span>vllm-env/bin/activate
<a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
<a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>pip<span class=w> </span>install<span class=w> </span>vllm
<a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>
<a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=c1># For specific CUDA version</span>
<a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>pip<span class=w> </span>install<span class=w> </span>vllm<span class=w>  </span><span class=c1># CUDA 12.1 by default</span>
<a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=c1># OR</span>
<a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>pip<span class=w> </span>install<span class=w> </span>vllm-cu118<span class=w>  </span><span class=c1># For CUDA 11.8</span>
</code></pre></div> <h3 id=option-c-use-docker>Option C: Use Docker<a class=headerlink href=#option-c-use-docker title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>docker<span class=w> </span>pull<span class=w> </span>vllm/vllm-openai:latest
</code></pre></div> <h2 id=step-2-download-a-model>Step 2: Download a Model<a class=headerlink href=#step-2-download-a-model title="Permanent link">&para;</a></h2> <p>Choose a model based on your GPU VRAM:</p> <table> <thead> <tr> <th>Model</th> <th>VRAM Required</th> <th>Performance</th> </tr> </thead> <tbody> <tr> <td>facebook/opt-125m</td> <td>~1GB</td> <td>Fast, good for testing</td> </tr> <tr> <td>meta-llama/Llama-2-7b-chat-hf</td> <td>~14GB</td> <td>Good quality</td> </tr> <tr> <td>meta-llama/Meta-Llama-3-8B-Instruct</td> <td>~16GB</td> <td>Better quality</td> </tr> <tr> <td>meta-llama/Llama-2-13b-chat-hf</td> <td>~26GB</td> <td>High quality</td> </tr> <tr> <td>meta-llama/Llama-2-70b-chat-hf</td> <td>~140GB</td> <td>Best quality (multi-GPU)</td> </tr> </tbody> </table> <h3 id=using-hugging-face>Using Hugging Face<a class=headerlink href=#using-hugging-face title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Login to Hugging Face (for gated models like Llama)</span>
<a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>pip<span class=w> </span>install<span class=w> </span>huggingface-hub
<a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>huggingface-cli<span class=w> </span>login
<a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>
<a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=c1># Models will auto-download on first use</span>
<a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=c1># Or pre-download:</span>
<a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>huggingface-cli<span class=w> </span>download<span class=w> </span>meta-llama/Meta-Llama-3-8B-Instruct
</code></pre></div> <h2 id=step-3-start-vllm-server>Step 3: Start vLLM Server<a class=headerlink href=#step-3-start-vllm-server title="Permanent link">&para;</a></h2> <h3 id=basic-start>Basic Start<a class=headerlink href=#basic-start title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=nb>source</span><span class=w> </span>vllm-env/bin/activate
<a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
<a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span><span class=se>\</span>
<a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=w>    </span>--model<span class=w> </span>facebook/opt-125m<span class=w> </span><span class=se>\</span>
<a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=w>    </span>--host<span class=w> </span><span class=m>0</span>.0.0.0<span class=w> </span><span class=se>\</span>
<a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=w>    </span>--port<span class=w> </span><span class=m>8001</span>
</code></pre></div> <h3 id=production-configuration>Production Configuration<a class=headerlink href=#production-configuration title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=w>    </span>--model<span class=w> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=w>    </span>--host<span class=w> </span><span class=m>0</span>.0.0.0<span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a><span class=w>    </span>--port<span class=w> </span><span class=m>8001</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a><span class=w>    </span>--tensor-parallel-size<span class=w> </span><span class=m>1</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=w>    </span>--gpu-memory-utilization<span class=w> </span><span class=m>0</span>.9<span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=w>    </span>--max-model-len<span class=w> </span><span class=m>4096</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=w>    </span>--dtype<span class=w> </span>auto<span class=w> </span><span class=se>\</span>
<a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a><span class=w>    </span>--enable-prefix-caching
</code></pre></div> <h3 id=multi-gpu-configuration>Multi-GPU Configuration<a class=headerlink href=#multi-gpu-configuration title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># For 4 GPUs</span>
<a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span><span class=se>\</span>
<a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=w>    </span>--model<span class=w> </span>meta-llama/Llama-2-70b-chat-hf<span class=w> </span><span class=se>\</span>
<a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=w>    </span>--host<span class=w> </span><span class=m>0</span>.0.0.0<span class=w> </span><span class=se>\</span>
<a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=w>    </span>--port<span class=w> </span><span class=m>8001</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a><span class=w>    </span>--tensor-parallel-size<span class=w> </span><span class=m>4</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class=w>    </span>--gpu-memory-utilization<span class=w> </span><span class=m>0</span>.9
</code></pre></div> <h3 id=using-docker>Using Docker<a class=headerlink href=#using-docker title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>docker<span class=w> </span>run<span class=w> </span>--gpus<span class=w> </span>all<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=w>    </span>-v<span class=w> </span>~/.cache/huggingface:/root/.cache/huggingface<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=w>    </span>-p<span class=w> </span><span class=m>8001</span>:8000<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=w>    </span>--env<span class=w> </span><span class=s2>&quot;HUGGING_FACE_HUB_TOKEN=&lt;your_token&gt;&quot;</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=w>    </span>vllm/vllm-openai:latest<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a><span class=w>    </span>--model<span class=w> </span>facebook/opt-125m<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=w>    </span>--host<span class=w> </span><span class=m>0</span>.0.0.0<span class=w> </span><span class=se>\</span>
<a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a><span class=w>    </span>--port<span class=w> </span><span class=m>8000</span>
</code></pre></div> <h2 id=step-4-test-vllm-server>Step 4: Test vLLM Server<a class=headerlink href=#step-4-test-vllm-server title="Permanent link">&para;</a></h2> <div class=highlight><pre><span></span><code><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>curl<span class=w> </span>http://localhost:8001/v1/chat/completions<span class=w> </span><span class=se>\</span>
<a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=w>    </span>-H<span class=w> </span><span class=s2>&quot;Content-Type: application/json&quot;</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a><span class=w>    </span>-d<span class=w> </span><span class=s1>&#39;{</span>
<a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a><span class=s1>        &quot;model&quot;: &quot;facebook/opt-125m&quot;,</span>
<a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a><span class=s1>        &quot;messages&quot;: [</span>
<a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a><span class=s1>            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}</span>
<a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a><span class=s1>        ]</span>
<a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a><span class=s1>    }&#39;</span>
</code></pre></div> <h2 id=step-5-create-systemd-service-optional>Step 5: Create systemd Service (Optional)<a class=headerlink href=#step-5-create-systemd-service-optional title="Permanent link">&para;</a></h2> <p>For production deployments, run vLLM as a system service:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>sudo<span class=w> </span>nano<span class=w> </span>/etc/systemd/system/vllm.service
</code></pre></div> <p>Add:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=k>[Unit]</span>
<a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=na>Description</span><span class=o>=</span><span class=s>vLLM Inference Server</span>
<a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=na>After</span><span class=o>=</span><span class=s>network.target</span>
<a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>
<a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a><span class=k>[Service]</span>
<a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=na>Type</span><span class=o>=</span><span class=s>simple</span>
<a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a><span class=na>User</span><span class=o>=</span><span class=s>your-username</span>
<a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a><span class=na>WorkingDirectory</span><span class=o>=</span><span class=s>/home/your-username</span>
<a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a><span class=na>Environment</span><span class=o>=</span><span class=s>&quot;PATH=/home/your-username/vllm-env/bin&quot;</span>
<a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a><span class=na>ExecStart</span><span class=o>=</span><span class=s>/home/your-username/vllm-env/bin/python -m vllm.entrypoints.openai.api_server </span>\
<a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a><span class=w>    </span><span class=s>--model meta-llama/Meta-Llama-3-8B-Instruct </span>\
<a id=__codelineno-11-12 name=__codelineno-11-12 href=#__codelineno-11-12></a><span class=w>    </span><span class=s>--host 0.0.0.0 </span>\
<a id=__codelineno-11-13 name=__codelineno-11-13 href=#__codelineno-11-13></a><span class=w>    </span><span class=s>--port 8001 </span>\
<a id=__codelineno-11-14 name=__codelineno-11-14 href=#__codelineno-11-14></a><span class=w>    </span><span class=s>--tensor-parallel-size 1 </span>\
<a id=__codelineno-11-15 name=__codelineno-11-15 href=#__codelineno-11-15></a><span class=w>    </span><span class=s>--gpu-memory-utilization 0.9</span>
<a id=__codelineno-11-16 name=__codelineno-11-16 href=#__codelineno-11-16></a><span class=na>Restart</span><span class=o>=</span><span class=s>always</span>
<a id=__codelineno-11-17 name=__codelineno-11-17 href=#__codelineno-11-17></a><span class=na>RestartSec</span><span class=o>=</span><span class=s>10</span>
<a id=__codelineno-11-18 name=__codelineno-11-18 href=#__codelineno-11-18></a>
<a id=__codelineno-11-19 name=__codelineno-11-19 href=#__codelineno-11-19></a><span class=k>[Install]</span>
<a id=__codelineno-11-20 name=__codelineno-11-20 href=#__codelineno-11-20></a><span class=na>WantedBy</span><span class=o>=</span><span class=s>multi-user.target</span>
</code></pre></div> <p>Enable and start:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>sudo<span class=w> </span>systemctl<span class=w> </span>daemon-reload
<a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>sudo<span class=w> </span>systemctl<span class=w> </span><span class=nb>enable</span><span class=w> </span>vllm
<a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>sudo<span class=w> </span>systemctl<span class=w> </span>start<span class=w> </span>vllm
<a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>sudo<span class=w> </span>systemctl<span class=w> </span>status<span class=w> </span>vllm
</code></pre></div> <h2 id=step-6-configure-gateway>Step 6: Configure Gateway<a class=headerlink href=#step-6-configure-gateway title="Permanent link">&para;</a></h2> <h3 id=update-gateway-environment>Update Gateway Environment<a class=headerlink href=#update-gateway-environment title="Permanent link">&para;</a></h3> <p>Edit your gateway's <code>.env</code>:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a># Add if using local vLLM without Globus Compute
<a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a>LOCAL_VLLM_URL=&quot;http://localhost:8001&quot;
</code></pre></div> <h3 id=create-endpoint-fixture>Create Endpoint Fixture<a class=headerlink href=#create-endpoint-fixture title="Permanent link">&para;</a></h3> <p>Create or edit <code>fixtures/endpoints.json</code> in your gateway directory:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=p>[</span>
<a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=w>    </span><span class=p>{</span>
<a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=w>        </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;resource_server.endpoint&quot;</span><span class=p>,</span>
<a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=w>        </span><span class=nt>&quot;pk&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>1</span><span class=p>,</span>
<a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=w>        </span><span class=nt>&quot;fields&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a><span class=w>            </span><span class=nt>&quot;endpoint_slug&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local-vllm-opt-125m&quot;</span><span class=p>,</span>
<a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=w>            </span><span class=nt>&quot;cluster&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local&quot;</span><span class=p>,</span>
<a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a><span class=w>            </span><span class=nt>&quot;framework&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;vllm&quot;</span><span class=p>,</span>
<a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a><span class=w>            </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;facebook/opt-125m&quot;</span><span class=p>,</span>
<a id=__codelineno-14-10 name=__codelineno-14-10 href=#__codelineno-14-10></a><span class=w>            </span><span class=nt>&quot;api_port&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>8001</span><span class=p>,</span>
<a id=__codelineno-14-11 name=__codelineno-14-11 href=#__codelineno-14-11></a><span class=w>            </span><span class=nt>&quot;endpoint_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-14-12 name=__codelineno-14-12 href=#__codelineno-14-12></a><span class=w>            </span><span class=nt>&quot;function_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-14-13 name=__codelineno-14-13 href=#__codelineno-14-13></a><span class=w>            </span><span class=nt>&quot;batch_endpoint_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-14-14 name=__codelineno-14-14 href=#__codelineno-14-14></a><span class=w>            </span><span class=nt>&quot;batch_function_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-14-15 name=__codelineno-14-15 href=#__codelineno-14-15></a><span class=w>            </span><span class=nt>&quot;allowed_globus_groups&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span>
<a id=__codelineno-14-16 name=__codelineno-14-16 href=#__codelineno-14-16></a><span class=w>        </span><span class=p>}</span>
<a id=__codelineno-14-17 name=__codelineno-14-17 href=#__codelineno-14-17></a><span class=w>    </span><span class=p>}</span>
<a id=__codelineno-14-18 name=__codelineno-14-18 href=#__codelineno-14-18></a><span class=p>]</span>
</code></pre></div> <p>For a Llama model:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=p>[</span>
<a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=w>    </span><span class=p>{</span>
<a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=w>        </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;resource_server.endpoint&quot;</span><span class=p>,</span>
<a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a><span class=w>        </span><span class=nt>&quot;pk&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>2</span><span class=p>,</span>
<a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a><span class=w>        </span><span class=nt>&quot;fields&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a><span class=w>            </span><span class=nt>&quot;endpoint_slug&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local-vllm-llama3-8b&quot;</span><span class=p>,</span>
<a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a><span class=w>            </span><span class=nt>&quot;cluster&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local&quot;</span><span class=p>,</span>
<a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a><span class=w>            </span><span class=nt>&quot;framework&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;vllm&quot;</span><span class=p>,</span>
<a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a><span class=w>            </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class=p>,</span>
<a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a><span class=w>            </span><span class=nt>&quot;api_port&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>8001</span><span class=p>,</span>
<a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a><span class=w>            </span><span class=nt>&quot;endpoint_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-15-12 name=__codelineno-15-12 href=#__codelineno-15-12></a><span class=w>            </span><span class=nt>&quot;function_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-15-13 name=__codelineno-15-13 href=#__codelineno-15-13></a><span class=w>            </span><span class=nt>&quot;batch_endpoint_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-15-14 name=__codelineno-15-14 href=#__codelineno-15-14></a><span class=w>            </span><span class=nt>&quot;batch_function_uuid&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
<a id=__codelineno-15-15 name=__codelineno-15-15 href=#__codelineno-15-15></a><span class=w>            </span><span class=nt>&quot;allowed_globus_groups&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span>
<a id=__codelineno-15-16 name=__codelineno-15-16 href=#__codelineno-15-16></a><span class=w>        </span><span class=p>}</span>
<a id=__codelineno-15-17 name=__codelineno-15-17 href=#__codelineno-15-17></a><span class=w>    </span><span class=p>}</span>
<a id=__codelineno-15-18 name=__codelineno-15-18 href=#__codelineno-15-18></a><span class=p>]</span>
</code></pre></div> <h3 id=load-fixture>Load Fixture<a class=headerlink href=#load-fixture title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=c1># Docker</span>
<a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>docker-compose<span class=w> </span><span class=nb>exec</span><span class=w> </span>inference-gateway<span class=w> </span>python<span class=w> </span>manage.py<span class=w> </span>loaddata<span class=w> </span>fixtures/endpoints.json
<a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>
<a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a><span class=c1># Bare metal</span>
<a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>python<span class=w> </span>manage.py<span class=w> </span>loaddata<span class=w> </span>fixtures/endpoints.json
</code></pre></div> <h2 id=step-7-test-end-to-end>Step 7: Test End-to-End<a class=headerlink href=#step-7-test-end-to-end title="Permanent link">&para;</a></h2> <p>Get a Globus token:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=nb>export</span><span class=w> </span><span class=nv>TOKEN</span><span class=o>=</span><span class=k>$(</span>python<span class=w> </span>inference-auth-token.py<span class=w> </span>get_access_token<span class=k>)</span>
</code></pre></div> <p>Test via gateway:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a>curl<span class=w> </span>-X<span class=w> </span>POST<span class=w> </span>http://localhost:8000/resource_server/local/vllm/v1/chat/completions<span class=w> </span><span class=se>\</span>
<a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;Authorization: Bearer </span><span class=nv>$TOKEN</span><span class=s2>&quot;</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;Content-Type: application/json&quot;</span><span class=w> </span><span class=se>\</span>
<a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a><span class=w>  </span>-d<span class=w> </span><span class=s1>&#39;{</span>
<a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a><span class=s1>    &quot;model&quot;: &quot;facebook/opt-125m&quot;,</span>
<a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a><span class=s1>    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain machine learning in one sentence&quot;}],</span>
<a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a><span class=s1>    &quot;max_tokens&quot;: 50</span>
<a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a><span class=s1>  }&#39;</span>
</code></pre></div> <h2 id=performance-tuning>Performance Tuning<a class=headerlink href=#performance-tuning title="Permanent link">&para;</a></h2> <h3 id=gpu-memory-optimization>GPU Memory Optimization<a class=headerlink href=#gpu-memory-optimization title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=c1># Use less GPU memory (if OOM errors)</span>
<a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a>--gpu-memory-utilization<span class=w> </span><span class=m>0</span>.8
<a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>
<a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a><span class=c1># Use more GPU memory (if you have headroom)</span>
<a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a>--gpu-memory-utilization<span class=w> </span><span class=m>0</span>.95
</code></pre></div> <h3 id=batch-processing>Batch Processing<a class=headerlink href=#batch-processing title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=c1># Increase batch size for throughput</span>
<a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a>--max-num-batched-tokens<span class=w> </span><span class=m>8192</span>
<a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>--max-num-seqs<span class=w> </span><span class=m>256</span>
</code></pre></div> <h3 id=context-length>Context Length<a class=headerlink href=#context-length title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=c1># Reduce for better throughput</span>
<a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a>--max-model-len<span class=w> </span><span class=m>2048</span>
<a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>
<a id=__codelineno-21-4 name=__codelineno-21-4 href=#__codelineno-21-4></a><span class=c1># Increase for longer contexts</span>
<a id=__codelineno-21-5 name=__codelineno-21-5 href=#__codelineno-21-5></a>--max-model-len<span class=w> </span><span class=m>8192</span>
</code></pre></div> <h3 id=quantization>Quantization<a class=headerlink href=#quantization title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=c1># Use 4-bit quantization (AWQ)</span>
<a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a>--quantization<span class=w> </span>awq
<a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a>--model<span class=w> </span>TheBloke/Llama-2-7B-Chat-AWQ
<a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a>
<a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a><span class=c1># Use 8-bit quantization (GPTQ)</span>
<a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a>--quantization<span class=w> </span>gptq
<a id=__codelineno-22-7 name=__codelineno-22-7 href=#__codelineno-22-7></a>--model<span class=w> </span>TheBloke/Llama-2-7B-Chat-GPTQ
</code></pre></div> <h2 id=monitoring>Monitoring<a class=headerlink href=#monitoring title="Permanent link">&para;</a></h2> <h3 id=vllm-metrics>vLLM Metrics<a class=headerlink href=#vllm-metrics title="Permanent link">&para;</a></h3> <p>vLLM exposes Prometheus metrics at <code>http://localhost:8001/metrics</code>:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>curl<span class=w> </span>http://localhost:8001/metrics
</code></pre></div> <h3 id=gpu-monitoring>GPU Monitoring<a class=headerlink href=#gpu-monitoring title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-24-1 name=__codelineno-24-1 href=#__codelineno-24-1></a><span class=c1># Watch GPU usage</span>
<a id=__codelineno-24-2 name=__codelineno-24-2 href=#__codelineno-24-2></a>watch<span class=w> </span>-n<span class=w> </span><span class=m>1</span><span class=w> </span>nvidia-smi
<a id=__codelineno-24-3 name=__codelineno-24-3 href=#__codelineno-24-3></a>
<a id=__codelineno-24-4 name=__codelineno-24-4 href=#__codelineno-24-4></a><span class=c1># More detailed stats</span>
<a id=__codelineno-24-5 name=__codelineno-24-5 href=#__codelineno-24-5></a>nvidia-smi<span class=w> </span>dmon
</code></pre></div> <h3 id=log-monitoring>Log Monitoring<a class=headerlink href=#log-monitoring title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-25-1 name=__codelineno-25-1 href=#__codelineno-25-1></a><span class=c1># systemd service logs</span>
<a id=__codelineno-25-2 name=__codelineno-25-2 href=#__codelineno-25-2></a>sudo<span class=w> </span>journalctl<span class=w> </span>-u<span class=w> </span>vllm<span class=w> </span>-f
<a id=__codelineno-25-3 name=__codelineno-25-3 href=#__codelineno-25-3></a>
<a id=__codelineno-25-4 name=__codelineno-25-4 href=#__codelineno-25-4></a><span class=c1># Or direct output if running in terminal</span>
<a id=__codelineno-25-5 name=__codelineno-25-5 href=#__codelineno-25-5></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span>...<span class=w> </span><span class=p>|</span><span class=w> </span>tee<span class=w> </span>vllm.log
</code></pre></div> <h2 id=troubleshooting>Troubleshooting<a class=headerlink href=#troubleshooting title="Permanent link">&para;</a></h2> <h3 id=out-of-memory-oom-errors>Out of Memory (OOM) Errors<a class=headerlink href=#out-of-memory-oom-errors title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-26-1 name=__codelineno-26-1 href=#__codelineno-26-1></a><span class=c1># Reduce GPU memory usage</span>
<a id=__codelineno-26-2 name=__codelineno-26-2 href=#__codelineno-26-2></a>--gpu-memory-utilization<span class=w> </span><span class=m>0</span>.7
<a id=__codelineno-26-3 name=__codelineno-26-3 href=#__codelineno-26-3></a>
<a id=__codelineno-26-4 name=__codelineno-26-4 href=#__codelineno-26-4></a><span class=c1># Reduce context length</span>
<a id=__codelineno-26-5 name=__codelineno-26-5 href=#__codelineno-26-5></a>--max-model-len<span class=w> </span><span class=m>2048</span>
<a id=__codelineno-26-6 name=__codelineno-26-6 href=#__codelineno-26-6></a>
<a id=__codelineno-26-7 name=__codelineno-26-7 href=#__codelineno-26-7></a><span class=c1># Use quantization</span>
<a id=__codelineno-26-8 name=__codelineno-26-8 href=#__codelineno-26-8></a>--quantization<span class=w> </span>awq
<a id=__codelineno-26-9 name=__codelineno-26-9 href=#__codelineno-26-9></a>
<a id=__codelineno-26-10 name=__codelineno-26-10 href=#__codelineno-26-10></a><span class=c1># Use smaller model</span>
</code></pre></div> <h3 id=slow-response-times>Slow Response Times<a class=headerlink href=#slow-response-times title="Permanent link">&para;</a></h3> <ul> <li>Check GPU utilization with <code>nvidia-smi</code></li> <li>Increase <code>--gpu-memory-utilization</code> if GPU memory is underutilized</li> <li>Enable <code>--enable-prefix-caching</code> for repeated prompts</li> <li>Use tensor parallelism for multi-GPU setups</li> </ul> <h3 id=model-not-found>Model Not Found<a class=headerlink href=#model-not-found title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-27-1 name=__codelineno-27-1 href=#__codelineno-27-1></a><span class=c1># Pre-download model</span>
<a id=__codelineno-27-2 name=__codelineno-27-2 href=#__codelineno-27-2></a>huggingface-cli<span class=w> </span>download<span class=w> </span>meta-llama/Meta-Llama-3-8B-Instruct
<a id=__codelineno-27-3 name=__codelineno-27-3 href=#__codelineno-27-3></a>
<a id=__codelineno-27-4 name=__codelineno-27-4 href=#__codelineno-27-4></a><span class=c1># Or set cache directory</span>
<a id=__codelineno-27-5 name=__codelineno-27-5 href=#__codelineno-27-5></a><span class=nb>export</span><span class=w> </span><span class=nv>HF_HOME</span><span class=o>=</span>/path/to/cache
</code></pre></div> <h3 id=cuda-errors>CUDA Errors<a class=headerlink href=#cuda-errors title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><a id=__codelineno-28-1 name=__codelineno-28-1 href=#__codelineno-28-1></a><span class=c1># Check CUDA version</span>
<a id=__codelineno-28-2 name=__codelineno-28-2 href=#__codelineno-28-2></a>nvidia-smi
<a id=__codelineno-28-3 name=__codelineno-28-3 href=#__codelineno-28-3></a>
<a id=__codelineno-28-4 name=__codelineno-28-4 href=#__codelineno-28-4></a><span class=c1># Install matching vLLM version</span>
<a id=__codelineno-28-5 name=__codelineno-28-5 href=#__codelineno-28-5></a>pip<span class=w> </span>install<span class=w> </span>vllm-cu118<span class=w>  </span><span class=c1># For CUDA 11.8</span>
</code></pre></div> <h3 id=connection-refused-from-gateway>Connection Refused from Gateway<a class=headerlink href=#connection-refused-from-gateway title="Permanent link">&para;</a></h3> <ul> <li>Verify vLLM is running: <code>curl http://localhost:8001/health</code></li> <li>Check firewall settings</li> <li>Ensure correct port in fixture configuration</li> <li>Verify host is <code>0.0.0.0</code> not <code>localhost</code></li> </ul> <h2 id=running-multiple-models>Running Multiple Models<a class=headerlink href=#running-multiple-models title="Permanent link">&para;</a></h2> <p>You can run multiple vLLM instances on different ports:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-29-1 name=__codelineno-29-1 href=#__codelineno-29-1></a><span class=c1># Terminal 1 - OPT-125M</span>
<a id=__codelineno-29-2 name=__codelineno-29-2 href=#__codelineno-29-2></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span><span class=se>\</span>
<a id=__codelineno-29-3 name=__codelineno-29-3 href=#__codelineno-29-3></a><span class=w>    </span>--model<span class=w> </span>facebook/opt-125m<span class=w> </span><span class=se>\</span>
<a id=__codelineno-29-4 name=__codelineno-29-4 href=#__codelineno-29-4></a><span class=w>    </span>--port<span class=w> </span><span class=m>8001</span>
<a id=__codelineno-29-5 name=__codelineno-29-5 href=#__codelineno-29-5></a>
<a id=__codelineno-29-6 name=__codelineno-29-6 href=#__codelineno-29-6></a><span class=c1># Terminal 2 - Llama-2-7B</span>
<a id=__codelineno-29-7 name=__codelineno-29-7 href=#__codelineno-29-7></a>python<span class=w> </span>-m<span class=w> </span>vllm.entrypoints.openai.api_server<span class=w> </span><span class=se>\</span>
<a id=__codelineno-29-8 name=__codelineno-29-8 href=#__codelineno-29-8></a><span class=w>    </span>--model<span class=w> </span>meta-llama/Llama-2-7b-chat-hf<span class=w> </span><span class=se>\</span>
<a id=__codelineno-29-9 name=__codelineno-29-9 href=#__codelineno-29-9></a><span class=w>    </span>--port<span class=w> </span><span class=m>8002</span>
</code></pre></div> <p>Add both to your fixtures:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-30-1 name=__codelineno-30-1 href=#__codelineno-30-1></a><span class=p>[</span>
<a id=__codelineno-30-2 name=__codelineno-30-2 href=#__codelineno-30-2></a><span class=w>    </span><span class=p>{</span>
<a id=__codelineno-30-3 name=__codelineno-30-3 href=#__codelineno-30-3></a><span class=w>        </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;resource_server.endpoint&quot;</span><span class=p>,</span>
<a id=__codelineno-30-4 name=__codelineno-30-4 href=#__codelineno-30-4></a><span class=w>        </span><span class=nt>&quot;pk&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>1</span><span class=p>,</span>
<a id=__codelineno-30-5 name=__codelineno-30-5 href=#__codelineno-30-5></a><span class=w>        </span><span class=nt>&quot;fields&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<a id=__codelineno-30-6 name=__codelineno-30-6 href=#__codelineno-30-6></a><span class=w>            </span><span class=nt>&quot;endpoint_slug&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local-vllm-opt-125m&quot;</span><span class=p>,</span>
<a id=__codelineno-30-7 name=__codelineno-30-7 href=#__codelineno-30-7></a><span class=w>            </span><span class=nt>&quot;cluster&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local&quot;</span><span class=p>,</span>
<a id=__codelineno-30-8 name=__codelineno-30-8 href=#__codelineno-30-8></a><span class=w>            </span><span class=nt>&quot;framework&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;vllm&quot;</span><span class=p>,</span>
<a id=__codelineno-30-9 name=__codelineno-30-9 href=#__codelineno-30-9></a><span class=w>            </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;facebook/opt-125m&quot;</span><span class=p>,</span>
<a id=__codelineno-30-10 name=__codelineno-30-10 href=#__codelineno-30-10></a><span class=w>            </span><span class=nt>&quot;api_port&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>8001</span><span class=p>,</span>
<a id=__codelineno-30-11 name=__codelineno-30-11 href=#__codelineno-30-11></a><span class=w>            </span><span class=err>...</span>
<a id=__codelineno-30-12 name=__codelineno-30-12 href=#__codelineno-30-12></a><span class=w>        </span><span class=p>}</span>
<a id=__codelineno-30-13 name=__codelineno-30-13 href=#__codelineno-30-13></a><span class=w>    </span><span class=p>},</span>
<a id=__codelineno-30-14 name=__codelineno-30-14 href=#__codelineno-30-14></a><span class=w>    </span><span class=p>{</span>
<a id=__codelineno-30-15 name=__codelineno-30-15 href=#__codelineno-30-15></a><span class=w>        </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;resource_server.endpoint&quot;</span><span class=p>,</span>
<a id=__codelineno-30-16 name=__codelineno-30-16 href=#__codelineno-30-16></a><span class=w>        </span><span class=nt>&quot;pk&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>2</span><span class=p>,</span>
<a id=__codelineno-30-17 name=__codelineno-30-17 href=#__codelineno-30-17></a><span class=w>        </span><span class=nt>&quot;fields&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
<a id=__codelineno-30-18 name=__codelineno-30-18 href=#__codelineno-30-18></a><span class=w>            </span><span class=nt>&quot;endpoint_slug&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local-vllm-llama2-7b&quot;</span><span class=p>,</span>
<a id=__codelineno-30-19 name=__codelineno-30-19 href=#__codelineno-30-19></a><span class=w>            </span><span class=nt>&quot;cluster&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;local&quot;</span><span class=p>,</span>
<a id=__codelineno-30-20 name=__codelineno-30-20 href=#__codelineno-30-20></a><span class=w>            </span><span class=nt>&quot;framework&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;vllm&quot;</span><span class=p>,</span>
<a id=__codelineno-30-21 name=__codelineno-30-21 href=#__codelineno-30-21></a><span class=w>            </span><span class=nt>&quot;model&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class=p>,</span>
<a id=__codelineno-30-22 name=__codelineno-30-22 href=#__codelineno-30-22></a><span class=w>            </span><span class=nt>&quot;api_port&quot;</span><span class=p>:</span><span class=w> </span><span class=mi>8002</span><span class=p>,</span>
<a id=__codelineno-30-23 name=__codelineno-30-23 href=#__codelineno-30-23></a><span class=w>            </span><span class=err>...</span>
<a id=__codelineno-30-24 name=__codelineno-30-24 href=#__codelineno-30-24></a><span class=w>        </span><span class=p>}</span>
<a id=__codelineno-30-25 name=__codelineno-30-25 href=#__codelineno-30-25></a><span class=w>    </span><span class=p>}</span>
<a id=__codelineno-30-26 name=__codelineno-30-26 href=#__codelineno-30-26></a><span class=p>]</span>
</code></pre></div> <h2 id=next-steps>Next Steps<a class=headerlink href=#next-steps title="Permanent link">&para;</a></h2> <ul> <li><a href=../../deployment/production/ >Production Best Practices</a></li> <li><a href=../../monitoring/ >Monitoring Setup</a></li> <li><a href=../../../user-guide/ >User Guide</a></li> <li>Upgrade to <a href=../globus-compute/ >Globus Compute + vLLM</a> for federated deployment</li> </ul> <h2 id=additional-resources>Additional Resources<a class=headerlink href=#additional-resources title="Permanent link">&para;</a></h2> <ul> <li><a href=https://docs.vllm.ai/ >vLLM Documentation</a></li> <li><a href=https://huggingface.co/models>Hugging Face Models</a></li> <li><a href=https://docs.nvidia.com/cuda/ >NVIDIA GPU Documentation</a></li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 Argonne National Laboratory - Apache 2.0 License </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/auroraGPT-ANL/inference-gateway target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://doi.org/10.1145/3731599.3767346 target=_blank rel=noopener title=doi.org class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "navigation.tracking", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.e71a0d61.min.js></script> </body> </html>