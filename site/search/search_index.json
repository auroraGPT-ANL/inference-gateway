{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"FIRST Inference Gateway Documentation","text":"<p>Welcome to the documentation for the Federated Inference Resource Scheduling Toolkit (FIRST). FIRST enables LLM inference as a service across distributed HPC clusters through an OpenAI-compatible API.</p>"},{"location":"#what-is-first","title":"What is FIRST?","text":"<p>FIRST (Federated Inference Resource Scheduling Toolkit) is a system that allows secure, remote execution of Large Language Models through an OpenAI-compatible API. It validates and authorizes inference requests to scientific computing clusters using Globus Auth and Globus Compute.</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>The Inference Gateway consists of several components:</p> <ul> <li>API Gateway: Django-based REST/Ninja API that handles authorization and request routing</li> <li>Globus Auth: Authentication and authorization service</li> <li>Globus Compute Endpoints: Remote execution framework on HPC clusters (or local machines)</li> <li>Inference Server Backend: High-performance inference service for LLMs (e.g., vLLM)</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#for-administrators","title":"For Administrators","text":"<ul> <li>Docker Deployment - Fast-track Docker deployment in under 10 minutes</li> <li>Bare Metal Setup - Complete installation on your own infrastructure</li> <li>Inference Backend - Connect to OpenAI API, local vLLM, or Globus Compute</li> <li>Kubernetes - Deploy on Kubernetes clusters (Coming Soon)</li> </ul>"},{"location":"#for-users","title":"For Users","text":"<ul> <li>User Guide - Complete guide for authentication and making requests</li> <li>API Reference - API endpoint documentation</li> <li>Examples - Code examples and tutorials</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Federated Access: Route requests across multiple HPC clusters automatically</li> <li>OpenAI-Compatible: Works with existing OpenAI SDK and tools</li> <li>Secure: Globus Auth integration with group-based access control</li> <li>High Performance: Support for vLLM and other optimized inference backends</li> <li>Flexible: Deploy via Docker, bare metal, or Kubernetes</li> <li>Scalable: Auto-scaling and resource management for HPC environments</li> </ul>"},{"location":"#example-deployment","title":"Example Deployment","text":"<p>For a production example, see the ALCF Inference Endpoints documentation.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>GitHub: Report issues or contribute</li> <li>Citation: Research Paper</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p> <p>Quick Start Paths</p> <ul> <li>Just want to try it out? \u2192 Docker Quickstart</li> <li>Need full control? \u2192 Bare Metal Setup</li> <li>Want to use the API? \u2192 User Guide</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to Documentation","text":"<p>This guide explains how to contribute to the FIRST Inference Gateway documentation.</p>"},{"location":"CONTRIBUTING/#documentation-structure","title":"Documentation Structure","text":"<p>The documentation is built using MkDocs with the Material theme.</p> <pre><code>docs/\n\u251c\u2500\u2500 index.md                    # Homepage\n\u251c\u2500\u2500 admin-guide/                # Administrator documentation\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 gateway-setup/\n\u2502   \u2502   \u251c\u2500\u2500 docker.md\n\u2502   \u2502   \u251c\u2500\u2500 bare-metal.md\n\u2502   \u2502   \u2514\u2500\u2500 configuration.md\n\u2502   \u251c\u2500\u2500 inference-setup/\n\u2502   \u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2502   \u251c\u2500\u2500 direct-api.md\n\u2502   \u2502   \u251c\u2500\u2500 local-vllm.md\n\u2502   \u2502   \u2514\u2500\u2500 globus-compute.md\n\u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u251c\u2500\u2500 kubernetes.md\n\u2502   \u2502   \u2514\u2500\u2500 production.md\n\u2502   \u2514\u2500\u2500 monitoring.md\n\u251c\u2500\u2500 user-guide/                 # User documentation\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 authentication.md\n\u2502   \u251c\u2500\u2500 requests.md\n\u2502   \u251c\u2500\u2500 batch.md\n\u2502   \u2514\u2500\u2500 examples.md\n\u2514\u2500\u2500 reference/                  # Reference materials\n    \u251c\u2500\u2500 citation.md\n    \u251c\u2500\u2500 api.md\n    \u2514\u2500\u2500 config.md\n</code></pre>"},{"location":"CONTRIBUTING/#local-development","title":"Local Development","text":""},{"location":"CONTRIBUTING/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install -r requirements-docs.txt\n</code></pre> <p>Or:</p> <pre><code>pip install mkdocs-material mkdocs-minify-plugin\n</code></pre>"},{"location":"CONTRIBUTING/#build-documentation","title":"Build Documentation","text":"<pre><code>mkdocs build\n</code></pre> <p>This creates the <code>site/</code> directory with static HTML files.</p>"},{"location":"CONTRIBUTING/#serve-locally","title":"Serve Locally","text":"<pre><code>mkdocs serve\n</code></pre> <p>Then visit: http://127.0.0.1:8000</p> <p>The site will automatically reload when you save changes.</p>"},{"location":"CONTRIBUTING/#writing-documentation","title":"Writing Documentation","text":""},{"location":"CONTRIBUTING/#markdown-files","title":"Markdown Files","text":"<p>All documentation is written in Markdown with support for:</p> <ul> <li>Standard Markdown syntax</li> <li>Material for MkDocs extensions</li> <li>Code syntax highlighting</li> <li>Admonitions (notes, warnings, tips)</li> <li>Tables</li> <li>Mermaid diagrams</li> </ul>"},{"location":"CONTRIBUTING/#admonitions","title":"Admonitions","text":"<pre><code>!!! note \"Optional Title\"\n    This is a note\n\n!!! warning\n    This is a warning\n\n!!! tip\n    This is a tip\n\n!!! danger\n    This is dangerous!\n</code></pre>"},{"location":"CONTRIBUTING/#code-blocks","title":"Code Blocks","text":"<pre><code>```python\ndef hello():\n    print(\"Hello, World!\")\n```\n\n```bash\nmkdocs serve\n```\n</code></pre>"},{"location":"CONTRIBUTING/#mermaid-diagrams","title":"Mermaid Diagrams","text":"<pre><code>```mermaid\ngraph LR\n    A[User] --&gt; B[Gateway]\n    B --&gt; C[Backend]\n```\n</code></pre>"},{"location":"CONTRIBUTING/#internal-links","title":"Internal Links","text":"<pre><code>[Link text](path/to/file.md)\n[Link to section](path/to/file.md#section-name)\n</code></pre>"},{"location":"CONTRIBUTING/#navigation","title":"Navigation","text":"<p>Navigation is configured in <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - Administrator Guide:\n      - Overview: admin-guide/index.md\n      - Gateway Setup:\n          - Docker: admin-guide/gateway-setup/docker.md\n</code></pre>"},{"location":"CONTRIBUTING/#deployment","title":"Deployment","text":""},{"location":"CONTRIBUTING/#automatic-deployment","title":"Automatic Deployment","text":"<p>Documentation automatically deploys to GitHub Pages when you push to <code>main</code>:</p> <ol> <li>Make your changes to files in <code>docs/</code></li> <li>Commit and push:    <pre><code>git add docs/\ngit commit -m \"docs: update documentation\"\ngit push origin main\n</code></pre></li> <li>GitHub Actions builds and deploys automatically</li> <li>View at: https://auroragpt-anl.github.io/inference-gateway/</li> </ol>"},{"location":"CONTRIBUTING/#manual-deployment","title":"Manual Deployment","text":"<p>You can also manually trigger deployment:</p> <ol> <li>Go to Actions tab on GitHub</li> <li>Select \"Deploy Documentation\" workflow</li> <li>Click \"Run workflow\"</li> </ol>"},{"location":"CONTRIBUTING/#style-guide","title":"Style Guide","text":""},{"location":"CONTRIBUTING/#headings","title":"Headings","text":"<ul> <li>Use sentence case for headings</li> <li>One H1 (<code>#</code>) per page (the page title)</li> <li>Use hierarchical heading levels (don't skip levels)</li> </ul>"},{"location":"CONTRIBUTING/#code-examples","title":"Code Examples","text":"<ul> <li>Always include language identifier for syntax highlighting</li> <li>Add comments to explain complex code</li> <li>Test code examples before committing</li> </ul>"},{"location":"CONTRIBUTING/#file-names","title":"File Names","text":"<ul> <li>Use lowercase with hyphens: <code>my-file.md</code></li> <li>Be descriptive: <code>docker-deployment.md</code> not <code>docker.md</code></li> </ul>"},{"location":"CONTRIBUTING/#writing-style","title":"Writing Style","text":"<ul> <li>Use clear, concise language</li> <li>Write in second person (\"you\") for instructions</li> <li>Use active voice</li> <li>Include examples wherever possible</li> <li>Add context before diving into details</li> </ul>"},{"location":"CONTRIBUTING/#contributing-process","title":"Contributing Process","text":"<ol> <li>Fork the repository</li> <li>Create a branch: <code>git checkout -b docs/my-improvement</code></li> <li>Make your changes</li> <li>Test locally: <code>mkdocs serve</code></li> <li>Commit with clear message: <code>git commit -m \"docs: improve docker guide\"</code></li> <li>Push and create a Pull Request</li> </ol>"},{"location":"CONTRIBUTING/#questions","title":"Questions?","text":"<ul> <li>Open an issue on GitHub</li> <li>Check existing documentation</li> <li>Review MkDocs Material documentation</li> </ul>"},{"location":"CONTRIBUTING/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Markdown Guide</li> </ul>"},{"location":"admin-guide/","title":"Administrator Guide","text":"<p>Welcome to the FIRST Inference Gateway Administrator Guide. This guide will help you deploy and configure the gateway for your organization.</p>"},{"location":"admin-guide/#overview","title":"Overview","text":"<p>Setting up FIRST involves two main components:</p> <ol> <li>Gateway Setup: The central API service that handles authentication and routing</li> <li>Inference Backend Setup: The actual inference servers where models run</li> </ol> <p>These can be deployed independently and connected together through configuration.</p>"},{"location":"admin-guide/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>[x] Python 3.12 or later</li> <li>[x] Docker and Docker Compose (for Docker deployment)</li> <li>[x] PostgreSQL Server (or use Docker)</li> <li>[x] Globus Account</li> <li>[x] Access to compute resources (for inference backends)</li> </ul>"},{"location":"admin-guide/#deployment-architecture","title":"Deployment Architecture","text":"<p>Choose your deployment approach:</p>"},{"location":"admin-guide/#gateway-deployment-options","title":"Gateway Deployment Options","text":"<p>Docker Deployment (Recommended)</p> <ul> <li>Quick setup with Docker Compose</li> <li>Pros: Easy to deploy, includes all dependencies, portable</li> <li>Cons: Requires Docker knowledge</li> <li>Docker Guide \u2192</li> </ul> <p>Bare Metal Deployment</p> <ul> <li>Direct installation on your server infrastructure</li> <li>Pros: More control, better performance, easier debugging</li> <li>Cons: Manual dependency management</li> <li>Bare Metal Guide \u2192</li> </ul>"},{"location":"admin-guide/#inference-backend-options","title":"Inference Backend Options","text":"<p>Globus Compute + vLLM (Recommended for Production)</p> <ul> <li>Deploy vLLM on HPC clusters with Globus Compute for remote execution</li> <li>Best for: Multi-cluster, federated deployments, HPC environments</li> <li>Globus Compute Setup \u2192</li> </ul> <p>Local vLLM</p> <ul> <li>Run vLLM inference server locally without Globus Compute</li> <li>Best for: Single-node deployments, development</li> <li>Local vLLM Setup \u2192</li> </ul> <p>Direct API Connection</p> <ul> <li>Connect to existing OpenAI-compatible APIs (OpenAI, Anthropic, etc.)</li> <li>Best for: Simple setup, using commercial APIs</li> <li>Direct API Setup \u2192</li> </ul>"},{"location":"admin-guide/#setup-workflow","title":"Setup Workflow","text":""},{"location":"admin-guide/#phase-1-gateway-setup","title":"Phase 1: Gateway Setup","text":"<ol> <li>Choose your deployment method (Docker or Bare Metal)</li> <li>Register Globus applications</li> <li>Configure environment variables</li> <li>Initialize the database</li> <li>Start the gateway service</li> </ol>"},{"location":"admin-guide/#phase-2-inference-backend-setup","title":"Phase 2: Inference Backend Setup","text":"<ol> <li>Choose your backend type</li> <li>Install required software (vLLM, Globus Compute, etc.)</li> <li>Configure the backend</li> <li>Register endpoints/functions</li> <li>Test the connection</li> </ol>"},{"location":"admin-guide/#phase-3-connect-gateway-and-backend","title":"Phase 3: Connect Gateway and Backend","text":"<ol> <li>Update fixture files with backend details</li> <li>Load fixtures into the gateway database</li> <li>Verify end-to-end functionality</li> </ol>"},{"location":"admin-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"admin-guide/#pattern-1-quick-local-development","title":"Pattern 1: Quick Local Development","text":"<pre><code>graph LR\n    A[Docker Gateway] --&gt; B[Local vLLM]\n    B --&gt; C[Small Model&lt;br/&gt;OPT-125M]\n</code></pre> <p>Use: Development and testing</p> <p>Setup Time: ~15 minutes</p> <p>Resources: 1 GPU or CPU</p>"},{"location":"admin-guide/#pattern-2-production-single-cluster","title":"Pattern 2: Production Single Cluster","text":"<pre><code>graph LR\n    A[Bare Metal Gateway] --&gt; B[Globus Compute]\n    B --&gt; C[HPC Cluster]\n    C --&gt; D[Multiple Models]\n</code></pre> <p>Use: Production deployment on single HPC cluster</p> <p>Setup Time: ~2 hours</p> <p>Resources: HPC cluster access</p>"},{"location":"admin-guide/#pattern-3-federated-multi-cluster","title":"Pattern 3: Federated Multi-Cluster","text":"<pre><code>graph LR\n    A[Gateway] --&gt; B[Cluster 1&lt;br/&gt;Globus Compute]\n    A --&gt; C[Cluster 2&lt;br/&gt;Globus Compute]\n    A --&gt; D[Cluster 3&lt;br/&gt;Globus Compute]\n</code></pre> <p>Use: Maximum availability and resource pooling</p> <p>Setup Time: ~4 hours</p> <p>Resources: Multiple HPC clusters</p>"},{"location":"admin-guide/#next-steps","title":"Next Steps","text":"<p>Ready to get started? Choose your path:</p> <ul> <li>Quick Start: Docker Deployment</li> <li>Full Setup: Bare Metal Deployment</li> <li>Backend Setup: Inference Backend Overview</li> </ul>"},{"location":"admin-guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Reference</li> <li>Production Best Practices</li> <li>Monitoring &amp; Troubleshooting</li> <li>Kubernetes Deployment (Coming Soon)</li> </ul>"},{"location":"admin-guide/monitoring/","title":"Monitoring &amp; Troubleshooting","text":"<p>This guide covers monitoring the FIRST Inference Gateway and troubleshooting common issues.</p>"},{"location":"admin-guide/monitoring/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/monitoring/#application-logs","title":"Application Logs","text":""},{"location":"admin-guide/monitoring/#docker-deployment","title":"Docker Deployment","text":"<pre><code># View all logs\ndocker-compose logs -f\n\n# View gateway logs only\ndocker-compose logs -f inference-gateway\n\n# Last 100 lines\ndocker-compose logs --tail=100 inference-gateway\n</code></pre>"},{"location":"admin-guide/monitoring/#bare-metal-deployment","title":"Bare Metal Deployment","text":"<pre><code># Application logs\ntail -f logs/django_info.log\n\n# Gunicorn logs\ntail -f logs/backend_gateway.error.log\ntail -f logs/backend_gateway.access.log\n\n# Systemd service logs\nsudo journalctl -u inference-gateway -f\n</code></pre>"},{"location":"admin-guide/monitoring/#database-monitoring","title":"Database Monitoring","text":"<pre><code># Connection stats\npsql -h localhost -U inferencedev -d inferencegateway -c \"SELECT * FROM pg_stat_activity;\"\n\n# Table sizes\npsql -h localhost -U inferencedev -d inferencegateway -c \"\nSELECT schemaname,tablename,pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename))\nFROM pg_tables WHERE schemaname='public' ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\"\n</code></pre>"},{"location":"admin-guide/monitoring/#redis-monitoring","title":"Redis Monitoring","text":"<pre><code># Connect to Redis CLI\nredis-cli\n\n# In Redis CLI:\nINFO\nDBSIZE\nMONITOR  # Watch commands in real-time\n</code></pre>"},{"location":"admin-guide/monitoring/#globus-compute-endpoints","title":"Globus Compute Endpoints","text":"<pre><code># List endpoints\nglobus-compute-endpoint list\n\n# Check status\nglobus-compute-endpoint status my-endpoint\n\n# View logs\nglobus-compute-endpoint log my-endpoint -n 100\n\n# Follow logs\ntail -f ~/.globus_compute/my-endpoint/endpoint.log\n</code></pre>"},{"location":"admin-guide/monitoring/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/monitoring/#gateway-wont-start","title":"Gateway Won't Start","text":"<p>Symptoms: Container/service fails to start</p> <p>Check:</p> <pre><code># Docker\ndocker-compose logs inference-gateway\n\n# Bare metal\nsudo journalctl -u inference-gateway -n 50\npython manage.py check\n</code></pre> <p>Common Causes:</p> <ul> <li>Missing environment variables</li> <li>Database connection failure</li> <li>Port already in use</li> <li>Syntax error in settings</li> </ul>"},{"location":"admin-guide/monitoring/#database-connection-errors","title":"Database Connection Errors","text":"<p>Symptoms: <code>OperationalError: could not connect to server</code></p> <p>Solutions:</p> <pre><code># Verify PostgreSQL is running\nsudo systemctl status postgresql\ndocker-compose ps postgres\n\n# Test connection\npsql -h localhost -U inferencedev -d inferencegateway\n\n# Check pg_hba.conf\nsudo nano /etc/postgresql/*/main/pg_hba.conf\n\n# Restart PostgreSQL\nsudo systemctl restart postgresql\n</code></pre>"},{"location":"admin-guide/monitoring/#authentication-failures","title":"Authentication Failures","text":"<p>Symptoms: 401 Unauthorized, Globus token errors</p> <p>Solutions:</p> <ol> <li>Verify Globus application credentials in <code>.env</code></li> <li>Check scope was created successfully:    <pre><code>curl -s --user $CLIENT_ID:$CLIENT_SECRET \\\n    https://auth.globus.org/v2/api/clients/$CLIENT_ID\n</code></pre></li> <li>Force re-authentication:    <pre><code>python inference-auth-token.py authenticate --force\n</code></pre></li> <li>Verify redirect URIs match in Globus app settings</li> </ol>"},{"location":"admin-guide/monitoring/#globus-compute-errors","title":"Globus Compute Errors","text":"<p>Symptoms: Function execution failures, timeout errors</p> <p>Solutions:</p> <pre><code># Check endpoint is running\nglobus-compute-endpoint list\n\n# Restart endpoint\nglobus-compute-endpoint restart my-endpoint\n\n# View detailed logs\nglobus-compute-endpoint log my-endpoint -n 200\n\n# Verify function UUID is allowed\ncat ~/.globus_compute/my-endpoint/config.yaml\n</code></pre>"},{"location":"admin-guide/monitoring/#model-not-found","title":"Model Not Found","text":"<p>Symptoms: <code>Model 'xxx' not found</code> errors</p> <p>Solutions:</p> <ol> <li>Verify fixture was loaded:    <pre><code>python manage.py dumpdata resource_server.endpoint\n</code></pre></li> <li>Check model name matches exactly in fixture</li> <li>Reload fixtures:    <pre><code>python manage.py loaddata fixtures/endpoints.json\n</code></pre></li> </ol>"},{"location":"admin-guide/monitoring/#slow-response-times","title":"Slow Response Times","text":"<p>Causes:</p> <ul> <li>Cold start (first request to endpoint)</li> <li>GPU not available</li> <li>Model loading time</li> <li>Network latency</li> </ul> <p>Solutions:</p> <ol> <li>Enable hot nodes (min_blocks &gt; 0 in Globus Compute config)</li> <li>Monitor GPU usage: <code>nvidia-smi</code></li> <li>Check vLLM logs for bottlenecks</li> <li>Increase Gunicorn timeout:    <pre><code>timeout = 300\n</code></pre></li> </ol>"},{"location":"admin-guide/monitoring/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Symptoms: OOM kills, CUDA out of memory</p> <p>Solutions:</p> <pre><code># vLLM: Reduce GPU memory usage\n--gpu-memory-utilization 0.7\n\n# vLLM: Use quantization\n--quantization awq\n\n# vLLM: Reduce context length\n--max-model-len 2048\n\n# System: Add swap space\nsudo fallocate -l 32G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre>"},{"location":"admin-guide/monitoring/#health-checks","title":"Health Checks","text":""},{"location":"admin-guide/monitoring/#manual-health-checks","title":"Manual Health Checks","text":"<pre><code># Gateway health\ncurl http://localhost:8000/\n\n# vLLM health\ncurl http://localhost:8001/health\n\n# Database connectivity\npython manage.py dbshell\n\n# Redis connectivity\nredis-cli ping\n</code></pre>"},{"location":"admin-guide/monitoring/#automated-health-monitoring","title":"Automated Health Monitoring","text":"<p>Create a health check script:</p> <pre><code>#!/bin/bash\n# health_check.sh\n\n# Check gateway\nif curl -s http://localhost:8000/ &gt; /dev/null; then\n    echo \"\u2713 Gateway is healthy\"\nelse\n    echo \"\u2717 Gateway is down\"\n    systemctl restart inference-gateway\nfi\n\n# Check database\nif psql -h localhost -U inferencedev -d inferencegateway -c \"SELECT 1;\" &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u2713 Database is healthy\"\nelse\n    echo \"\u2717 Database is down\"\nfi\n</code></pre> <p>Add to crontab:</p> <pre><code>*/5 * * * * /path/to/health_check.sh &gt;&gt; /var/log/health_check.log 2&gt;&amp;1\n</code></pre>"},{"location":"admin-guide/monitoring/#performance-metrics","title":"Performance Metrics","text":""},{"location":"admin-guide/monitoring/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ol> <li>Request Rate: Requests per second</li> <li>Latency: Response time (p50, p95, p99)</li> <li>Error Rate: Percentage of failed requests</li> <li>Queue Depth: Pending Globus Compute tasks</li> <li>GPU Utilization: GPU memory and compute usage</li> <li>Database Connections: Active connections</li> <li>Cache Hit Rate: Redis cache effectiveness</li> </ol>"},{"location":"admin-guide/monitoring/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>If using Prometheus, key metrics to track:</p> <pre><code># Request metrics\nhttp_requests_total\nhttp_request_duration_seconds\n\n# Globus Compute metrics\nglobus_compute_tasks_submitted\nglobus_compute_tasks_completed\nglobus_compute_tasks_failed\n\n# System metrics\nprocess_cpu_seconds_total\nprocess_resident_memory_bytes\n</code></pre>"},{"location":"admin-guide/monitoring/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<p>When issues occur, work through this checklist:</p> <ul> <li>[ ] Check application logs</li> <li>[ ] Verify all services are running</li> <li>[ ] Test database connectivity</li> <li>[ ] Check Redis connectivity</li> <li>[ ] Verify Globus Compute endpoints are online</li> <li>[ ] Test authentication flow</li> <li>[ ] Check network connectivity</li> <li>[ ] Review recent configuration changes</li> <li>[ ] Check disk space</li> <li>[ ] Monitor resource usage (CPU, RAM, GPU)</li> </ul>"},{"location":"admin-guide/monitoring/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li>Check documentation: Review the relevant setup guides</li> <li>Search issues: Look for similar issues on GitHub</li> <li>Enable debug logging: Set <code>DEBUG=True</code> temporarily</li> <li>Collect information:</li> <li>Version information</li> <li>Error messages and stack traces</li> <li>Configuration (sanitize secrets!)</li> <li>Relevant log excerpts</li> <li>Open an issue: Provide all collected information</li> </ol>"},{"location":"admin-guide/monitoring/#additional-resources","title":"Additional Resources","text":"<ul> <li>Production Best Practices</li> <li>Configuration Reference</li> <li>Globus Compute Documentation</li> </ul>"},{"location":"admin-guide/deployment/kubernetes/","title":"Kubernetes Deployment","text":"<p>Coming Soon</p> <p>Kubernetes deployment manifests and Helm charts are currently under development.</p>"},{"location":"admin-guide/deployment/kubernetes/#planned-features","title":"Planned Features","text":"<p>The Kubernetes deployment will include:</p> <ul> <li>Helm Chart: Easy installation and configuration management</li> <li>High Availability: Multi-replica deployments with load balancing</li> <li>Auto-Scaling: Horizontal Pod Autoscaler based on metrics</li> <li>StatefulSets: For PostgreSQL and Redis persistence</li> <li>Ingress Configuration: HTTPS/TLS termination and routing</li> <li>Secrets Management: Kubernetes secrets for sensitive data</li> <li>ConfigMaps: Environment-specific configuration</li> <li>Health Probes: Liveness and readiness checks</li> <li>Resource Limits: CPU and memory management</li> <li>Monitoring Integration: Prometheus and Grafana</li> </ul>"},{"location":"admin-guide/deployment/kubernetes/#current-status","title":"Current Status","text":"<p>We are actively working on:</p> <ol> <li>Creating Kubernetes manifests for all components</li> <li>Developing a Helm chart for simplified deployment</li> <li>Testing on various Kubernetes distributions (EKS, GKE, OpenShift)</li> <li>Documentation and best practices</li> </ol>"},{"location":"admin-guide/deployment/kubernetes/#alternative-docker-deployment","title":"Alternative: Docker Deployment","text":"<p>For now, please use one of these deployment methods:</p> <ul> <li>Docker Deployment - Containerized deployment with Docker Compose</li> <li>Bare Metal Setup - Direct installation on servers</li> </ul>"},{"location":"admin-guide/deployment/kubernetes/#get-notified","title":"Get Notified","text":"<p>To be notified when Kubernetes support is available:</p> <ul> <li>:star: Star the GitHub repository</li> <li>Watch the repository for releases</li> <li>Check the releases page</li> </ul>"},{"location":"admin-guide/deployment/kubernetes/#contribute","title":"Contribute","text":"<p>Interested in helping with Kubernetes deployment?</p> <ul> <li>Check open issues tagged with <code>kubernetes</code></li> <li>Submit a pull request</li> <li>Share your deployment configurations</li> </ul>"},{"location":"admin-guide/deployment/kubernetes/#contact","title":"Contact","text":"<p>For enterprise Kubernetes deployments or consulting:</p> <ul> <li>Open an issue on GitHub</li> <li>Contact the development team</li> </ul> <p>Last Updated: November 2025</p> <p>Check back soon for updates!</p>"},{"location":"admin-guide/deployment/production/","title":"Production Best Practices","text":"<p>This guide covers best practices for deploying FIRST Inference Gateway in production environments.</p>"},{"location":"admin-guide/deployment/production/#security","title":"Security","text":""},{"location":"admin-guide/deployment/production/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"admin-guide/deployment/production/#globus-group-restrictions","title":"Globus Group Restrictions","text":"<p>Restrict access to specific Globus groups:</p> <pre><code>GLOBUS_GROUPS=\"group-uuid-1 group-uuid-2\"\n</code></pre>"},{"location":"admin-guide/deployment/production/#identity-provider-restrictions","title":"Identity Provider Restrictions","text":"<p>Limit to specific institutions:</p> <pre><code>AUTHORIZED_IDPS='{\"University Name\": \"idp-uuid\"}'\nAUTHORIZED_GROUPS_PER_IDP='{\"University Name\": \"group-uuid-1,group-uuid-2\"}'\n</code></pre>"},{"location":"admin-guide/deployment/production/#high-assurance-policies","title":"High Assurance Policies","text":"<p>Require MFA and other security policies:</p> <pre><code>GLOBUS_POLICIES=\"policy-uuid-1 policy-uuid-2\"\n</code></pre>"},{"location":"admin-guide/deployment/production/#secrets-management","title":"Secrets Management","text":"<p>Never store secrets in code or version control.</p>"},{"location":"admin-guide/deployment/production/#use-environment-files","title":"Use Environment Files","text":"<pre><code># .env (add to .gitignore)\nSECRET_KEY=\"...\"\nPOSTGRES_PASSWORD=\"...\"\n</code></pre>"},{"location":"admin-guide/deployment/production/#docker-secrets","title":"Docker Secrets","text":"<pre><code>services:\n  gateway:\n    secrets:\n      - db_password\n      - globus_secret\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n  globus_secret:\n    file: ./secrets/globus_secret.txt\n</code></pre>"},{"location":"admin-guide/deployment/production/#vault-integration","title":"Vault Integration","text":"<p>For enterprise deployments, integrate with HashiCorp Vault or similar.</p>"},{"location":"admin-guide/deployment/production/#httpstls","title":"HTTPS/TLS","text":"<p>Always use HTTPS in production.</p>"},{"location":"admin-guide/deployment/production/#lets-encrypt-with-certbot","title":"Let's Encrypt with Certbot","text":"<pre><code>sudo certbot --nginx -d yourdomain.com\n</code></pre>"},{"location":"admin-guide/deployment/production/#custom-certificates","title":"Custom Certificates","text":"<pre><code>server {\n    listen 443 ssl http2;\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#firewall-configuration","title":"Firewall Configuration","text":"<pre><code># Ubuntu/Debian\nsudo ufw allow 80/tcp\nsudo ufw allow 443/tcp\nsudo ufw deny 8000/tcp  # Don't expose Django directly\n\n# CentOS/RHEL\nsudo firewall-cmd --permanent --add-service=http\nsudo firewall-cmd --permanent --add-service=https\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"admin-guide/deployment/production/#performance","title":"Performance","text":""},{"location":"admin-guide/deployment/production/#database-optimization","title":"Database Optimization","text":""},{"location":"admin-guide/deployment/production/#connection-pooling","title":"Connection Pooling","text":"<pre><code># settings.py\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'CONN_MAX_AGE': 600,  # Persistent connections\n        'OPTIONS': {\n            'connect_timeout': 10,\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#indexes","title":"Indexes","text":"<p>Ensure proper indexes on frequently queried fields:</p> <pre><code>python manage.py dbshell\nCREATE INDEX idx_endpoint_slug ON resource_server_endpoint(endpoint_slug);\nCREATE INDEX idx_created_at ON resource_server_listendpointslog(created_at);\n</code></pre>"},{"location":"admin-guide/deployment/production/#caching","title":"Caching","text":""},{"location":"admin-guide/deployment/production/#redis-configuration","title":"Redis Configuration","text":"<pre><code># settings.py\nCACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n        'LOCATION': 'redis://redis:6379/0',\n        'OPTIONS': {\n            'CLIENT_CLASS': 'django_redis.client.DefaultClient',\n            'CONNECTION_POOL_KWARGS': {\n                'max_connections': 50\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#gunicorn-configuration","title":"Gunicorn Configuration","text":""},{"location":"admin-guide/deployment/production/#worker-calculation","title":"Worker Calculation","text":"<pre><code>workers = (2 * CPU_cores) + 1\n</code></pre> <p>For a 16-core machine:</p> <pre><code>workers = (2 * 16) + 1 = 33\n</code></pre>"},{"location":"admin-guide/deployment/production/#production-config","title":"Production Config","text":"<pre><code># gunicorn_asgi.config.py\nimport multiprocessing\n\nbind = \"0.0.0.0:8000\"\nworkers = multiprocessing.cpu_count() * 2 + 1\nworker_class = \"uvicorn.workers.UvicornWorker\"\nworker_connections = 1000\ntimeout = 120\nkeepalive = 5\nmax_requests = 1000\nmax_requests_jitter = 50\n</code></pre>"},{"location":"admin-guide/deployment/production/#nginx-optimization","title":"Nginx Optimization","text":"<pre><code>upstream gateway {\n    least_conn;  # Load balancing algorithm\n    server 127.0.0.1:8000 max_fails=3 fail_timeout=30s;\n    server 127.0.0.1:8001 max_fails=3 fail_timeout=30s;\n    keepalive 64;\n}\n\nserver {\n    listen 443 ssl http2;\n\n    # Gzip compression\n    gzip on;\n    gzip_types text/plain text/css application/json application/javascript;\n    gzip_min_length 1000;\n\n    # Client body size\n    client_max_body_size 100M;\n    client_body_buffer_size 1M;\n\n    # Timeouts\n    proxy_connect_timeout 600s;\n    proxy_send_timeout 600s;\n    proxy_read_timeout 600s;\n    send_timeout 600s;\n\n    # Buffering\n    proxy_buffering off;  # Important for streaming\n    proxy_request_buffering off;\n\n    # Headers\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n\n    location /static/ {\n        alias /path/to/staticfiles/;\n        expires 30d;\n        add_header Cache-Control \"public, immutable\";\n    }\n\n    location / {\n        proxy_pass http://gateway;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/deployment/production/#application-monitoring","title":"Application Monitoring","text":""},{"location":"admin-guide/deployment/production/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Add to <code>docker-compose.yml</code>:</p> <pre><code>services:\n  prometheus:\n    image: prom/prometheus\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n    ports:\n      - \"9090:9090\"\n</code></pre>"},{"location":"admin-guide/deployment/production/#grafana-dashboards","title":"Grafana Dashboards","text":"<pre><code>services:\n  grafana:\n    image: grafana/grafana\n    volumes:\n      - grafana_data:/var/lib/grafana\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=secure_password\n</code></pre>"},{"location":"admin-guide/deployment/production/#log-aggregation","title":"Log Aggregation","text":""},{"location":"admin-guide/deployment/production/#structured-logging","title":"Structured Logging","text":"<pre><code># logging_config.py\nLOGGING = {\n    'version': 1,\n    'formatters': {\n        'json': {\n            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n            'format': '%(asctime)s %(name)s %(levelname)s %(message)s'\n        }\n    },\n    'handlers': {\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'filename': 'logs/gateway.log',\n            'maxBytes': 10485760,  # 10MB\n            'backupCount': 10,\n            'formatter': 'json'\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#elk-stack-integration","title":"ELK Stack Integration","text":"<p>For large deployments, consider Elasticsearch + Logstash + Kibana.</p>"},{"location":"admin-guide/deployment/production/#health-checks","title":"Health Checks","text":""},{"location":"admin-guide/deployment/production/#kubernetes-probes","title":"Kubernetes Probes","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8000\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8000\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"admin-guide/deployment/production/#custom-health-endpoint","title":"Custom Health Endpoint","text":"<p>Create a health check view in Django to verify database, Redis, and Globus Compute connectivity.</p>"},{"location":"admin-guide/deployment/production/#backup-recovery","title":"Backup &amp; Recovery","text":""},{"location":"admin-guide/deployment/production/#database-backups","title":"Database Backups","text":""},{"location":"admin-guide/deployment/production/#automated-backups","title":"Automated Backups","text":"<pre><code>#!/bin/bash\n# backup_db.sh\n\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"/backups/postgres\"\nBACKUP_FILE=\"$BACKUP_DIR/backup_$DATE.sql.gz\"\n\npg_dump -h localhost -U inferencedev inferencegateway | gzip &gt; $BACKUP_FILE\n\n# Keep only last 30 days\nfind $BACKUP_DIR -name \"backup_*.sql.gz\" -mtime +30 -delete\n</code></pre> <p>Add to crontab:</p> <pre><code>0 2 * * * /path/to/backup_db.sh\n</code></pre>"},{"location":"admin-guide/deployment/production/#point-in-time-recovery","title":"Point-in-Time Recovery","text":"<p>Configure PostgreSQL for WAL archiving:</p> <pre><code># postgresql.conf\nwal_level = replica\narchive_mode = on\narchive_command = 'cp %p /backup/wal/%f'\n</code></pre>"},{"location":"admin-guide/deployment/production/#configuration-backups","title":"Configuration Backups","text":"<pre><code># Backup environment and fixtures\ntar -czf config_backup_$(date +%Y%m%d).tar.gz \\\n    .env \\\n    fixtures/ \\\n    nginx_app.conf \\\n    gunicorn_asgi.config.py\n</code></pre>"},{"location":"admin-guide/deployment/production/#scaling","title":"Scaling","text":""},{"location":"admin-guide/deployment/production/#horizontal-scaling","title":"Horizontal Scaling","text":""},{"location":"admin-guide/deployment/production/#multiple-gateway-instances","title":"Multiple Gateway Instances","text":"<pre><code>upstream gateway {\n    server gateway1:8000;\n    server gateway2:8000;\n    server gateway3:8000;\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#session-affinity","title":"Session Affinity","text":"<p>For stateful sessions:</p> <pre><code>upstream gateway {\n    ip_hash;\n    server gateway1:8000;\n    server gateway2:8000;\n}\n</code></pre>"},{"location":"admin-guide/deployment/production/#database-scaling","title":"Database Scaling","text":""},{"location":"admin-guide/deployment/production/#read-replicas","title":"Read Replicas","text":"<pre><code># settings.py\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'HOST': 'primary.db.internal',\n    },\n    'replica': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'HOST': 'replica.db.internal',\n    }\n}\n\nDATABASE_ROUTERS = ['path.to.ReplicaRouter']\n</code></pre>"},{"location":"admin-guide/deployment/production/#connection-pooling-pgbouncer","title":"Connection Pooling (PgBouncer)","text":"<pre><code># pgbouncer.ini\n[databases]\ninferencegateway = host=localhost port=5432 dbname=inferencegateway\n\n[pgbouncer]\npool_mode = transaction\nmax_client_conn = 1000\ndefault_pool_size = 20\n</code></pre>"},{"location":"admin-guide/deployment/production/#inference-backend-scaling","title":"Inference Backend Scaling","text":""},{"location":"admin-guide/deployment/production/#federated-endpoints","title":"Federated Endpoints","text":"<p>Deploy multiple Globus Compute endpoints and configure federated routing for automatic load balancing.</p>"},{"location":"admin-guide/deployment/production/#auto-scaling","title":"Auto-Scaling","text":"<p>Configure Globus Compute endpoints to auto-scale based on demand:</p> <pre><code>engine:\n  provider:\n    min_blocks: 1\n    max_blocks: 20\n</code></pre>"},{"location":"admin-guide/deployment/production/#maintenance","title":"Maintenance","text":""},{"location":"admin-guide/deployment/production/#zero-downtime-deployments","title":"Zero-Downtime Deployments","text":""},{"location":"admin-guide/deployment/production/#blue-green-deployment","title":"Blue-Green Deployment","text":"<ol> <li>Deploy new version alongside old</li> <li>Switch traffic to new version</li> <li>Monitor for issues</li> <li>Decommission old version</li> </ol>"},{"location":"admin-guide/deployment/production/#rolling-updates","title":"Rolling Updates","text":"<pre><code># Update one instance at a time\nfor server in gateway1 gateway2 gateway3; do\n    ssh $server \"cd /app &amp;&amp; git pull &amp;&amp; systemctl restart gateway\"\n    sleep 60  # Allow time to stabilize\ndone\n</code></pre>"},{"location":"admin-guide/deployment/production/#database-migrations","title":"Database Migrations","text":"<p>Always test migrations in staging first:</p> <pre><code># Backup before migrating\n./backup_db.sh\n\n# Run migration\npython manage.py migrate\n\n# If issues occur, restore backup\npsql -h localhost -U inferencedev inferencegateway &lt; backup.sql\n</code></pre>"},{"location":"admin-guide/deployment/production/#disaster-recovery","title":"Disaster Recovery","text":""},{"location":"admin-guide/deployment/production/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":"<ol> <li>Recovery Time Objective (RTO): 2 hours</li> <li>Recovery Point Objective (RPO): 1 hour</li> </ol>"},{"location":"admin-guide/deployment/production/#backup-strategy","title":"Backup Strategy","text":"<ul> <li>Hourly: Database transaction logs</li> <li>Daily: Full database backup</li> <li>Weekly: Complete system backup (config, logs, data)</li> <li>Monthly: Archived to off-site storage</li> </ul>"},{"location":"admin-guide/deployment/production/#failover-procedures","title":"Failover Procedures","text":"<p>Document step-by-step procedures for:</p> <ol> <li>Gateway failure \u2192 Switch to backup gateway</li> <li>Database failure \u2192 Promote read replica</li> <li>Complete site failure \u2192 Activate DR site</li> </ol>"},{"location":"admin-guide/deployment/production/#checklist","title":"Checklist","text":""},{"location":"admin-guide/deployment/production/#pre-production","title":"Pre-Production","text":"<ul> <li>[ ] All secrets are externalized</li> <li>[ ] HTTPS/TLS configured</li> <li>[ ] Firewall rules applied</li> <li>[ ] DEBUG=False</li> <li>[ ] Strong passwords set</li> <li>[ ] Database backed up</li> <li>[ ] Monitoring configured</li> <li>[ ] Log aggregation set up</li> <li>[ ] Health checks working</li> <li>[ ] Load testing completed</li> <li>[ ] Disaster recovery plan documented</li> </ul>"},{"location":"admin-guide/deployment/production/#post-deployment","title":"Post-Deployment","text":"<ul> <li>[ ] Monitor logs for errors</li> <li>[ ] Verify all endpoints responding</li> <li>[ ] Check database performance</li> <li>[ ] Test authentication flow</li> <li>[ ] Verify Globus Compute connectivity</li> <li>[ ] Run integration tests</li> <li>[ ] Document any issues</li> </ul>"},{"location":"admin-guide/deployment/production/#additional-resources","title":"Additional Resources","text":"<ul> <li>Django Security Best Practices</li> <li>Nginx Performance Tuning</li> <li>PostgreSQL Performance Tips</li> <li>Monitoring Guide</li> </ul>"},{"location":"admin-guide/gateway-setup/bare-metal/","title":"Bare Metal Setup","text":"<p>This guide covers installing the FIRST Inference Gateway directly on your server without Docker.</p>"},{"location":"admin-guide/gateway-setup/bare-metal/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux server (Ubuntu 20.04+, CentOS 8+, or similar)</li> <li>Python 3.12 or later</li> <li>PostgreSQL 13 or later</li> <li>Redis 6 or later</li> <li>Poetry (Python dependency manager)</li> <li>Sudo access for system packages</li> <li>At least 4GB RAM</li> </ul>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-1-install-system-dependencies","title":"Step 1: Install System Dependencies","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt update\nsudo apt install -y python3.12 python3.12-dev python3.12-venv \\\n    postgresql postgresql-contrib redis-server \\\n    build-essential libpq-dev git curl\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#centosrhel","title":"CentOS/RHEL","text":"<pre><code>sudo dnf install -y python3.12 python3.12-devel \\\n    postgresql postgresql-server redis \\\n    gcc gcc-c++ make libpq-devel git\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-2-install-poetry","title":"Step 2: Install Poetry","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\n\n# Add to PATH\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Verify installation\npoetry --version\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-3-clone-and-setup-project","title":"Step 3: Clone and Setup Project","text":"<pre><code>git clone https://github.com/auroraGPT-ANL/inference-gateway.git\ncd inference-gateway\n\n# Configure Poetry to create venv in project\npoetry config virtualenvs.in-project true\n\n# Set Python version\npoetry env use python3.12\n\n# Install dependencies\npoetry install\n\n# Activate environment\npoetry shell\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-4-configure-postgresql","title":"Step 4: Configure PostgreSQL","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#initialize-postgresql-if-first-time","title":"Initialize PostgreSQL (if first time)","text":"<pre><code># Ubuntu/Debian (usually auto-initialized)\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n\n# CentOS/RHEL\nsudo postgresql-setup --initdb\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#create-database-and-user","title":"Create Database and User","text":"<pre><code>sudo -u postgres psql\n\n# In PostgreSQL shell:\nCREATE DATABASE inferencegateway;\nCREATE USER inferencedev WITH PASSWORD 'your-secure-password';\nALTER ROLE inferencedev SET client_encoding TO 'utf8';\nALTER ROLE inferencedev SET default_transaction_isolation TO 'read committed';\nALTER ROLE inferencedev SET timezone TO 'UTC';\nGRANT ALL PRIVILEGES ON DATABASE inferencegateway TO inferencedev;\n\\q\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#configure-postgresql-authentication","title":"Configure PostgreSQL Authentication","text":"<p>Edit <code>/etc/postgresql/*/main/pg_hba.conf</code> (path may vary):</p> <pre><code># Add this line (adjust for your security needs)\nhost    inferencegateway    inferencedev    127.0.0.1/32    md5\n</code></pre> <p>Restart PostgreSQL:</p> <pre><code>sudo systemctl restart postgresql\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-5-configure-redis","title":"Step 5: Configure Redis","text":"<p>Start and enable Redis:</p> <pre><code>sudo systemctl start redis\nsudo systemctl enable redis\n\n# Verify it's running\nredis-cli ping\n# Should return: PONG\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-6-register-globus-applications","title":"Step 6: Register Globus Applications","text":"<p>Follow the same steps as in the Docker guide:</p>"},{"location":"admin-guide/gateway-setup/bare-metal/#service-api-application","title":"Service API Application","text":"<ol> <li>Visit developers.globus.org</li> <li>Register a service API application</li> <li>Add redirect URI: <code>http://your-server-ip:8000/complete/globus/</code></li> <li>Note Client UUID and Secret</li> </ol>"},{"location":"admin-guide/gateway-setup/bare-metal/#add-scope","title":"Add Scope","text":"<pre><code>export CLIENT_ID=\"&lt;Your-Service-API-Client-UUID&gt;\"\nexport CLIENT_SECRET=\"&lt;Your-Service-API-Client-Secret&gt;\"\n\ncurl -X POST -s --user $CLIENT_ID:$CLIENT_SECRET \\\n    https://auth.globus.org/v2/api/clients/$CLIENT_ID/scopes \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"scope\": {\n            \"name\": \"Action Provider - all\",\n            \"description\": \"Access to inference service.\",\n            \"scope_suffix\": \"action_all\",\n            \"dependent_scopes\": [\n                {\n                    \"scope\": \"73320ffe-4cb4-4b25-a0a3-83d53d59ce4f\",\n                    \"optional\": false,\n                    \"requires_refresh_token\": true\n                }\n            ]\n        }\n    }'\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#service-account-application","title":"Service Account Application","text":"<ol> <li>In the same project, register a service account application</li> <li>Note Client UUID and Secret</li> </ol>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-7-configure-environment","title":"Step 7: Configure Environment","text":"<p>Create <code>.env</code> file in project root:</p> <pre><code>cat &gt; .env &lt;&lt; 'EOF'\n# --- Core Django Settings ---\nSECRET_KEY=\"&lt;generate-with-command-below&gt;\"\nDEBUG=False\nALLOWED_HOSTS=\"your-server-ip,your-domain.com\"\n\n# --- Globus Credentials ---\nGLOBUS_APPLICATION_ID=\"&lt;Your-Service-API-Client-UUID&gt;\"\nGLOBUS_APPLICATION_SECRET=\"&lt;Your-Service-API-Client-Secret&gt;\"\nSERVICE_ACCOUNT_ID=\"&lt;Your-Service-Account-Client-UUID&gt;\"\nSERVICE_ACCOUNT_SECRET=\"&lt;Your-Service-Account-Client-Secret&gt;\"\n\n# --- Database Credentials ---\nPOSTGRES_DB=\"inferencegateway\"\nPOSTGRES_USER=\"inferencedev\"\nPOSTGRES_PASSWORD=\"your-secure-password\"\nPGHOST=\"localhost\"\nPGPORT=5432\nPGUSER=\"inferencedev\"\nPGPASSWORD=\"your-secure-password\"\nPGDATABASE=\"inferencegateway\"\n\n# --- Redis ---\nREDIS_URL=\"redis://localhost:6379/0\"\n\n# --- Gateway Settings ---\nMAX_BATCHES_PER_USER=2\nSTREAMING_SERVER_HOST=\"localhost:8080\"\nINTERNAL_STREAMING_SECRET=\"&lt;generate-random-secret&gt;\"\nCLI_AUTH_CLIENT_ID=\"58fdd3bc-e1c3-4ce5-80ea-8d6b87cfb944\"\nEOF\n</code></pre> <p>Generate secret key:</p> <pre><code>python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())'\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-8-initialize-database","title":"Step 8: Initialize Database","text":"<pre><code># Make sure you're in the poetry shell\npoetry shell\n\n# Run migrations\npython manage.py makemigrations\npython manage.py migrate\n\n# Create superuser\npython manage.py createsuperuser\n\n# Collect static files\npython manage.py collectstatic --noinput\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-9-test-the-gateway","title":"Step 9: Test the Gateway","text":"<p>Run development server:</p> <pre><code>python manage.py runserver 0.0.0.0:8000\n</code></pre> <p>Test in another terminal:</p> <pre><code>curl http://localhost:8000/\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-10-setup-production-server-gunicorn","title":"Step 10: Setup Production Server (Gunicorn)","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#install-gunicorn-already-included-in-poetry-dependencies","title":"Install Gunicorn (already included in poetry dependencies)","text":"<p>Create a systemd service file:</p> <pre><code>sudo nano /etc/systemd/system/inference-gateway.service\n</code></pre> <p>Add the following:</p> <pre><code>[Unit]\nDescription=FIRST Inference Gateway\nAfter=network.target postgresql.service redis.service\n\n[Service]\nType=notify\nUser=your-username\nGroup=your-username\nWorkingDirectory=/path/to/inference-gateway\nEnvironment=\"PATH=/path/to/inference-gateway/.venv/bin\"\nEnvironmentFile=/path/to/inference-gateway/.env\nExecStart=/path/to/inference-gateway/.venv/bin/gunicorn \\\n    inference_gateway.asgi:application \\\n    -k uvicorn.workers.UvicornWorker \\\n    -b 0.0.0.0:8000 \\\n    --workers 4 \\\n    --log-level info \\\n    --access-logfile /path/to/inference-gateway/logs/access.log \\\n    --error-logfile /path/to/inference-gateway/logs/error.log\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#start-and-enable-service","title":"Start and Enable Service","text":"<pre><code># Create logs directory\nmkdir -p logs\n\n# Reload systemd\nsudo systemctl daemon-reload\n\n# Start service\nsudo systemctl start inference-gateway\n\n# Enable on boot\nsudo systemctl enable inference-gateway\n\n# Check status\nsudo systemctl status inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-11-setup-nginx-recommended","title":"Step 11: Setup Nginx (Recommended)","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#install-nginx","title":"Install Nginx","text":"<pre><code># Ubuntu/Debian\nsudo apt install nginx\n\n# CentOS/RHEL\nsudo dnf install nginx\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#configure-nginx","title":"Configure Nginx","text":"<p>Create site configuration:</p> <pre><code>sudo nano /etc/nginx/sites-available/inference-gateway\n</code></pre> <p>Add the following:</p> <pre><code>upstream inference_gateway {\n    server 127.0.0.1:8000 fail_timeout=0;\n}\n\nserver {\n    listen 80;\n    server_name your-domain.com;\n    client_max_body_size 100M;\n\n    location /static/ {\n        alias /path/to/inference-gateway/staticfiles/;\n    }\n\n    location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n        proxy_buffering off;\n        proxy_pass http://inference_gateway;\n    }\n}\n</code></pre> <p>Enable the site:</p> <pre><code># Ubuntu/Debian\nsudo ln -s /etc/nginx/sites-available/inference-gateway /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl restart nginx\n\n# CentOS/RHEL\nsudo ln -s /etc/nginx/sites-available/inference-gateway /etc/nginx/conf.d/\nsudo nginx -t\nsudo systemctl restart nginx\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#setup-ssl-with-lets-encrypt","title":"Setup SSL with Let's Encrypt","text":"<pre><code># Ubuntu/Debian\nsudo apt install certbot python3-certbot-nginx\n\n# CentOS/RHEL\nsudo dnf install certbot python3-certbot-nginx\n\n# Get certificate\nsudo certbot --nginx -d your-domain.com\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#step-12-configure-firewall","title":"Step 12: Configure Firewall","text":"<pre><code># Ubuntu/Debian (UFW)\nsudo ufw allow 80/tcp\nsudo ufw allow 443/tcp\nsudo ufw enable\n\n# CentOS/RHEL (firewalld)\nsudo firewall-cmd --permanent --add-service=http\nsudo firewall-cmd --permanent --add-service=https\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#maintenance","title":"Maintenance","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#view-logs","title":"View Logs","text":"<pre><code># Application logs\ntail -f logs/error.log\ntail -f logs/access.log\n\n# System service logs\nsudo journalctl -u inference-gateway -f\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#restart-service","title":"Restart Service","text":"<pre><code>sudo systemctl restart inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#update-application","title":"Update Application","text":"<pre><code>cd /path/to/inference-gateway\ngit pull origin main\npoetry install\npython manage.py migrate\npython manage.py collectstatic --noinput\nsudo systemctl restart inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/gateway-setup/bare-metal/#service-wont-start","title":"Service won't start","text":"<p>Check logs:</p> <pre><code>sudo journalctl -u inference-gateway -n 50\n</code></pre> <p>Check configuration:</p> <pre><code>poetry shell\npython manage.py check\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#database-connection-errors","title":"Database connection errors","text":"<p>Verify PostgreSQL is running:</p> <pre><code>sudo systemctl status postgresql\n</code></pre> <p>Test connection:</p> <pre><code>psql -h localhost -U inferencedev -d inferencegateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#permission-errors","title":"Permission errors","text":"<p>Ensure the service user owns the files:</p> <pre><code>sudo chown -R your-username:your-username /path/to/inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#nginx-errors","title":"Nginx errors","text":"<p>Check nginx error log:</p> <pre><code>sudo tail -f /var/log/nginx/error.log\n</code></pre> <p>Test configuration:</p> <pre><code>sudo nginx -t\n</code></pre>"},{"location":"admin-guide/gateway-setup/bare-metal/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Inference Backends</li> <li>Production Best Practices</li> <li>Monitoring Setup</li> </ul>"},{"location":"admin-guide/gateway-setup/bare-metal/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Reference</li> <li>Gunicorn Documentation</li> <li>Nginx Documentation</li> </ul>"},{"location":"admin-guide/gateway-setup/configuration/","title":"Configuration Reference","text":"<p>This page documents all environment variables and configuration options for the FIRST Inference Gateway.</p>"},{"location":"admin-guide/gateway-setup/configuration/#environment-variables","title":"Environment Variables","text":"<p>All configuration is done through environment variables, typically stored in a <code>.env</code> file.</p>"},{"location":"admin-guide/gateway-setup/configuration/#core-django-settings","title":"Core Django Settings","text":"Variable Required Default Description <code>SECRET_KEY</code> Yes - Django secret key for cryptographic signing <code>DEBUG</code> No <code>False</code> Enable debug mode (never use in production) <code>ALLOWED_HOSTS</code> Yes - Comma-separated list of allowed hostnames <code>RUNNING_AUTOMATED_TEST_SUITE</code> No <code>False</code> Set to <code>True</code> to skip Globus High Assurance policy checks (development/testing only) <code>LOG_TO_STDOUT</code> No <code>False</code> Set to <code>True</code> to output logs to stdout (useful for Docker) <p>Security Warning</p> <ul> <li>Never use <code>DEBUG=True</code> in production! This exposes sensitive information.</li> <li>Never use <code>RUNNING_AUTOMATED_TEST_SUITE=True</code> in production! This disables important security checks.</li> </ul>"},{"location":"admin-guide/gateway-setup/configuration/#globus-authentication","title":"Globus Authentication","text":"Variable Required Default Description <code>GLOBUS_APPLICATION_ID</code> Yes - Service API application client UUID <code>GLOBUS_APPLICATION_SECRET</code> Yes - Service API application client secret <code>SERVICE_ACCOUNT_ID</code> Yes - Service Account application client UUID <code>SERVICE_ACCOUNT_SECRET</code> Yes - Service Account application client secret <code>GLOBUS_GROUPS</code> No - Space-separated UUIDs of allowed Globus groups <code>AUTHORIZED_IDPS</code> No - JSON string of authorized identity providers <code>AUTHORIZED_GROUPS_PER_IDP</code> No - JSON string of groups per IDP <code>GLOBUS_POLICIES</code> No - Space-separated policy UUIDs <p>Example with group restrictions:</p> <pre><code>GLOBUS_GROUPS=\"uuid-1 uuid-2 uuid-3\"\n</code></pre> <p>Example with IDP restrictions:</p> <pre><code>AUTHORIZED_IDPS='{\"University\": \"uuid-here\"}'\nAUTHORIZED_GROUPS_PER_IDP='{\"University\": \"group-uuid-1,group-uuid-2\"}'\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#database-configuration","title":"Database Configuration","text":"Variable Required Default Description <code>POSTGRES_DB</code> Yes - Database name <code>POSTGRES_USER</code> Yes - Database user <code>POSTGRES_PASSWORD</code> Yes - Database password <code>PGHOST</code> Yes - Database host (<code>postgres</code> for Docker, <code>localhost</code> for bare metal) <code>PGPORT</code> No <code>5432</code> Database port <code>PGUSER</code> Yes - Database user (can be same as POSTGRES_USER) <code>PGPASSWORD</code> Yes - Database password <code>PGDATABASE</code> Yes - Database name <p>Docker Networking</p> <p>When using Docker Compose, set <code>PGHOST=postgres</code> to use the container name. For bare metal, use <code>PGHOST=localhost</code> or the actual hostname.</p>"},{"location":"admin-guide/gateway-setup/configuration/#redis-configuration","title":"Redis Configuration","text":"Variable Required Default Description <code>REDIS_URL</code> Yes - Redis connection URL <p>Examples:</p> <pre><code># Docker\nREDIS_URL=\"redis://redis:6379/0\"\n\n# Bare metal\nREDIS_URL=\"redis://localhost:6379/0\"\n\n# With password\nREDIS_URL=\"redis://:password@localhost:6379/0\"\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#gateway-settings","title":"Gateway Settings","text":"Variable Required Default Description <code>MAX_BATCHES_PER_USER</code> No <code>2</code> Maximum concurrent batch jobs per user <code>STREAMING_SERVER_HOST</code> No - Internal streaming server host:port <code>INTERNAL_STREAMING_SECRET</code> No - Secret for internal streaming authentication"},{"location":"admin-guide/gateway-setup/configuration/#cli-authentication-helper","title":"CLI Authentication Helper","text":"Variable Required Default Description <code>CLI_AUTH_CLIENT_ID</code> No <code>58fdd3bc...</code> Public client ID for CLI auth script <code>CLI_ALLOWED_DOMAINS</code> No - Comma-separated domains for CLI login <code>CLI_TOKEN_DIR</code> No <code>~/.globus/app</code> Token storage directory <code>CLI_APP_NAME</code> No <code>inference_auth</code> App name for token storage <p>Example:</p> <pre><code>CLI_ALLOWED_DOMAINS=\"anl.gov,alcf.anl.gov,university.edu\"\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#metis-direct-api-configuration","title":"Metis (Direct API) Configuration","text":"<p>For direct OpenAI-compatible API connections:</p> Variable Required Default Description <code>METIS_STATUS_URL</code> No - URL to status manifest JSON <code>METIS_API_TOKENS</code> No - JSON object of endpoint_id -&gt; API token <p>Example:</p> <pre><code>METIS_STATUS_URL=\"https://example.com/status.json\"\nMETIS_API_TOKENS='{\"openai-prod\": \"sk-...\", \"anthropic-prod\": \"sk-ant-...\"}'\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#monitoring-optional","title":"Monitoring (Optional)","text":"Variable Required Default Description <code>GF_SECURITY_ADMIN_USER</code> No <code>admin</code> Grafana admin username <code>GF_SECURITY_ADMIN_PASSWORD</code> No <code>admin</code> Grafana admin password"},{"location":"admin-guide/gateway-setup/configuration/#qstat-endpoints-optional","title":"Qstat Endpoints (Optional)","text":"<p>For HPC cluster status monitoring:</p> Variable Required Default Description <code>SOPHIA_QSTAT_ENDPOINT_UUID</code> No - Endpoint UUID for qstat function <code>SOPHIA_QSTAT_FUNCTION_UUID</code> No - Function UUID for qstat"},{"location":"admin-guide/gateway-setup/configuration/#fixture-configuration","title":"Fixture Configuration","text":"<p>Fixtures are JSON files that define available endpoints and models.</p>"},{"location":"admin-guide/gateway-setup/configuration/#endpoint-fixture-format","title":"Endpoint Fixture Format","text":"<p>Located at <code>fixtures/endpoints.json</code>:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"endpoint_slug\": \"local-vllm-opt-125m\",\n            \"cluster\": \"local\",\n            \"framework\": \"vllm\",\n            \"model\": \"facebook/opt-125m\",\n            \"api_port\": 8001,\n            \"endpoint_uuid\": \"&lt;globus-compute-endpoint-uuid&gt;\",\n            \"function_uuid\": \"&lt;globus-compute-function-uuid&gt;\",\n            \"batch_endpoint_uuid\": \"\",\n            \"batch_function_uuid\": \"\",\n            \"allowed_globus_groups\": \"\"\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#federated-endpoint-fixture-format","title":"Federated Endpoint Fixture Format","text":"<p>Located at <code>fixtures/federated_endpoints.json</code>:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.federatedendpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"name\": \"OPT 125M (Federated)\",\n            \"slug\": \"federated-opt-125m\",\n            \"target_model_name\": \"facebook/opt-125m\",\n            \"description\": \"Federated access point\",\n            \"targets\": [\n                {\n                    \"cluster\": \"local\",\n                    \"framework\": \"vllm\",\n                    \"model\": \"facebook/opt-125m\",\n                    \"endpoint_slug\": \"local-vllm-opt-125m\",\n                    \"endpoint_uuid\": \"&lt;endpoint-uuid&gt;\",\n                    \"function_uuid\": \"&lt;function-uuid&gt;\",\n                    \"api_port\": 8001\n                }\n            ]\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#django-settings","title":"Django Settings","text":"<p>Advanced settings in <code>inference_gateway/settings.py</code>:</p>"},{"location":"admin-guide/gateway-setup/configuration/#cors-settings","title":"CORS Settings","text":"<pre><code>CORS_ALLOW_ALL_ORIGINS = False  # Set True for development only\nCORS_ALLOWED_ORIGINS = [\n    \"https://yourdomain.com\",\n]\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#logging-configuration","title":"Logging Configuration","text":"<p>Located in <code>logging_config.py</code>:</p> <pre><code>LOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'file': {\n            'level': 'INFO',\n            'class': 'logging.handlers.RotatingFileHandler',\n            'filename': 'logs/django_info.log',\n            'maxBytes': 1024 * 1024 * 15,  # 15MB\n            'backupCount': 10,\n        },\n    },\n    # ... more configuration\n}\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#cache-configuration","title":"Cache Configuration","text":"<pre><code>CACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n        'LOCATION': os.environ.get('REDIS_URL'),\n    }\n}\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#gunicorn-configuration","title":"Gunicorn Configuration","text":"<p>For production deployments, configure Gunicorn in <code>gunicorn_asgi.config.py</code>:</p> <pre><code>bind = \"0.0.0.0:8000\"\nworkers = 4  # Adjust based on CPU cores\nworker_class = \"uvicorn.workers.UvicornWorker\"\ntimeout = 120\nkeepalive = 5\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#worker-calculation","title":"Worker Calculation","text":"<p>Recommended formula:</p> <pre><code>workers = (2 * CPU_cores) + 1\n</code></pre> <p>For a 4-core machine: <pre><code>workers = (2 * 4) + 1 = 9\n</code></pre></p>"},{"location":"admin-guide/gateway-setup/configuration/#nginx-configuration","title":"Nginx Configuration","text":"<p>Example production configuration:</p> <pre><code>upstream inference_gateway {\n    server 127.0.0.1:8000 fail_timeout=0;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name yourdomain.com;\n\n    # SSL Configuration\n    ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n\n    # File upload size limit\n    client_max_body_size 100M;\n\n    # Timeouts\n    proxy_connect_timeout 600s;\n    proxy_send_timeout 600s;\n    proxy_read_timeout 600s;\n\n    location /static/ {\n        alias /path/to/staticfiles/;\n        expires 30d;\n        add_header Cache-Control \"public, immutable\";\n    }\n\n    location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n        proxy_buffering off;\n        proxy_pass http://inference_gateway;\n    }\n}\n\n# Redirect HTTP to HTTPS\nserver {\n    listen 80;\n    server_name yourdomain.com;\n    return 301 https://$server_name$request_uri;\n}\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"admin-guide/gateway-setup/configuration/#development","title":"Development","text":"<pre><code>DEBUG=True\nALLOWED_HOSTS=\"localhost,127.0.0.1\"\nSECRET_KEY=\"dev-secret-key-not-secure\"\nPGHOST=\"localhost\"\nREDIS_URL=\"redis://localhost:6379/0\"\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#staging","title":"Staging","text":"<pre><code>DEBUG=False\nALLOWED_HOSTS=\"staging.yourdomain.com\"\nSECRET_KEY=\"&lt;strong-secret-key&gt;\"\nPGHOST=\"postgres-staging.internal\"\nREDIS_URL=\"redis://redis-staging.internal:6379/0\"\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#production","title":"Production","text":"<pre><code>DEBUG=False\nALLOWED_HOSTS=\"yourdomain.com,api.yourdomain.com\"\nSECRET_KEY=\"&lt;strong-secret-key&gt;\"\nPGHOST=\"postgres-prod.internal\"\nREDIS_URL=\"redis://redis-prod.internal:6379/0\"\n\n# Enable security features\nSECURE_SSL_REDIRECT=True\nSESSION_COOKIE_SECURE=True\nCSRF_COOKIE_SECURE=True\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#secrets-management","title":"Secrets Management","text":""},{"location":"admin-guide/gateway-setup/configuration/#using-docker-secrets","title":"Using Docker Secrets","text":"<pre><code>services:\n  inference-gateway:\n    secrets:\n      - globus_secret\n      - db_password\n\nsecrets:\n  globus_secret:\n    file: ./secrets/globus_secret.txt\n  db_password:\n    file: ./secrets/db_password.txt\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#using-environment-files","title":"Using Environment Files","text":"<pre><code># .env.local (gitignored)\nsource .env.production\nexport POSTGRES_PASSWORD=\"&lt;secret&gt;\"\nexport GLOBUS_APPLICATION_SECRET=\"&lt;secret&gt;\"\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#validation","title":"Validation","text":"<p>Check your configuration:</p> <pre><code># Django check\npython manage.py check\n\n# Database connectivity\npython manage.py dbshell\n\n# Show current settings (dev only!)\npython manage.py diffsettings\n</code></pre>"},{"location":"admin-guide/gateway-setup/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Docker Deployment</li> <li>Bare Metal Setup</li> <li>Production Best Practices</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/","title":"Docker Deployment","text":"<p>This guide shows you how to deploy the FIRST Inference Gateway using Docker and Docker Compose.</p>"},{"location":"admin-guide/gateway-setup/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop 4.29+ (or Docker Engine 24+) with Docker Compose v2</li> <li>Git</li> <li>Globus Account and registered applications</li> <li>At least 4GB RAM available for containers</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/auroraGPT-ANL/inference-gateway.git\ncd inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#step-2-register-globus-applications","title":"Step 2: Register Globus Applications","text":"<p>Before deploying, you need to register two Globus applications.</p>"},{"location":"admin-guide/gateway-setup/docker/#service-api-application","title":"Service API Application","text":"<p>This handles API authorization:</p> <ol> <li>Visit developers.globus.org</li> <li>Click Register a service API</li> <li>Fill in the form:</li> <li>App Name: \"My Inference Gateway\"</li> <li>Redirect URIs: <code>http://localhost:8000/complete/globus/</code> (for local development)</li> <li>Add your production URL if deploying to a server</li> <li>Note the Client UUID and generate a Client Secret</li> </ol>"},{"location":"admin-guide/gateway-setup/docker/#add-scope-to-service-api-application","title":"Add Scope to Service API Application","text":"<pre><code>export CLIENT_ID=\"&lt;Your-Service-API-Client-UUID&gt;\"\nexport CLIENT_SECRET=\"&lt;Your-Service-API-Client-Secret&gt;\"\n\ncurl -X POST -s --user $CLIENT_ID:$CLIENT_SECRET \\\n    https://auth.globus.org/v2/api/clients/$CLIENT_ID/scopes \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"scope\": {\n            \"name\": \"Action Provider - all\",\n            \"description\": \"Access to inference service.\",\n            \"scope_suffix\": \"action_all\",\n            \"dependent_scopes\": [\n                {\n                    \"scope\": \"73320ffe-4cb4-4b25-a0a3-83d53d59ce4f\",\n                    \"optional\": false,\n                    \"requires_refresh_token\": true\n                }\n            ]\n        }\n    }'\n</code></pre> <p>Verify the scope:</p> <pre><code>curl -s --user $CLIENT_ID:$CLIENT_SECRET https://auth.globus.org/v2/api/clients/$CLIENT_ID\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#service-account-application","title":"Service Account Application","text":"<p>To handle the communication between the Gateway API and the compute resources (the Inference Backend), you need to create a Globus Service Account application. This application represents the Globus identity that will own the Globus Compute endpoints.</p> <ol> <li>Visit developers.globus.org and sign in.</li> <li>Under Projects, click on the project used to register your Service API application from the previous step.</li> <li>Click on Add an App.</li> <li>Select Register a service account ....</li> <li>Complete the registration form:</li> <li>Set App Name (e.g., \"My Inference Endpoints\").</li> <li>Set Privacy Policy and Terms &amp; Conditions URLs if applicable.</li> <li>After registration, a Client UUID will be assigned to your Globus application. Generate a Client Secret by clicking on the Add Client Secret button on the right-hand side. You will need both for the <code>.env</code> configuration. The UUID will be for <code>SERVICE_ACCOUNT_ID</code>, and the secret will be for <code>SERVICE_ACCOUNT_SECRET</code>.</li> </ol>"},{"location":"admin-guide/gateway-setup/docker/#step-3-configure-environment","title":"Step 3: Configure Environment","text":"<p>Copy the example environment file:</p> <pre><code>cp deploy/docker/env.example .env\n</code></pre> <p>Edit <code>.env</code> with your configuration:</p> <pre><code># --- Core Django Settings ---\nSECRET_KEY=\"&lt;generate-with-command-below&gt;\"\nDEBUG=True\nALLOWED_HOSTS=\"localhost,127.0.0.1\"\n\n# --- Testing/Development Flags ---\n# Set to True to skip Globus High Assurance policy checks (for development/testing)\n# Set to False for production deployment\nRUNNING_AUTOMATED_TEST_SUITE=True\nLOG_TO_STDOUT=True  # Makes logs visible via docker-compose logs\n\n# --- Globus Credentials ---\nGLOBUS_APPLICATION_ID=\"&lt;Your-Service-API-Client-UUID&gt;\"\nGLOBUS_APPLICATION_SECRET=\"&lt;Your-Service-API-Client-Secret&gt;\"\nSERVICE_ACCOUNT_ID=\"&lt;Your-Service-Account-Client-UUID&gt;\"\nSERVICE_ACCOUNT_SECRET=\"&lt;Your-Service-Account-Client-Secret&gt;\"\n\n# --- Database Credentials (change for production) ---\nPOSTGRES_DB=\"inferencegateway\"\nPOSTGRES_USER=\"inferencedev\"\nPOSTGRES_PASSWORD=\"change-this-password\"\nPGHOST=\"postgres\"\nPGPORT=5432\nPGUSER=\"inferencedev\"\nPGPASSWORD=\"change-this-password\"\nPGDATABASE=\"inferencegateway\"\n\n# --- Redis ---\nREDIS_URL=\"redis://redis:6379/0\"\n\n# --- Gateway Settings ---\nMAX_BATCHES_PER_USER=2\nSTREAMING_SERVER_HOST=\"localhost:8080\"\nINTERNAL_STREAMING_SECRET=\"change-this-secret\"\nCLI_AUTH_CLIENT_ID=\"58fdd3bc-e1c3-4ce5-80ea-8d6b87cfb944\"\n</code></pre> <p>Generate a secret key:</p> <pre><code>python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())'\n</code></pre> <p>Production Security</p> <p>For production deployments:</p> <ul> <li>Set <code>RUNNING_AUTOMATED_TEST_SUITE=False</code></li> <li>Change all passwords and secrets</li> <li>Set <code>DEBUG=False</code></li> <li>Add your domain to <code>ALLOWED_HOSTS</code></li> <li>Configure proper Globus policies (<code>GLOBUS_POLICIES</code>)</li> <li>Set authorized IDP domains (<code>AUTHORIZED_IDP_DOMAINS</code>)</li> <li>Use strong, unique passwords</li> <li>Consider using secrets management (e.g., Docker secrets)</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#step-4-start-the-services","title":"Step 4: Start the Services","text":"<pre><code>cd deploy/docker\ndocker-compose up -d --build\n</code></pre> <p>This starts:</p> <ul> <li><code>inference-gateway</code>: The Django API application</li> <li><code>postgres</code>: PostgreSQL database</li> <li><code>redis</code>: Redis cache</li> <li><code>nginx</code>: Reverse proxy (optional, if configured)</li> </ul> <p>Verify services are running:</p> <pre><code>docker-compose ps\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#step-5-initialize-the-database","title":"Step 5: Initialize the Database","text":"<p>Run migrations:</p> <pre><code>docker-compose exec inference-gateway python manage.py migrate\n</code></pre> <p>Create a superuser (optional, for Django admin access):</p> <pre><code>docker-compose exec inference-gateway python manage.py createsuperuser\n</code></pre> <p>Collect static files:</p> <pre><code>docker-compose exec inference-gateway python manage.py collectstatic --noinput\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#step-6-verify-the-gateway","title":"Step 6: Verify the Gateway","text":"<p>Check that the gateway is running:</p> <pre><code>curl http://localhost:8000/\n</code></pre> <p>Access the Django admin (if superuser was created):</p> <ul> <li>URL: http://localhost:8000/admin/</li> <li>Login with your superuser credentials</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#step-7-configure-backends","title":"Step 7: Configure Backends","text":"<p>Now you need to connect inference backends. Choose one:</p> <ul> <li>Direct API Connection - Connect to OpenAI or similar APIs</li> <li>Local vLLM - Run vLLM locally</li> <li>Globus Compute + vLLM - HPC cluster deployment</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#docker-compose-services","title":"Docker Compose Services","text":"<p>The <code>docker-compose.yml</code> includes:</p>"},{"location":"admin-guide/gateway-setup/docker/#core-services","title":"Core Services","text":"<ul> <li>inference-gateway: Django application (port 8000)</li> <li>postgres: PostgreSQL 15 (port 5432)</li> <li>redis: Redis 7 (port 6379)</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#optional-services","title":"Optional Services","text":"<p>You can add these to your compose file:</p> <ul> <li>nginx: Reverse proxy for production</li> <li>prometheus: Metrics collection</li> <li>grafana: Visualization dashboard</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#common-commands","title":"Common Commands","text":""},{"location":"admin-guide/gateway-setup/docker/#view-logs","title":"View logs","text":"<pre><code># All services\ndocker-compose logs -f\n\n# Specific service\ndocker-compose logs -f inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#restart-services","title":"Restart services","text":"<pre><code># All services\ndocker-compose restart\n\n# Specific service\ndocker-compose restart inference-gateway\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#stop-services","title":"Stop services","text":"<pre><code>docker-compose down\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#stop-and-remove-volumes-clean-slate","title":"Stop and remove volumes (clean slate)","text":"<pre><code>docker-compose down -v\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#access-container-shell","title":"Access container shell","text":"<pre><code>docker-compose exec inference-gateway /bin/bash\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#run-django-management-commands","title":"Run Django management commands","text":"<pre><code>docker-compose exec inference-gateway python manage.py &lt;command&gt;\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#updating-the-deployment","title":"Updating the Deployment","text":"<p>Pull latest changes:</p> <pre><code>git pull origin main\ndocker-compose build\ndocker-compose up -d\ndocker-compose exec inference-gateway python manage.py migrate\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/gateway-setup/docker/#gateway-wont-start","title":"Gateway won't start","text":"<p>Check logs:</p> <pre><code>docker-compose logs inference-gateway\n</code></pre> <p>Common issues:</p> <ul> <li>Missing environment variables</li> <li>Database connection failed</li> <li>Port 8000 already in use</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#database-connection-errors","title":"Database connection errors","text":"<p>Verify PostgreSQL is running:</p> <pre><code>docker-compose ps postgres\n</code></pre> <p>Check database logs:</p> <pre><code>docker-compose logs postgres\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#cant-access-admin-panel","title":"Can't access admin panel","text":"<p>Ensure you created a superuser:</p> <pre><code>docker-compose exec inference-gateway python manage.py createsuperuser\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#502-bad-gateway-from-nginx","title":"502 Bad Gateway from Nginx","text":"<p>Check that the gateway container is running:</p> <pre><code>docker-compose ps inference-gateway\n</code></pre> <p>Verify nginx configuration:</p> <pre><code>docker-compose exec nginx nginx -t\n</code></pre>"},{"location":"admin-guide/gateway-setup/docker/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Inference Backends</li> <li>Production Best Practices</li> <li>Monitoring Setup</li> </ul>"},{"location":"admin-guide/gateway-setup/docker/#additional-resources","title":"Additional Resources","text":"<ul> <li>Docker Compose Documentation</li> <li>Configuration Reference</li> <li>User Guide</li> </ul>"},{"location":"admin-guide/inference-setup/","title":"Inference Backend Setup","text":"<p>The FIRST Inference Gateway supports multiple types of inference backends. Choose the approach that best fits your use case.</p>"},{"location":"admin-guide/inference-setup/#backend-options","title":"Backend Options","text":""},{"location":"admin-guide/inference-setup/#1-globus-compute-vllm-recommended","title":"1. Globus Compute + vLLM (Recommended)","text":"<p>Deploy vLLM on HPC clusters or multiple servers with Globus Compute for remote execution and federated routing.</p> <p>Best for:</p> <ul> <li>Multi-cluster deployments</li> <li>HPC environments</li> <li>Federated inference across organizations</li> <li>Production deployments requiring high availability</li> </ul> <p>Pros:</p> <ul> <li>Federated routing across clusters</li> <li>Automatic failover</li> <li>HPC job scheduler integration</li> <li>Scales across organizations</li> <li>Secure, authenticated remote execution</li> </ul> <p>Cons:</p> <ul> <li>More complex setup</li> <li>Requires Globus Compute knowledge</li> <li>Additional configuration overhead</li> </ul> <p>\u2192 Setup Globus Compute + vLLM</p>"},{"location":"admin-guide/inference-setup/#2-local-vllm-setup","title":"2. Local vLLM Setup","text":"<p>Run vLLM inference server locally without Globus Compute.</p> <p>Best for:</p> <ul> <li>Single-server deployments</li> <li>Development and testing with local models</li> <li>Controlled environments where Globus Compute isn't needed</li> <li>Simple architectures</li> </ul> <p>Pros:</p> <ul> <li>Full control over models and data</li> <li>No external dependencies</li> <li>Simple architecture</li> <li>Lower latency for local requests</li> </ul> <p>Cons:</p> <ul> <li>Requires GPU resources</li> <li>Single point of failure</li> <li>Manual scaling</li> <li>No federated routing</li> </ul> <p>\u2192 Setup Local vLLM</p>"},{"location":"admin-guide/inference-setup/#3-direct-api-connection","title":"3. Direct API Connection","text":"<p>Connect to existing OpenAI-compatible APIs without any local inference infrastructure.</p> <p>Best for:</p> <ul> <li>Quick testing and development</li> <li>Using commercial API services (OpenAI, Anthropic, etc.)</li> <li>Organizations without GPU resources</li> <li>Proxying/adding authentication to existing APIs</li> </ul> <p>Pros:</p> <ul> <li>Fastest setup (5-10 minutes)</li> <li>No local compute resources needed</li> <li>Scales with the provider</li> <li>Multiple models immediately available</li> </ul> <p>Cons:</p> <ul> <li>Costs per API call</li> <li>Data leaves your infrastructure</li> <li>Dependent on third-party service</li> </ul> <p>\u2192 Setup Direct API Connection</p>"},{"location":"admin-guide/inference-setup/#comparison-matrix","title":"Comparison Matrix","text":"Feature Globus Compute + vLLM Local vLLM Direct API Setup Time 2-4 hours 30-60 min 5-10 min GPU Required Yes Yes No Data Privacy Local Local External Multi-Cluster Yes No No Failover Automatic Manual Provider Cost Infrastructure Infrastructure Pay-per-use HPC Integration Yes No No Complexity High Medium Low"},{"location":"admin-guide/inference-setup/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[Choose Backend] --&gt; B{Have GPU?}\n    B --&gt;|No| C[Direct API]\n    B --&gt;|Yes| D{Multiple Clusters?}\n    D --&gt;|No| E{Need HPC Integration?}\n    D --&gt;|Yes| F[Globus Compute + vLLM]\n    E --&gt;|No| G[Local vLLM]\n    E --&gt;|Yes| F\n</code></pre>"},{"location":"admin-guide/inference-setup/#combined-approach","title":"Combined Approach","text":"<p>You can use multiple backends simultaneously! The gateway supports:</p> <ul> <li>Multiple Direct API connections: Route to OpenAI, Anthropic, etc.</li> <li>Mix Local and Remote: Some models local, some via Globus Compute</li> <li>Federated Endpoints: Route same model across multiple clusters</li> </ul> <p>Example combined setup:</p> <pre><code>{\n  \"endpoints\": [\n    {\n      \"cluster\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"type\": \"direct_api\"\n    },\n    {\n      \"cluster\": \"local\",\n      \"model\": \"llama-3-8b\",\n      \"type\": \"vllm\"\n    },\n    {\n      \"cluster\": \"sophia\",\n      \"model\": \"llama-3-70b\",\n      \"type\": \"globus_compute\"\n    }\n  ]\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/#prerequisites-by-backend-type","title":"Prerequisites by Backend Type","text":""},{"location":"admin-guide/inference-setup/#all-backends","title":"All Backends","text":"<ul> <li>FIRST Gateway already deployed</li> <li>Access to gateway configuration</li> <li>Admin access to load fixtures</li> </ul>"},{"location":"admin-guide/inference-setup/#direct-api","title":"Direct API","text":"<ul> <li>API keys from providers</li> <li>Status manifest endpoint (can be static file)</li> </ul>"},{"location":"admin-guide/inference-setup/#local-vllm","title":"Local vLLM","text":"<ul> <li>GPU with sufficient VRAM for your model</li> <li>Python 3.12+</li> <li>Ability to run vLLM server</li> </ul>"},{"location":"admin-guide/inference-setup/#globus-compute-vllm","title":"Globus Compute + vLLM","text":"<ul> <li>All Local vLLM requirements plus:</li> <li>Globus Service Account application</li> <li>Access to HPC cluster (optional)</li> <li>Ability to configure Globus Compute endpoints</li> </ul>"},{"location":"admin-guide/inference-setup/#setup-workflow","title":"Setup Workflow","text":""},{"location":"admin-guide/inference-setup/#phase-1-choose-and-setup-backend","title":"Phase 1: Choose and Setup Backend","text":"<p>Follow the guide for your chosen backend type.</p>"},{"location":"admin-guide/inference-setup/#phase-2-register-with-gateway","title":"Phase 2: Register with Gateway","text":"<p>Update fixture files (<code>fixtures/endpoints.json</code> or <code>fixtures/federated_endpoints.json</code>) with your backend details.</p>"},{"location":"admin-guide/inference-setup/#phase-3-load-configuration","title":"Phase 3: Load Configuration","text":"<pre><code># Docker\ndocker-compose exec inference-gateway python manage.py loaddata fixtures/endpoints.json\n\n# Bare metal\npython manage.py loaddata fixtures/endpoints.json\n</code></pre>"},{"location":"admin-guide/inference-setup/#phase-4-test","title":"Phase 4: Test","text":"<p>Send a test request to verify the connection:</p> <pre><code>curl -X POST http://localhost:8000/resource_server/v1/chat/completions \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"your-model-name\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"admin-guide/inference-setup/#next-steps","title":"Next Steps","text":"<p>Choose your backend setup guide:</p> <ul> <li>Globus Compute + vLLM (Recommended for Production)</li> <li>Local vLLM Setup</li> <li>Direct API Connection</li> </ul> <p>After setup:</p> <ul> <li>Production Best Practices</li> <li>Monitoring &amp; Troubleshooting</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/","title":"Direct API Connection","text":"<p>This guide shows you how to connect the FIRST Gateway to existing OpenAI-compatible APIs without running any local inference infrastructure.</p>"},{"location":"admin-guide/inference-setup/direct-api/#overview","title":"Overview","text":"<p>The Direct API backend allows you to:</p> <ul> <li>Proxy requests to commercial APIs (OpenAI, Anthropic, etc.)</li> <li>Add Globus authentication to existing APIs</li> <li>Manage API keys centrally</li> <li>Route between multiple API providers</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[User] --&gt;|Globus Token| B[FIRST Gateway]\n    B --&gt;|API Key| C[OpenAI API]\n    B --&gt;|API Key| D[Anthropic API]\n    B --&gt;|API Key| E[Custom API]\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#prerequisites","title":"Prerequisites","text":"<ul> <li>FIRST Gateway deployed and running</li> <li>API keys from your providers</li> <li>A way to host a status manifest (static file or endpoint)</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#step-1-create-status-manifest","title":"Step 1: Create Status Manifest","text":"<p>The gateway uses a status manifest to discover available endpoints. Create a JSON file:</p> <pre><code>{\n  \"openai-gpt4\": {\n    \"status\": \"Live\",\n    \"model\": \"OpenAI GPT-4\",\n    \"description\": \"GPT-4 models via OpenAI API\",\n    \"experts\": [\n      \"gpt-4\",\n      \"gpt-4-turbo\",\n      \"gpt-4o\",\n      \"gpt-4o-mini\"\n    ],\n    \"url\": \"https://api.openai.com/v1\",\n    \"endpoint_id\": \"openai-production\"\n  },\n  \"anthropic-claude\": {\n    \"status\": \"Live\",\n    \"model\": \"Anthropic Claude\",\n    \"description\": \"Claude models via Anthropic API\",\n    \"experts\": [\n      \"claude-3-opus-20240229\",\n      \"claude-3-sonnet-20240229\",\n      \"claude-3-haiku-20240307\"\n    ],\n    \"url\": \"https://api.anthropic.com/v1\",\n    \"endpoint_id\": \"anthropic-production\"\n  }\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#manifest-field-descriptions","title":"Manifest Field Descriptions","text":"Field Required Description <code>status</code> Yes \"Live\", \"Offline\", or \"Maintenance\" <code>model</code> Yes Human-readable model description <code>description</code> Yes Detailed description <code>experts</code> Yes Array of model identifiers <code>url</code> Yes Base URL for the API <code>endpoint_id</code> Yes Unique identifier (used for API key mapping)"},{"location":"admin-guide/inference-setup/direct-api/#step-2-host-the-status-manifest","title":"Step 2: Host the Status Manifest","text":""},{"location":"admin-guide/inference-setup/direct-api/#option-a-static-file-server","title":"Option A: Static File Server","text":"<pre><code># Simple Python HTTP server\nmkdir -p /var/www/metis\ncp status.json /var/www/metis/\ncd /var/www/metis\npython3 -m http.server 8055\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#option-b-nginx","title":"Option B: Nginx","text":"<pre><code>server {\n    listen 80;\n    server_name status.yourdomain.com;\n\n    location / {\n        root /var/www/metis;\n        add_header Content-Type application/json;\n        add_header Access-Control-Allow-Origin *;\n    }\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#option-c-s3cloud-storage","title":"Option C: S3/Cloud Storage","text":"<p>Upload <code>status.json</code> to a public S3 bucket or equivalent:</p> <pre><code># AWS S3\naws s3 cp status.json s3://your-bucket/status.json --acl public-read\n\n# Access via: https://your-bucket.s3.amazonaws.com/status.json\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#option-d-local-for-docker-development-only","title":"Option D: Local for Docker (Development Only)","text":"<p>For local Docker testing:</p> <pre><code># On host machine\nmkdir -p deploy/docker/examples\ncat &gt; deploy/docker/examples/metis-status.json &lt;&lt; 'EOF'\n{\n  \"openai-gateway\": {\n    \"status\": \"Live\",\n    \"model\": \"OpenAI Pass-through\",\n    \"description\": \"Routes to OpenAI's GPT models\",\n    \"experts\": [\"gpt-4o-mini\", \"gpt-4\"],\n    \"url\": \"https://api.openai.com/v1\",\n    \"endpoint_id\": \"openai-production\"\n  }\n}\nEOF\n\n# Serve it\npython3 -m http.server 8055 --directory deploy/docker/examples\n</code></pre> <p>Then use <code>http://host.docker.internal:8055/metis-status.json</code> in your Docker <code>.env</code>.</p>"},{"location":"admin-guide/inference-setup/direct-api/#step-3-configure-gateway","title":"Step 3: Configure Gateway","text":"<p>Add these to your gateway's <code>.env</code> file:</p> <pre><code># URL to your status manifest\nMETIS_STATUS_URL=\"http://your-server:8055/status.json\"\n\n# API keys mapped to endpoint_id from the manifest\nMETIS_API_TOKENS='{\"openai-production\": \"sk-proj-...\", \"anthropic-production\": \"sk-ant-...\"}'\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#environment-variable-format","title":"Environment Variable Format","text":"<p>METIS_STATUS_URL: Direct URL to your JSON manifest</p> <p>METIS_API_TOKENS: JSON object where:</p> <ul> <li>Keys are <code>endpoint_id</code> values from your manifest</li> <li>Values are the API keys for those services</li> </ul> <p>Example with multiple providers:</p> <pre><code>METIS_API_TOKENS='{\n  \"openai-production\": \"sk-proj-abc123...\",\n  \"anthropic-production\": \"sk-ant-xyz789...\",\n  \"custom-api\": \"custom-key-here\"\n}'\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#step-4-restart-gateway","title":"Step 4: Restart Gateway","text":""},{"location":"admin-guide/inference-setup/direct-api/#docker","title":"Docker","text":"<pre><code>docker-compose up -d inference-gateway\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#bare-metal","title":"Bare Metal","text":"<pre><code>sudo systemctl restart inference-gateway\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#step-5-test-the-connection","title":"Step 5: Test the Connection","text":"<p>Get a Globus token:</p> <pre><code>export TOKEN=$(python inference-auth-token.py get_access_token)\n</code></pre> <p>Test OpenAI endpoint:</p> <pre><code>curl -X POST http://localhost:8000/resource_server/metis/api/v1/chat/completions \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello from FIRST!\"}],\n    \"stream\": false\n  }'\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"id\": \"chatcmpl-...\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o-mini\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Hello! How can I assist you today?\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 25\n  }\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"admin-guide/inference-setup/direct-api/#multiple-endpoints-per-provider","title":"Multiple Endpoints Per Provider","text":"<pre><code>{\n  \"openai-us-east\": {\n    \"status\": \"Live\",\n    \"model\": \"OpenAI US East\",\n    \"experts\": [\"gpt-4\"],\n    \"url\": \"https://api.openai.com/v1\",\n    \"endpoint_id\": \"openai-us-east\"\n  },\n  \"openai-eu-west\": {\n    \"status\": \"Live\",\n    \"model\": \"OpenAI EU West\",\n    \"experts\": [\"gpt-4\"],\n    \"url\": \"https://api.openai.com/v1\",\n    \"endpoint_id\": \"openai-eu-west\"\n  }\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#custom-api-headers","title":"Custom API Headers","text":"<p>For APIs requiring additional headers, you can extend the gateway code or use environment variables. Contact your administrator for custom integration.</p>"},{"location":"admin-guide/inference-setup/direct-api/#load-balancing","title":"Load Balancing","text":"<p>The gateway automatically load-balances across all \"Live\" endpoints for the same model.</p>"},{"location":"admin-guide/inference-setup/direct-api/#failover","title":"Failover","text":"<p>If one endpoint returns an error, the gateway automatically tries the next available endpoint.</p>"},{"location":"admin-guide/inference-setup/direct-api/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/inference-setup/direct-api/#check-endpoint-status","title":"Check Endpoint Status","text":"<p>The gateway periodically fetches the status manifest. View logs:</p> <pre><code># Docker\ndocker-compose logs -f inference-gateway | grep metis\n\n# Bare metal\ntail -f logs/django_info.log | grep metis\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#track-api-usage","title":"Track API Usage","text":"<p>Monitor usage through:</p> <ul> <li>Gateway access logs</li> <li>Provider API dashboards (OpenAI, Anthropic)</li> <li>Custom usage tracking (implement in gateway)</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#cost-management","title":"Cost Management","text":""},{"location":"admin-guide/inference-setup/direct-api/#setting-budgets","title":"Setting Budgets","text":"<p>Configure per-user or per-group budgets in your application logic or via API key restrictions at the provider level.</p>"},{"location":"admin-guide/inference-setup/direct-api/#rate-limiting","title":"Rate Limiting","text":"<p>The gateway supports rate limiting per user/group. Configure in Django admin or via settings.</p>"},{"location":"admin-guide/inference-setup/direct-api/#security-considerations","title":"Security Considerations","text":""},{"location":"admin-guide/inference-setup/direct-api/#api-key-security","title":"API Key Security","text":"<p>Protect Your API Keys</p> <ul> <li>Never commit API keys to version control</li> <li>Use environment variables or secrets management</li> <li>Rotate keys regularly</li> <li>Use separate keys for dev/staging/production</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#status-manifest-security","title":"Status Manifest Security","text":"<p>If your manifest contains sensitive information:</p> <ul> <li>Serve over HTTPS</li> <li>Implement authentication (basic auth, token)</li> <li>Restrict IP access via firewall</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#access-control","title":"Access Control","text":"<p>Restrict which Globus groups can access which APIs:</p> <pre><code>GLOBUS_GROUPS=\"allowed-group-uuid-1 allowed-group-uuid-2\"\n</code></pre>"},{"location":"admin-guide/inference-setup/direct-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/inference-setup/direct-api/#gateway-cant-fetch-status-manifest","title":"Gateway can't fetch status manifest","text":"<p>Check connectivity:</p> <pre><code>curl http://your-server:8055/status.json\n</code></pre> <p>Verify <code>METIS_STATUS_URL</code> is correct and accessible from the gateway.</p>"},{"location":"admin-guide/inference-setup/direct-api/#authentication-errors-from-provider","title":"Authentication errors from provider","text":"<ul> <li>Verify API key is correct</li> <li>Check key hasn't expired</li> <li>Ensure key has required permissions</li> <li>Check provider status page</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#model-not-found","title":"Model not found","text":"<p>Ensure the model name matches exactly what's in the <code>experts</code> array of your manifest.</p>"},{"location":"admin-guide/inference-setup/direct-api/#rate-limiting-errors","title":"Rate limiting errors","text":"<ul> <li>Check provider rate limits</li> <li>Implement gateway-side rate limiting</li> <li>Consider upgrading provider plan</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#example-adding-azure-openai","title":"Example: Adding Azure OpenAI","text":"<pre><code>{\n  \"azure-openai\": {\n    \"status\": \"Live\",\n    \"model\": \"Azure OpenAI\",\n    \"description\": \"GPT models via Azure OpenAI Service\",\n    \"experts\": [\n      \"gpt-4\",\n      \"gpt-35-turbo\"\n    ],\n    \"url\": \"https://your-resource.openai.azure.com/openai/deployments/your-deployment\",\n    \"endpoint_id\": \"azure-openai-prod\"\n  }\n}\n</code></pre> <p>Azure requires additional configuration - consult Azure OpenAI documentation.</p>"},{"location":"admin-guide/inference-setup/direct-api/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide - How to use the API</li> <li>Monitoring - Set up monitoring and alerts</li> <li>Local vLLM - Add local inference capabilities</li> <li>Production Best Practices</li> </ul>"},{"location":"admin-guide/inference-setup/direct-api/#additional-resources","title":"Additional Resources","text":"<ul> <li>OpenAI API Documentation</li> <li>Anthropic API Documentation</li> <li>Gateway Configuration Reference</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/","title":"Globus Compute + vLLM Setup","text":"<p>This guide shows you how to deploy vLLM on HPC clusters or remote servers using Globus Compute for federated inference.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#overview","title":"Overview","text":"<p>This is the recommended approach for:</p> <ul> <li>Multi-cluster federated deployments</li> <li>HPC environments with job schedulers (PBS, Slurm)</li> <li>Organizations requiring high availability</li> <li>Remote execution with secure authentication</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[User] --&gt;|Globus Token| B[FIRST Gateway]\n    B --&gt;|Globus Compute| C[HPC Cluster 1]\n    B --&gt;|Globus Compute| D[HPC Cluster 2]\n    B --&gt;|Globus Compute| E[Local Server]\n    C --&gt;|Job Scheduler| F[vLLM + Model]\n    D --&gt;|Job Scheduler| G[vLLM + Model]\n    E --&gt;|Direct| H[vLLM + Model]\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#prerequisites","title":"Prerequisites","text":"<ul> <li>FIRST Gateway deployed with Service Account application credentials</li> <li>Access to compute resources (HPC cluster or powerful workstation)</li> <li>GPU resources for running models</li> <li>Python 3.12+ (same version as gateway)</li> <li>Globus Compute SDK and Endpoint software</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#part-1-setup-on-compute-resource","title":"Part 1: Setup on Compute Resource","text":"<p>This is done on the machine(s) where models will run.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#step-1-create-python-environment","title":"Step 1: Create Python Environment","text":"<p>Version Match</p> <p>Use the same Python version as your gateway to avoid compatibility issues.</p> <pre><code># Using conda (recommended for HPC)\nconda create -n vllm-env python=3.12 -y\nconda activate vllm-env\n\n# OR using venv\npython3.12 -m venv vllm-env\nsource vllm-env/bin/activate\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-2-install-vllm","title":"Step 2: Install vLLM","text":"<pre><code># For CUDA 12.1 (default)\npip install vllm\n\n# For specific CUDA version\npip install vllm-cu118  # CUDA 11.8\n\n# From source (for latest features)\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-3-install-globus-compute","title":"Step 3: Install Globus Compute","text":"<pre><code>pip install globus-compute-sdk globus-compute-endpoint\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-4-export-service-account-credentials","title":"Step 4: Export Service Account Credentials","text":"<p>These are from your Gateway's Service Account application:</p> <pre><code>export GLOBUS_COMPUTE_CLIENT_ID=\"&lt;SERVICE_ACCOUNT_ID-from-gateway-.env&gt;\"\nexport GLOBUS_COMPUTE_CLIENT_SECRET=\"&lt;SERVICE_ACCOUNT_SECRET-from-gateway-.env&gt;\"\n</code></pre> <p>Add to your <code>~/.bashrc</code> or <code>~/.bash_profile</code> for persistence:</p> <pre><code>echo 'export GLOBUS_COMPUTE_CLIENT_ID=\"your-id\"' &gt;&gt; ~/.bashrc\necho 'export GLOBUS_COMPUTE_CLIENT_SECRET=\"your-secret\"' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-5-register-globus-compute-functions","title":"Step 5: Register Globus Compute Functions","text":"<p>Navigate to the gateway repository's compute-functions directory:</p> <pre><code>cd /path/to/inference-gateway/compute-functions\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#register-inference-function","title":"Register Inference Function","text":"<pre><code>python vllm_register_function_with_streaming.py\n</code></pre> <p>Output:</p> <pre><code>Function registered with UUID: 12345678-1234-1234-1234-123456789abc\nThe UUID is stored in vllm_register_function_streaming.txt\n</code></pre> <p>Save this Function UUID - you'll need it later.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#register-status-function-optional-but-recommended","title":"Register Status Function (Optional but Recommended)","text":"<p>For HPC clusters with job schedulers:</p> <pre><code>python qstat_register_function.py\n</code></pre> <p>Save the Function UUID from the output.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#register-batch-function-optional","title":"Register Batch Function (Optional)","text":"<p>For batch processing support:</p> <pre><code>python vllm_batch_function.py\n</code></pre> <p>Save the Function UUID.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#step-6-configure-globus-compute-endpoint","title":"Step 6: Configure Globus Compute Endpoint","text":"<p>Create a new endpoint:</p> <pre><code>globus-compute-endpoint configure my-inference-endpoint\n</code></pre> <p>This creates <code>~/.globus_compute/my-inference-endpoint/config.yaml</code>.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#for-localworkstation-deployment","title":"For Local/Workstation Deployment","text":"<p>Edit <code>config.yaml</code>:</p> <pre><code>display_name: My Inference Endpoint\nengine:\n  type: GlobusComputeEngine\n  provider:\n    type: LocalProvider\n    init_blocks: 1\n    max_blocks: 1\n    min_blocks: 0\n\n# Allow only your registered functions\nallowed_functions:\n  - 12345678-1234-1234-1234-123456789abc  # Your vLLM function UUID\n  - 87654321-4321-4321-4321-cba987654321  # Your qstat function UUID (if any)\n\n# Activate your environment before running functions\nworker_init: |\n  source /path/to/vllm-env/bin/activate\n  # OR: conda activate vllm-env\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#for-hpc-with-pbs-eg-alcf-sophia","title":"For HPC with PBS (e.g., ALCF Sophia)","text":"<pre><code>display_name: Sophia vLLM Endpoint\nengine:\n  type: GlobusComputeEngine\n  provider:\n    type: PBSProProvider\n    account: YourProjectAccount\n    queue: demand\n    nodes_per_block: 1\n    init_blocks: 1\n    max_blocks: 4\n    min_blocks: 0\n    walltime: \"06:00:00\"\n    scheduler_options: |\n      #PBS -l filesystems=home:eagle\n      #PBS -l place=scatter\n    worker_init: |\n      module load conda\n      conda activate /path/to/vllm-env\n      # OR: source /path/to/common_setup.sh\n\n# GPU allocation\nmax_workers_per_node: 1\n\n# Allow only your registered functions\nallowed_functions:\n  - 12345678-1234-1234-1234-123456789abc\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#for-hpc-with-slurm","title":"For HPC with Slurm","text":"<pre><code>display_name: Slurm vLLM Endpoint\nengine:\n  type: GlobusComputeEngine\n  provider:\n    type: SlurmProvider\n    account: your_account\n    partition: gpu\n    nodes_per_block: 1\n    init_blocks: 1\n    max_blocks: 4\n    walltime: \"06:00:00\"\n    scheduler_options: |\n      #SBATCH --gpus-per-node=1\n      #SBATCH --mem=64G\n    worker_init: |\n      source /path/to/vllm-env/bin/activate\n\nallowed_functions:\n  - 12345678-1234-1234-1234-123456789abc\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-7-start-the-endpoint","title":"Step 7: Start the Endpoint","text":"<pre><code>globus-compute-endpoint start my-inference-endpoint\n</code></pre> <p>Output:</p> <pre><code>Starting endpoint; registered ID: abcdef12-3456-7890-abcd-ef1234567890\n</code></pre> <p>Save this Endpoint UUID - you'll need it for gateway configuration.</p> <p>Verify it's running:</p> <pre><code>globus-compute-endpoint list\n</code></pre> <p>View logs:</p> <pre><code>globus-compute-endpoint log my-inference-endpoint\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#part-2-configure-gateway","title":"Part 2: Configure Gateway","text":"<p>Now configure the gateway to use your Globus Compute endpoint.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#step-1-create-endpoint-fixture","title":"Step 1: Create Endpoint Fixture","text":"<p>On your gateway machine, edit <code>fixtures/endpoints.json</code>:</p>"},{"location":"admin-guide/inference-setup/globus-compute/#single-endpoint-configuration","title":"Single Endpoint Configuration","text":"<pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"endpoint_slug\": \"sophia-vllm-llama3-8b\",\n            \"cluster\": \"sophia\",\n            \"framework\": \"vllm\",\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"api_port\": 8000,\n            \"endpoint_uuid\": \"abcdef12-3456-7890-abcd-ef1234567890\",\n            \"function_uuid\": \"12345678-1234-1234-1234-123456789abc\",\n            \"batch_endpoint_uuid\": \"\",\n            \"batch_function_uuid\": \"\",\n            \"allowed_globus_groups\": \"\"\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#multiple-clusters-configuration","title":"Multiple Clusters Configuration","text":"<pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"endpoint_slug\": \"sophia-vllm-llama3-8b\",\n            \"cluster\": \"sophia\",\n            \"framework\": \"vllm\",\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"api_port\": 8000,\n            \"endpoint_uuid\": \"sophia-endpoint-uuid\",\n            \"function_uuid\": \"sophia-function-uuid\",\n            \"allowed_globus_groups\": \"\"\n        }\n    },\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 2,\n        \"fields\": {\n            \"endpoint_slug\": \"polaris-vllm-llama3-70b\",\n            \"cluster\": \"polaris\",\n            \"framework\": \"vllm\",\n            \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n            \"api_port\": 8000,\n            \"endpoint_uuid\": \"polaris-endpoint-uuid\",\n            \"function_uuid\": \"polaris-function-uuid\",\n            \"allowed_globus_groups\": \"\"\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-2-federated-endpoints-optional-but-recommended","title":"Step 2: Federated Endpoints (Optional but Recommended)","text":"<p>For automatic load balancing and failover, use federated endpoints.</p> <p>Edit <code>fixtures/federated_endpoints.json</code>:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.federatedendpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"name\": \"Llama-3 8B (Federated)\",\n            \"slug\": \"federated-llama3-8b\",\n            \"target_model_name\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"description\": \"Federated access to Llama-3 8B across multiple clusters\",\n            \"targets\": [\n                {\n                    \"cluster\": \"sophia\",\n                    \"framework\": \"vllm\",\n                    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n                    \"endpoint_slug\": \"sophia-vllm-llama3-8b\",\n                    \"endpoint_uuid\": \"sophia-endpoint-uuid\",\n                    \"function_uuid\": \"sophia-function-uuid\",\n                    \"api_port\": 8000\n                },\n                {\n                    \"cluster\": \"polaris\",\n                    \"framework\": \"vllm\",\n                    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n                    \"endpoint_slug\": \"polaris-vllm-llama3-8b\",\n                    \"endpoint_uuid\": \"polaris-endpoint-uuid\",\n                    \"function_uuid\": \"polaris-function-uuid\",\n                    \"api_port\": 8000\n                }\n            ]\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#step-3-load-fixtures","title":"Step 3: Load Fixtures","text":"<pre><code># Docker\ndocker-compose exec inference-gateway python manage.py loaddata fixtures/endpoints.json\ndocker-compose exec inference-gateway python manage.py loaddata fixtures/federated_endpoints.json\n\n# Bare Metal\npython manage.py loaddata fixtures/endpoints.json\npython manage.py loaddata fixtures/federated_endpoints.json\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#part-3-testing","title":"Part 3: Testing","text":""},{"location":"admin-guide/inference-setup/globus-compute/#test-globus-compute-endpoint","title":"Test Globus Compute Endpoint","text":"<p>From your gateway machine, test that you can reach the endpoint:</p> <pre><code>from globus_compute_sdk import Client\n\ngcc = Client()\nendpoint_id = \"abcdef12-3456-7890-abcd-ef1234567890\"\nfunction_id = \"12345678-1234-1234-1234-123456789abc\"\n\n# Simple test function\ndef hello():\n    return \"Hello from Globus Compute!\"\n\n# Register and run\nfunc_uuid = gcc.register_function(hello)\ntask = gcc.run(endpoint_id=endpoint_id, function_id=func_uuid)\nprint(gcc.get_result(task))\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#test-vllm-via-gateway","title":"Test vLLM via Gateway","text":"<p>Get a Globus token:</p> <pre><code>export TOKEN=$(python inference-auth-token.py get_access_token)\n</code></pre> <p>Test non-federated endpoint:</p> <pre><code>curl -X POST http://localhost:8000/resource_server/sophia/vllm/v1/chat/completions \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is Globus Compute?\"}],\n    \"max_tokens\": 100\n  }'\n</code></pre> <p>Test federated endpoint:</p> <pre><code>curl -X POST http://localhost:8000/resource_server/v1/chat/completions \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What is Globus Compute?\"}],\n    \"max_tokens\": 100\n  }'\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"admin-guide/inference-setup/globus-compute/#hot-nodes-keep-alive","title":"Hot Nodes / Keep-Alive","text":"<p>For low-latency inference, keep compute nodes warm:</p> <pre><code># In config.yaml\nengine:\n  provider:\n    min_blocks: 1  # Always keep 1 node running\n    init_blocks: 2  # Start with 2 nodes\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#multi-node-vllm","title":"Multi-Node vLLM","text":"<p>For very large models (70B+), use tensor parallelism across multiple GPUs/nodes:</p> <p>See <code>compute-endpoints/sophia-vllm-multinode-example.yaml</code> for configuration examples.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#custom-inference-scripts","title":"Custom Inference Scripts","text":"<p>The registered functions use scripts in <code>compute-functions/</code>. You can customize:</p> <ul> <li><code>launch_vllm_model.sh</code>: How vLLM is started</li> <li><code>vllm_register_function_with_streaming.py</code>: The inference function logic</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#batch-processing","title":"Batch Processing","text":"<p>Enable batch processing by registering and configuring batch functions:</p> <pre><code>python vllm_batch_function.py\n</code></pre> <p>Add batch UUIDs to your endpoint fixture:</p> <pre><code>{\n    \"batch_endpoint_uuid\": \"batch-endpoint-uuid\",\n    \"batch_function_uuid\": \"batch-function-uuid\"\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/inference-setup/globus-compute/#endpoint-status","title":"Endpoint Status","text":"<pre><code># Check endpoint status\nglobus-compute-endpoint status my-inference-endpoint\n\n# View logs\nglobus-compute-endpoint log my-inference-endpoint -n 50\n\n# Follow logs\ntail -f ~/.globus_compute/my-inference-endpoint/endpoint.log\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#job-scheduler-monitoring","title":"Job Scheduler Monitoring","text":"<pre><code># PBS\nqstat -u $USER\n\n# Slurm\nsqueue -u $USER\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#gateway-logs","title":"Gateway Logs","text":"<pre><code># Docker\ndocker-compose logs -f inference-gateway\n\n# Bare Metal\ntail -f logs/django_info.log\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/inference-setup/globus-compute/#endpoint-wont-start","title":"Endpoint won't start","text":"<p>Check logs:</p> <pre><code>globus-compute-endpoint log my-inference-endpoint\n</code></pre> <p>Common issues:</p> <ul> <li>Service account credentials not set</li> <li>Function UUIDs not in <code>allowed_functions</code></li> <li>Python environment activation fails</li> <li>Job scheduler configuration incorrect</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#function-execution-fails","title":"Function execution fails","text":"<ul> <li>Verify environment can run vLLM: Test manually on compute node</li> <li>Check function is allowed in <code>config.yaml</code></li> <li>Ensure Service Account has permissions</li> <li>Check job scheduler logs</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#model-not-loading","title":"Model not loading","text":"<ul> <li>Verify GPU allocation in scheduler options</li> <li>Check VRAM requirements</li> <li>Ensure model path/name is correct</li> <li>Check Hugging Face token if needed</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#gateway-cant-connect","title":"Gateway can't connect","text":"<ul> <li>Verify endpoint is running: <code>globus-compute-endpoint list</code></li> <li>Check endpoint UUID in fixtures is correct</li> <li>Ensure Service Account credentials match in both places</li> <li>Check network connectivity (firewall, VPN)</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#hpc-specific-considerations","title":"HPC-Specific Considerations","text":""},{"location":"admin-guide/inference-setup/globus-compute/#alcf-systems-polaris-sophia","title":"ALCF Systems (Polaris, Sophia)","text":"<ul> <li>Use <code>/lus/eagle</code> for model cache</li> <li>Load CUDA modules in <code>worker_init</code></li> <li>Set appropriate walltime</li> <li>Use <code>#PBS -l filesystems=home:eagle</code></li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#other-hpc-centers","title":"Other HPC Centers","text":"<ul> <li>Check local documentation for:</li> <li>GPU allocation syntax</li> <li>File system paths</li> <li>Module system</li> <li>Queue/partition names</li> <li>Accounting project codes</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#scaling","title":"Scaling","text":""},{"location":"admin-guide/inference-setup/globus-compute/#auto-scaling","title":"Auto-Scaling","text":"<p>Configure <code>max_blocks</code> to allow automatic scaling:</p> <pre><code>engine:\n  provider:\n    init_blocks: 1\n    min_blocks: 0\n    max_blocks: 10  # Scale up to 10 nodes\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#load-balancing","title":"Load Balancing","text":"<p>Federated endpoints automatically load balance across all \"Live\" targets.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#geographic-distribution","title":"Geographic Distribution","text":"<p>Deploy endpoints at different locations and configure federated routing for optimal latency.</p>"},{"location":"admin-guide/inference-setup/globus-compute/#security","title":"Security","text":""},{"location":"admin-guide/inference-setup/globus-compute/#function-allowlisting","title":"Function Allowlisting","text":"<p>Always specify <code>allowed_functions</code> in <code>config.yaml</code>:</p> <pre><code>allowed_functions:\n  - 12345678-1234-1234-1234-123456789abc  # Only your functions\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#group-based-access","title":"Group-Based Access","text":"<p>Restrict endpoints to specific Globus groups:</p> <pre><code>{\n    \"allowed_globus_groups\": \"group-uuid-1,group-uuid-2\"\n}\n</code></pre>"},{"location":"admin-guide/inference-setup/globus-compute/#network-security","title":"Network Security","text":"<ul> <li>Globus Compute uses HTTPS and mutual TLS</li> <li>No inbound ports need to be opened on compute resources</li> <li>All communication is outbound from endpoint to Globus services</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#next-steps","title":"Next Steps","text":"<ul> <li>Production Best Practices</li> <li>Monitoring &amp; Troubleshooting</li> <li>User Guide</li> </ul>"},{"location":"admin-guide/inference-setup/globus-compute/#additional-resources","title":"Additional Resources","text":"<ul> <li>Globus Compute Documentation</li> <li>vLLM Documentation</li> <li>Example Configurations</li> <li>Function Registration Scripts</li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/","title":"Local vLLM Setup","text":"<p>This guide shows you how to run vLLM inference server locally and connect it to the FIRST Gateway without Globus Compute.</p>"},{"location":"admin-guide/inference-setup/local-vllm/#overview","title":"Overview","text":"<p>This setup is ideal for:</p> <ul> <li>Single-server deployments</li> <li>Development and testing</li> <li>Scenarios where Globus Compute overhead isn't needed</li> <li>Direct control over the inference process</li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[User] --&gt;|Globus Token| B[FIRST Gateway]\n    B --&gt;|HTTP| C[vLLM Server]\n    C --&gt;|GPU| D[Model]\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA GPU with sufficient VRAM for your model</li> <li>CUDA 11.8 or later</li> <li>Python 3.12+</li> <li>Docker (optional, for containerized vLLM)</li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/#step-1-install-vllm","title":"Step 1: Install vLLM","text":""},{"location":"admin-guide/inference-setup/local-vllm/#option-a-install-from-source-recommended","title":"Option A: Install from Source (Recommended)","text":"<pre><code># Create virtual environment\npython3.12 -m venv vllm-env\nsource vllm-env/bin/activate\n\n# Clone and install vLLM\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n\n# Install additional dependencies\npip install openai  # For testing\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#option-b-install-via-pip","title":"Option B: Install via pip","text":"<pre><code>python3.12 -m venv vllm-env\nsource vllm-env/bin/activate\n\npip install vllm\n\n# For specific CUDA version\npip install vllm  # CUDA 12.1 by default\n# OR\npip install vllm-cu118  # For CUDA 11.8\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#option-c-use-docker","title":"Option C: Use Docker","text":"<pre><code>docker pull vllm/vllm-openai:latest\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-2-download-a-model","title":"Step 2: Download a Model","text":"<p>Choose a model based on your GPU VRAM:</p> Model VRAM Required Performance facebook/opt-125m ~1GB Fast, good for testing meta-llama/Llama-2-7b-chat-hf ~14GB Good quality meta-llama/Meta-Llama-3-8B-Instruct ~16GB Better quality meta-llama/Llama-2-13b-chat-hf ~26GB High quality meta-llama/Llama-2-70b-chat-hf ~140GB Best quality (multi-GPU)"},{"location":"admin-guide/inference-setup/local-vllm/#using-hugging-face","title":"Using Hugging Face","text":"<pre><code># Login to Hugging Face (for gated models like Llama)\npip install huggingface-hub\nhuggingface-cli login\n\n# Models will auto-download on first use\n# Or pre-download:\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-3-start-vllm-server","title":"Step 3: Start vLLM Server","text":""},{"location":"admin-guide/inference-setup/local-vllm/#basic-start","title":"Basic Start","text":"<pre><code>source vllm-env/bin/activate\n\npython -m vllm.entrypoints.openai.api_server \\\n    --model facebook/opt-125m \\\n    --host 0.0.0.0 \\\n    --port 8001\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#production-configuration","title":"Production Configuration","text":"<pre><code>python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 8001 \\\n    --tensor-parallel-size 1 \\\n    --gpu-memory-utilization 0.9 \\\n    --max-model-len 4096 \\\n    --dtype auto \\\n    --enable-prefix-caching\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":"<pre><code># For 4 GPUs\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-2-70b-chat-hf \\\n    --host 0.0.0.0 \\\n    --port 8001 \\\n    --tensor-parallel-size 4 \\\n    --gpu-memory-utilization 0.9\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#using-docker","title":"Using Docker","text":"<pre><code>docker run --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8001:8000 \\\n    --env \"HUGGING_FACE_HUB_TOKEN=&lt;your_token&gt;\" \\\n    vllm/vllm-openai:latest \\\n    --model facebook/opt-125m \\\n    --host 0.0.0.0 \\\n    --port 8000\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-4-test-vllm-server","title":"Step 4: Test vLLM Server","text":"<pre><code>curl http://localhost:8001/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"facebook/opt-125m\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello!\"}\n        ]\n    }'\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-5-create-systemd-service-optional","title":"Step 5: Create systemd Service (Optional)","text":"<p>For production deployments, run vLLM as a system service:</p> <pre><code>sudo nano /etc/systemd/system/vllm.service\n</code></pre> <p>Add:</p> <pre><code>[Unit]\nDescription=vLLM Inference Server\nAfter=network.target\n\n[Service]\nType=simple\nUser=your-username\nWorkingDirectory=/home/your-username\nEnvironment=\"PATH=/home/your-username/vllm-env/bin\"\nExecStart=/home/your-username/vllm-env/bin/python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 8001 \\\n    --tensor-parallel-size 1 \\\n    --gpu-memory-utilization 0.9\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable vllm\nsudo systemctl start vllm\nsudo systemctl status vllm\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-6-configure-gateway","title":"Step 6: Configure Gateway","text":""},{"location":"admin-guide/inference-setup/local-vllm/#update-gateway-environment","title":"Update Gateway Environment","text":"<p>Edit your gateway's <code>.env</code>:</p> <pre><code># Add if using local vLLM without Globus Compute\nLOCAL_VLLM_URL=\"http://localhost:8001\"\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#create-endpoint-fixture","title":"Create Endpoint Fixture","text":"<p>Create or edit <code>fixtures/endpoints.json</code> in your gateway directory:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"endpoint_slug\": \"local-vllm-opt-125m\",\n            \"cluster\": \"local\",\n            \"framework\": \"vllm\",\n            \"model\": \"facebook/opt-125m\",\n            \"api_port\": 8001,\n            \"endpoint_uuid\": \"\",\n            \"function_uuid\": \"\",\n            \"batch_endpoint_uuid\": \"\",\n            \"batch_function_uuid\": \"\",\n            \"allowed_globus_groups\": \"\"\n        }\n    }\n]\n</code></pre> <p>For a Llama model:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 2,\n        \"fields\": {\n            \"endpoint_slug\": \"local-vllm-llama3-8b\",\n            \"cluster\": \"local\",\n            \"framework\": \"vllm\",\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"api_port\": 8001,\n            \"endpoint_uuid\": \"\",\n            \"function_uuid\": \"\",\n            \"batch_endpoint_uuid\": \"\",\n            \"batch_function_uuid\": \"\",\n            \"allowed_globus_groups\": \"\"\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#load-fixture","title":"Load Fixture","text":"<pre><code># Docker\ndocker-compose exec inference-gateway python manage.py loaddata fixtures/endpoints.json\n\n# Bare metal\npython manage.py loaddata fixtures/endpoints.json\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#step-7-test-end-to-end","title":"Step 7: Test End-to-End","text":"<p>Get a Globus token:</p> <pre><code>export TOKEN=$(python inference-auth-token.py get_access_token)\n</code></pre> <p>Test via gateway:</p> <pre><code>curl -X POST http://localhost:8000/resource_server/local/vllm/v1/chat/completions \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"facebook/opt-125m\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning in one sentence\"}],\n    \"max_tokens\": 50\n  }'\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#performance-tuning","title":"Performance Tuning","text":""},{"location":"admin-guide/inference-setup/local-vllm/#gpu-memory-optimization","title":"GPU Memory Optimization","text":"<pre><code># Use less GPU memory (if OOM errors)\n--gpu-memory-utilization 0.8\n\n# Use more GPU memory (if you have headroom)\n--gpu-memory-utilization 0.95\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#batch-processing","title":"Batch Processing","text":"<pre><code># Increase batch size for throughput\n--max-num-batched-tokens 8192\n--max-num-seqs 256\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#context-length","title":"Context Length","text":"<pre><code># Reduce for better throughput\n--max-model-len 2048\n\n# Increase for longer contexts\n--max-model-len 8192\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#quantization","title":"Quantization","text":"<pre><code># Use 4-bit quantization (AWQ)\n--quantization awq\n--model TheBloke/Llama-2-7B-Chat-AWQ\n\n# Use 8-bit quantization (GPTQ)\n--quantization gptq\n--model TheBloke/Llama-2-7B-Chat-GPTQ\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/inference-setup/local-vllm/#vllm-metrics","title":"vLLM Metrics","text":"<p>vLLM exposes Prometheus metrics at <code>http://localhost:8001/metrics</code>:</p> <pre><code>curl http://localhost:8001/metrics\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#gpu-monitoring","title":"GPU Monitoring","text":"<pre><code># Watch GPU usage\nwatch -n 1 nvidia-smi\n\n# More detailed stats\nnvidia-smi dmon\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#log-monitoring","title":"Log Monitoring","text":"<pre><code># systemd service logs\nsudo journalctl -u vllm -f\n\n# Or direct output if running in terminal\npython -m vllm.entrypoints.openai.api_server ... | tee vllm.log\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/inference-setup/local-vllm/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<pre><code># Reduce GPU memory usage\n--gpu-memory-utilization 0.7\n\n# Reduce context length\n--max-model-len 2048\n\n# Use quantization\n--quantization awq\n\n# Use smaller model\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#slow-response-times","title":"Slow Response Times","text":"<ul> <li>Check GPU utilization with <code>nvidia-smi</code></li> <li>Increase <code>--gpu-memory-utilization</code> if GPU memory is underutilized</li> <li>Enable <code>--enable-prefix-caching</code> for repeated prompts</li> <li>Use tensor parallelism for multi-GPU setups</li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/#model-not-found","title":"Model Not Found","text":"<pre><code># Pre-download model\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct\n\n# Or set cache directory\nexport HF_HOME=/path/to/cache\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#cuda-errors","title":"CUDA Errors","text":"<pre><code># Check CUDA version\nnvidia-smi\n\n# Install matching vLLM version\npip install vllm-cu118  # For CUDA 11.8\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#connection-refused-from-gateway","title":"Connection Refused from Gateway","text":"<ul> <li>Verify vLLM is running: <code>curl http://localhost:8001/health</code></li> <li>Check firewall settings</li> <li>Ensure correct port in fixture configuration</li> <li>Verify host is <code>0.0.0.0</code> not <code>localhost</code></li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/#running-multiple-models","title":"Running Multiple Models","text":"<p>You can run multiple vLLM instances on different ports:</p> <pre><code># Terminal 1 - OPT-125M\npython -m vllm.entrypoints.openai.api_server \\\n    --model facebook/opt-125m \\\n    --port 8001\n\n# Terminal 2 - Llama-2-7B\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-2-7b-chat-hf \\\n    --port 8002\n</code></pre> <p>Add both to your fixtures:</p> <pre><code>[\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 1,\n        \"fields\": {\n            \"endpoint_slug\": \"local-vllm-opt-125m\",\n            \"cluster\": \"local\",\n            \"framework\": \"vllm\",\n            \"model\": \"facebook/opt-125m\",\n            \"api_port\": 8001,\n            ...\n        }\n    },\n    {\n        \"model\": \"resource_server.endpoint\",\n        \"pk\": 2,\n        \"fields\": {\n            \"endpoint_slug\": \"local-vllm-llama2-7b\",\n            \"cluster\": \"local\",\n            \"framework\": \"vllm\",\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n            \"api_port\": 8002,\n            ...\n        }\n    }\n]\n</code></pre>"},{"location":"admin-guide/inference-setup/local-vllm/#next-steps","title":"Next Steps","text":"<ul> <li>Production Best Practices</li> <li>Monitoring Setup</li> <li>User Guide</li> <li>Upgrade to Globus Compute + vLLM for federated deployment</li> </ul>"},{"location":"admin-guide/inference-setup/local-vllm/#additional-resources","title":"Additional Resources","text":"<ul> <li>vLLM Documentation</li> <li>Hugging Face Models</li> <li>NVIDIA GPU Documentation</li> </ul>"},{"location":"reference/api/","title":"API Reference","text":"<p>The FIRST Inference Gateway provides an OpenAI-compatible API.</p>"},{"location":"reference/api/#base-url","title":"Base URL","text":"<pre><code>http://your-gateway-domain:8000/resource_server\n</code></pre>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>All requests require a Globus access token in the Authorization header:</p> <pre><code>Authorization: Bearer &lt;globus-access-token&gt;\n</code></pre>"},{"location":"reference/api/#endpoints","title":"Endpoints","text":""},{"location":"reference/api/#chat-completions","title":"Chat Completions","text":"<pre><code>POST /v1/chat/completions\nPOST /{cluster}/{framework}/v1/chat/completions\n</code></pre>"},{"location":"reference/api/#completions","title":"Completions","text":"<pre><code>POST /v1/completions\nPOST /{cluster}/{framework}/v1/completions\n</code></pre>"},{"location":"reference/api/#batch-processing","title":"Batch Processing","text":"<pre><code>POST /v1/batches\nGET /v1/batches/{batch_id}\n</code></pre> <p>For detailed API documentation, refer to the OpenAI API Reference as FIRST follows the same schema.</p>"},{"location":"reference/api/#request-parameters","title":"Request Parameters","text":"<p>See the User Guide for detailed parameter documentation.</p>"},{"location":"reference/citation/","title":"Citation","text":"<p>If you use ALCF Inference Endpoints or the Federated Inference Resource Scheduling Toolkit (FIRST) in your research or workflows, please cite our paper:</p>"},{"location":"reference/citation/#bibtex","title":"BibTeX","text":"<pre><code>@inproceedings{10.1145/3731599.3767346,\n  author = {Tanikanti, Aditya and C\\^{o}t\\'{e}, Benoit and Guo, Yanfei and Chen, Le and Saint, Nickolaus and Chard, Ryan and Raffenetti, Ken and Thakur, Rajeev and Uram, Thomas and Foster, Ian and Papka, Michael E. and Vishwanath, Venkatram},\n  title = {FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access},\n  year = {2025},\n  isbn = {9798400718717},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3731599.3767346},\n  doi = {10.1145/3731599.3767346},\n  abstract = {We present the Federated Inference Resource Scheduling Toolkit (FIRST), a framework enabling Inference-as-a-Service across distributed High-Performance Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI models, like Large Language Models (LLMs), on existing HPC infrastructure. Leveraging Globus Auth and Globus Compute, the system allows researchers to run parallel inference workloads via an OpenAI-compliant API on private, secure environments. This cluster-agnostic API allows requests to be distributed across federated clusters, targeting numerous hosted models. FIRST supports multiple inference backends (e.g., vLLM), auto-scales resources, maintains \"hot\" nodes for low-latency execution, and offers both high-throughput batch and interactive modes. The framework addresses the growing demand for private, secure, and scalable AI inference in scientific workflows, allowing researchers to generate billions of tokens daily on-premises without relying on commercial cloud infrastructure.},\n  booktitle = {Proceedings of the SC '25 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis},\n  pages = {52\u201360},\n  numpages = {9},\n  keywords = {Inference as a Service, High Performance Computing, Job Schedulers, Large Language Models, Globus, Scientific Computing},\n  series = {SC Workshops '25}\n}\n</code></pre>"},{"location":"reference/citation/#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, under Contract No. DE-AC02-06CH11357.</p> <p>This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility.</p>"},{"location":"reference/citation/#related-publications","title":"Related Publications","text":"<p>For more information about FIRST and related work:</p> <ul> <li>Research Paper</li> <li>ALCF Inference Endpoints</li> <li>GitHub Repository</li> </ul>"},{"location":"reference/config/","title":"Configuration Options","text":"<p>For a complete list of configuration options, see the Configuration Reference.</p>"},{"location":"user-guide/","title":"Usage Guide","text":"<p>This guide explains how to use the FIRST Inference Gateway to access Large Language Models through an OpenAI-compatible API.</p>"},{"location":"user-guide/#authentication","title":"Authentication","text":"<p>The Gateway uses Globus Auth for authentication. You need a valid access token to make requests.</p>"},{"location":"user-guide/#getting-your-access-token","title":"Getting Your Access Token","text":"<p>Use the provided authentication script:</p>"},{"location":"user-guide/#first-time-setup","title":"First-Time Setup","text":"<pre><code># Authenticate (opens browser for Globus login)\npython inference-auth-token.py authenticate\n</code></pre> <p>This stores refresh and access tokens locally (typically in <code>~/.globus/app/...</code>).</p>"},{"location":"user-guide/#getting-an-access-token","title":"Getting an Access Token","text":"<pre><code># Retrieve your current valid access token\nexport MY_TOKEN=$(python inference-auth-token.py get_access_token)\necho \"Token stored in MY_TOKEN environment variable.\"\n</code></pre> <p>The script automatically refreshes expired tokens using your stored refresh token.</p>"},{"location":"user-guide/#force-re-authentication","title":"Force Re-authentication","text":"<p>If you need to change accounts or encounter permission errors:</p> <pre><code># Log out from Globus\n# Visit: https://app.globus.org/logout\n\n# Force re-authentication\npython inference-auth-token.py authenticate --force\n</code></pre>"},{"location":"user-guide/#token-validity","title":"Token Validity","text":"<ul> <li>Access tokens: Valid for 48 hours</li> <li>Refresh tokens: Valid for 6 months of inactivity</li> <li>Some institutions may enforce more frequent re-authentication (e.g., weekly)</li> </ul>"},{"location":"user-guide/#making-inference-requests","title":"Making Inference Requests","text":"<p>The Gateway provides an OpenAI-compatible API with two main endpoints:</p>"},{"location":"user-guide/#chat-completions-endpoint","title":"Chat Completions Endpoint","text":"<p>For conversational interactions:</p> <p>Federated endpoint (routes across multiple backends):</p> <pre><code>curl -X POST http://127.0.0.1:8000/resource_server/v1/chat/completions \\\n  -H \"Authorization: Bearer $MY_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"facebook/opt-125m\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Explain the concept of Globus Compute in simple terms.\"}\n    ],\n    \"max_tokens\": 150\n  }'\n</code></pre> <p>Specific backend (targets a particular cluster/framework):</p> <pre><code>curl -X POST http://127.0.0.1:8000/resource_server/local/vllm/v1/chat/completions \\\n  -H \"Authorization: Bearer $MY_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"facebook/opt-125m\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Explain the concept of Globus Compute in simple terms.\"}\n    ],\n    \"max_tokens\": 150\n  }'\n</code></pre>"},{"location":"user-guide/#completions-endpoint","title":"Completions Endpoint","text":"<p>For text completion:</p> <pre><code>curl -X POST http://127.0.0.1:8000/resource_server/v1/completions \\\n  -H \"Authorization: Bearer $MY_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"facebook/opt-125m\",\n    \"prompt\": \"The future of AI is\",\n    \"max_tokens\": 100\n  }'\n</code></pre>"},{"location":"user-guide/#streaming-responses","title":"Streaming Responses","text":"<p>Both endpoints support streaming. Set <code>stream: true</code> in your request:</p> <p>Python example with streaming:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8000/resource_server/v1\",\n    api_key=MY_TOKEN  # Your Globus access token\n)\n\nstream = client.chat.completions.create(\n    model=\"facebook/opt-125m\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"user-guide/#testing-streaming-with-ngrok","title":"Testing Streaming with ngrok","text":"<p>For testing streaming with remote endpoints, use ngrok to create a secure tunnel:</p> <ol> <li> <p>Install ngrok: Visit ngrok.com</p> </li> <li> <p>Start tunnel: <pre><code>ngrok http 8000\n</code></pre></p> </li> <li> <p>Update your test script to use the ngrok URL: <pre><code>client = openai.OpenAI(\n    base_url=\"https://your-ngrok-url.ngrok.io/resource_server/v1\",\n    api_key=access_token\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/#using-the-openai-python-sdk","title":"Using the OpenAI Python SDK","text":"<p>The Gateway is fully compatible with the OpenAI Python SDK:</p> <pre><code>import openai\n\n# Configure client to point to the Gateway\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8000/resource_server/v1\",\n    api_key=MY_TOKEN  # Your Globus access token\n)\n\n# Make a request\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n    ],\n    max_tokens=200\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"user-guide/#available-models","title":"Available Models","text":"<p>To see available models, contact your Gateway administrator or check the deployment's model catalog.</p> <p>Common model naming conventions: - <code>facebook/opt-125m</code>, <code>facebook/opt-1.3b</code>, etc. - <code>meta-llama/Meta-Llama-3-8B-Instruct</code> - <code>openai/gpt-4o-mini</code> (if configured with OpenAI backend)</p> <p>The exact models available depend on your deployment's configuration.</p>"},{"location":"user-guide/#batch-processing","title":"Batch Processing","text":"<p>For large-scale inference workloads, the Gateway supports batch processing:</p> <pre><code>curl -X POST http://127.0.0.1:8000/resource_server/v1/batches \\\n  -H \"Authorization: Bearer $MY_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input_file_path\": \"/path/to/input.jsonl\",\n    \"output_folder_path\": \"/path/to/output/\",\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\"\n  }'\n</code></pre> <p>Input file format (JSONL):</p> <pre><code>{\"messages\": [{\"role\": \"user\", \"content\": \"What is AI?\"}], \"max_tokens\": 100}\n{\"messages\": [{\"role\": \"user\", \"content\": \"Explain ML\"}], \"max_tokens\": 100}\n</code></pre> <p>Check batch status:</p> <pre><code>curl -X GET http://127.0.0.1:8000/resource_server/v1/batches/&lt;batch_id&gt; \\\n  -H \"Authorization: Bearer $MY_TOKEN\"\n</code></pre>"},{"location":"user-guide/#benchmarking","title":"Benchmarking","text":"<p>The repository includes a benchmark script for performance testing:</p>"},{"location":"user-guide/#download-dataset","title":"Download Dataset","text":"<pre><code>wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json -P examples/load-testing/\n</code></pre>"},{"location":"user-guide/#run-benchmark","title":"Run Benchmark","text":"<pre><code>python examples/load-testing/benchmark-serving.py \\\n    --backend vllm \\\n    --model facebook/opt-125m \\\n    --base-url http://127.0.0.1:8000/resource_server/v1/chat/completions \\\n    --dataset-name sharegpt \\\n    --dataset-path examples/load-testing/ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --output-file benchmark_results.jsonl \\\n    --num-prompts 100\n</code></pre> <p>Additional options: - <code>--request-rate</code>: Control request rate (requests per second) - <code>--max-concurrency</code>: Limit concurrent requests - <code>--disable-ssl-verification</code>: For testing with self-signed certificates - <code>--disable-stream</code>: Test non-streaming mode</p>"},{"location":"user-guide/#request-parameters","title":"Request Parameters","text":""},{"location":"user-guide/#common-parameters","title":"Common Parameters","text":"Parameter Type Description <code>model</code> string Model identifier (required) <code>messages</code> array List of message objects (chat endpoint) <code>prompt</code> string Text prompt (completions endpoint) <code>max_tokens</code> integer Maximum tokens to generate <code>temperature</code> float Sampling temperature (0.0-2.0) <code>top_p</code> float Nucleus sampling parameter <code>stream</code> boolean Enable streaming responses <code>stop</code> string/array Stop sequences"},{"location":"user-guide/#example-with-advanced-parameters","title":"Example with Advanced Parameters","text":"<pre><code>curl -X POST http://127.0.0.1:8000/resource_server/v1/chat/completions \\\n  -H \"Authorization: Bearer $MY_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a story\"}],\n    \"max_tokens\": 500,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"stream\": false,\n    \"stop\": [\"\\n\\n\"]\n  }'\n</code></pre>"},{"location":"user-guide/#error-handling","title":"Error Handling","text":"<p>The Gateway returns standard HTTP status codes:</p> Code Meaning 200 Success 400 Bad Request (invalid parameters) 401 Unauthorized (invalid or missing token) 403 Forbidden (insufficient permissions) 404 Not Found (model or endpoint not available) 429 Too Many Requests (rate limited) 500 Internal Server Error 503 Service Unavailable (backend offline) <p>Error response format:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Model not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Token Management</li> <li>Store tokens securely</li> <li>Refresh tokens before they expire</li> <li> <p>Never commit tokens to version control</p> </li> <li> <p>Request Optimization</p> </li> <li>Use appropriate <code>max_tokens</code> values</li> <li>Implement retry logic with exponential backoff</li> <li> <p>Use streaming for long responses</p> </li> <li> <p>Rate Limiting</p> </li> <li>Respect rate limits</li> <li>Implement client-side throttling</li> <li> <p>Use batch processing for bulk operations</p> </li> <li> <p>Error Handling</p> </li> <li>Handle network errors gracefully</li> <li>Implement timeout logic</li> <li>Log errors for debugging</li> </ol>"},{"location":"user-guide/#support","title":"Support","text":"<p>For issues, questions, or feature requests: - Check the GitHub repository - Contact your Gateway administrator - Review the Administrator Setup Guide for deployment issues</p>"}]}