amqp_port: 443
display_name: sophia-vllm-llama4-scout-toolcalling-example
engine:
  type: GlobusComputeEngine
  max_retries_on_system_failure: 2
  max_workers_per_node: 100
  job_status_kwargs:
    max_idletime: 86400
  address:
    type: address_by_interface
    ifname: ens10f0
  provider:
    type: PBSProProvider
    launcher:
      type: SimpleLauncher
    account: argonne_tpc
    select_options: ngpus=8
    scheduler_options: '#PBS -l filesystems=home:eagle'
    queue: 'by-node'
    init_blocks: 0
    max_blocks: 2
    min_blocks: 0
    nodes_per_block: 1
    walltime: 24:00:00
    worker_init: |
      # Single-node with tool calling support (Llama 4 models)
      source /home/openinference_svc/launch_vllm_model.sh \
        --model-name meta-llama/Llama-4-Scout-17B-16E-Instruct \
        --vllm-version v0.11.0 \
        --tensor-parallel 8 \
        --max-model-len 32768 \
        --max-num-seqs 8 \
        --trust-remote-code \
        --enable-auto-tool-choice \
        --tool-call-parser llama4_pythonic \
        --chat-template /eagle/argonne_tpc/model_weights/chat-templates/tool_chat_template_llama4_pythonic.jinja \
        --disable-log-requests \
        --gpu-memory-util 0.95 \
        --framework vllm \
        --cluster sophia

allowed_functions:
  - $VLLM_FUNCTION_UUID

