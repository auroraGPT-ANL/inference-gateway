{% raw -%}
    worker_init: |
      # single_model_launch.sh
      # Source the common script
      source /home/openinference_svc/sophia_common_scripts.sh   # Replace with the actual path to common.sh
      # Setup the environment
      setup_environment
      # Define model parameters
      model_name="Qwen/Qwen2-VL-72B-Instruct"
      cluster="sophia"
      framework="vllm"
      model_command="vllm serve ${model_name} --host 127.0.0.1 --port 8000 \
      --tensor-parallel-size 8 --gpu-memory-utilization 0.95 \
      --disable-log-requests  \
      --multi-step-stream-outputs False \
      --ssl-keyfile ~/certificates/mykey.key --ssl-certfile ~/certificates/mycert.crt"
      log_file="$PWD/logfile_sophia_vllm_${model_name}_$(hostname).log"


      # Initialize retry counter for the model
      retry_counter_model_1=0

      # Start the model
      while true; do
        echo "Starting models sequence..."
        if ! start_model "$model_name" "$model_command" "$log_file" retry_counter_model_1; then
            continue  # Restart from the beginning if this fails
        fi
        echo "All models started successfully."
        break
      done
{% endraw %}