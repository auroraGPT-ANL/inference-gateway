amqp_port: 443
display_name: llama-cpp-python
engine:
 provider:
  init_blocks: 0
  max_blocks: 2
  min_blocks: 0
  nodes_per_block: 1
  type: PBSProProvider
  launcher:
   type: SimpleLauncher
  account: DataServicePrototype
  cpus_per_node: 64
  select_options: ngpus=4
  queue: 'debug'
  walltime: 00:60:00
  scheduler_options: "#PBS -l filesystems=home:eagle"
  worker_init: 'module use /soft/modulefiles; module load conda; conda activate /home/openinference_svc/envs/llama-cpp-cuda-env/; nohup /home/openinference_svc/frameworks/llama.cpp/build/bin/server -m /eagle/argonne_tpc/model_weights/gguf_files/Mistral-7B-Instruct-v0.3-Q6_K.gguf --batch-size 2048 --ubatch-size 512 --ctx-size 2048 --flash-attn --cont-batching --numa distribute --n-gpu-layers 4 --main-gpu 0 --split-mode layer --parallel 4 --log-format json --metrics --threads 32 --threads-batch 16 > /home/openinference_svc/logfile.log 2>&1 & sleep 20'
 max_workers_per_node: 1
 job_status_kwargs:
  max_idletime: 3000
 address:
  type: address_by_interface
  ifname: bond0
 type: GlobusComputeEngine

allowed_functions:
  - f5129f81-ab07-4910-8391-52d1316296a9