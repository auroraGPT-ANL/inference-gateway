amqp_port: 443
display_name: llama-cpp-python
engine:
 provider:
  init_blocks: 0
  max_blocks: 2
  min_blocks: 0
  nodes_per_block: 1
  type: PBSProProvider
  launcher:
   type: SimpleLauncher
  account: DataServicePrototype
  cpus_per_node: 64
  select_options: ngpus=4
  queue: 'debug'
  walltime: 00:60:00
  scheduler_options: "#PBS -l filesystems=home:eagle"
  worker_init: 'module use /soft/modulefiles; module load conda; conda activate /home/openinference_svc/envs/llama-env/; python3 -m llama_cpp.server --model /home/openinference_svc/tests/Meta-Llama-3-8B-Instruct-Q8_0.gguf --batch-size 2048 --ubatch-size 512 --ctx-size 2048 --predict -1 --flash-attn --cont-batching --numa distribute --n-gpu-layers 4 --main-gpu 0 --split-mode layer --parallel 4 --log-format json --metrics --threads 32 --threads-batch 16 & sleep 20' #change -m to your model path
 max_workers_per_node: 1
 job_status_kwargs:
  max_idletime: 3000
 address:
  type: address_by_interface
  ifname: bond0
 type: GlobusComputeEngine
 # Limit the functions UUID that can be execute
allowed_functions:
  - 3e3d62cd-5acb-4dd9-a049-667f558e0946