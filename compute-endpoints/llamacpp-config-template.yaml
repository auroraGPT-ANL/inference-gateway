amqp_port: 443
display_name: llama-cpp-python
engine:
 provider:
  init_blocks: 0
  max_blocks: 2
  min_blocks: 0
  nodes_per_block: 1
  type: PBSProProvider
  launcher:
   type: SimpleLauncher
  account: DataServicePrototype
  cpus_per_node: 64
  select_options: ngpus=4
  queue: 'debug'
  walltime: 00:60:00
  scheduler_options: "#PBS -l filesystems=home:eagle"
  worker_init: 'module use /soft/modulefiles; module load conda; conda activate /home/openinference_svc/envs/llama-cpp-cuda-env/; /home/openinference_svc/frameworks/llama.cpp/build/bin/server -m /eagle/argonne_tpc/model_weights/gguf_files/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf -c 2048 -a meta-llama3-70b-instruct -ngl 4 -np 2 -fa & sleep 20' #change -m to your model path
 max_workers_per_node: 1
 job_status_kwargs:
  max_idletime: 3000
 address:
  type: address_by_interface
  ifname: bond0
 type: GlobusComputeEngine