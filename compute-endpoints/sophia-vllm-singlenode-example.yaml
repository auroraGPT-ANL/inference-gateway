amqp_port: 443
display_name: sophia-vllm-llama3.1-8b-instruct-example
engine:
  type: GlobusComputeEngine
  max_retries_on_system_failure: 2
  max_workers_per_node: 100
  job_status_kwargs:
    max_idletime: 86400
  address:
    type: address_by_interface
    ifname: ens10f0
  provider:
    type: PBSProProvider
    launcher:
      type: SimpleLauncher
    account: argonne_tpc
    select_options: ngpus=8
    scheduler_options: '#PBS -l filesystems=home:eagle'
    queue: 'by-node'
    init_blocks: 0
    max_blocks: 2
    min_blocks: 0
    nodes_per_block: 1
    walltime: 24:00:00
    worker_init: |
      # Source and launch using the new modular scripts
      source /home/openinference_svc/launch_vllm_model.sh \
        --model-name meta-llama/Meta-Llama-3.1-8B-Instruct \
        --vllm-version v0.11.0 \
        --tensor-parallel 8 \
        --max-model-len 8192 \
        --enable-chunked-prefill \
        --enable-prefix-caching \
        --trust-remote-code \
        --disable-log-requests \
        --gpu-memory-util 0.95 \
        --framework vllm \
        --cluster sophia

allowed_functions:
  - $VLLM_FUNCTION_UUID

