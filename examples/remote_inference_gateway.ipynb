{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please go to this URL and login:\n",
      "\n",
      "https://auth.globus.org/v2/oauth2/authorize?client_id=99bc7f15-ebed-4d6b-b783-8f7ae8735135&redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code&scope=https%3A%2F%2Fauth.globus.org%2Fscopes%2F681c10cc-f684-4540-bcd7-0b4df3bc26ef%2Faction_all&state=_default&response_type=code&code_challenge=RGIgmvuQM5IyVz46RUUmAyXyptXSRDmpbE9QYFR5JyM&code_challenge_method=S256&access_type=online\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import globus_sdk\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Globus ID and Scope\n",
    "# ===================\n",
    "\n",
    "# Define your Globus thick-client ID\n",
    "# https://app.globus.org/settings/developers\n",
    "# This needs to be created by users (they can also re-use one of the existing clients)\n",
    "my_client_id = \"99bc7f15-ebed-4d6b-b783-8f7ae8735135\"\n",
    "\n",
    "# Define the inference-gateway resource service scope\n",
    "# This will be publicly available to users\n",
    "vllm_client_id = \"681c10cc-f684-4540-bcd7-0b4df3bc26ef\"\n",
    "vllm_scope = f\"https://auth.globus.org/scopes/{vllm_client_id}/action_all\"\n",
    "\n",
    "# Authentication and Access Token\n",
    "# ===============================\n",
    "\n",
    "# Start an Auth client with the vLLM scope\n",
    "auth_client = globus_sdk.NativeAppAuthClient(my_client_id)\n",
    "auth_client.oauth2_start_flow(requested_scopes=vllm_scope)\n",
    "\n",
    "# Authenticate with your Globus account\n",
    "authorize_url = auth_client.oauth2_get_authorize_url()\n",
    "print(f\"Please go to this URL and login:\\n\\n{authorize_url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect access token to vLLM service\n",
    "auth_code = \"knbhTvYQsQ1PdDrydAjixlKOjofC4E\"\n",
    "token_response = auth_client.oauth2_exchange_code_for_tokens(auth_code)\n",
    "access_token = token_response.by_resource_server[vllm_client_id][\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "{'Error Here': '\\n ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n Traceback from attempt: 1\\n Traceback (most recent call last):\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n parsl.executors.errors.BadStateException: Executor GlobusComputeEngine failed due to: Error 1:\\n \\tFailed to start block 0: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n Error 2:\\n \\tFailed to start block 1: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n\\n\\n --------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n Traceback from attempt: 2\\n Traceback (most recent call last):\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 180, in _done_cb\\n     packed = f.result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\\n     return self.__get_result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n parsl.executors.errors.BadStateException: Executor GlobusComputeEngine failed due to: Error 1:\\n \\tFailed to start block 0: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n Error 2:\\n \\tFailed to start block 1: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n\\n\\n During handling of the above exception, another exception occurred:\\n\\n Traceback (most recent call last):\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 222, in submit\\n     future = self._submit(execute_task, task_id, packed_task, self.endpoint_id)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/globus_compute.py\", line 219, in _submit\\n     return self.executor.submit(func, {}, *args, **kwargs)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/parsl/executors/high_throughput/executor.py\", line 661, in submit\\n     raise self.executor_exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 180, in _done_cb\\n     packed = f.result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\\n     return self.__get_result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 222, in submit\\n     future = self._submit(execute_task, task_id, packed_task, self.endpoint_id)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/globus_compute.py\", line 219, in _submit\\n     return self.executor.submit(func, {}, *args, **kwargs)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/parsl/executors/high_throughput/executor.py\", line 661, in submit\\n     raise self.executor_exception\\n parsl.jobs.errors.TooManyJobFailuresError: Error 1:\\n \\tFailed to start block 0: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n Error 2:\\n \\tFailed to start block 1: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n\\n\\n --------------------------------------------------------------------++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n Traceback from attempt: final attempt\\n Traceback (most recent call last):\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 180, in _done_cb\\n     packed = f.result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\\n     return self.__get_result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n parsl.executors.errors.BadStateException: Executor GlobusComputeEngine failed due to: Error 1:\\n \\tFailed to start block 0: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n Error 2:\\n \\tFailed to start block 1: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n\\n\\n During handling of the above exception, another exception occurred:\\n\\n Traceback (most recent call last):\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 222, in submit\\n     future = self._submit(execute_task, task_id, packed_task, self.endpoint_id)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/globus_compute.py\", line 219, in _submit\\n     return self.executor.submit(func, {}, *args, **kwargs)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/parsl/executors/high_throughput/executor.py\", line 661, in submit\\n     raise self.executor_exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 180, in _done_cb\\n     packed = f.result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\\n     return self.__get_result()\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\\n     raise self._exception\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/base.py\", line 222, in submit\\n     future = self._submit(execute_task, task_id, packed_task, self.endpoint_id)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/globus_compute_endpoint/engines/globus_compute.py\", line 219, in _submit\\n     return self.executor.submit(func, {}, *args, **kwargs)\\n   File \"/home/openinference_svc/envs/llama-cpp-cuda-env/lib/python3.10/site-packages/parsl/executors/high_throughput/executor.py\", line 661, in submit\\n     raise self.executor_exception\\n parsl.jobs.errors.TooManyJobFailuresError: Error 1:\\n \\tFailed to start block 0: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n Error 2:\\n \\tFailed to start block 1: Executor GlobusComputeEngine failed to scale due to: Attempt to provision nodes did not return a job ID\\n\\n\\n --------------------------------------------------------------------'}\n"
     ]
    }
   ],
   "source": [
    "# URL to the inference gateway (needs to end with forward slash /)\n",
    "#url = \"https://data-portal-dev-vmw-01.cels.anl.gov/resource_server/polaris/\"\n",
    "url = \"http://localhost:8000/resource_server/polaris/llama-cpp/completions/\"\n",
    "# Add access token to the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Prepare the inference request\n",
    "data = {\n",
    "        \"model\": \"meta-llama-3-8b-instruct\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 150,\n",
    "        \"prompt\": \"List all proteins that interact with RAD51\",\n",
    "        #\"n_probs\":1\n",
    "}\n",
    "\n",
    "# Convert data into Json51633660\n",
    "\n",
    "data_json = json.dumps(data)\n",
    "\n",
    "# Send the post request to the relay server\n",
    "# Verify=False is temporary since I use a self-signed certificate\n",
    "response = requests.post(url, data=data_json, headers=headers, verify=False)\n",
    "\n",
    "# Print inference response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the inference gateway (needs to end with forward slash /)\n",
    "#url = \"https://data-portal-dev-vmw-01.cels.anl.gov/resource_server/polaris/\"\n",
    "url = \"http://localhost:8000/resource_server/polaris/llama-cpp/completions/\"\n",
    "# Add access token to the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Prepare the inference request\n",
    "data = {\n",
    "        \"model\": \"mistral-7B-instruct-v03\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"prompt\": \"List all proteins that interact with RAD51\",\n",
    "        #\"n_probs\":1\n",
    "}\n",
    "\n",
    "# Convert data into Json51633660\n",
    "\n",
    "data_json = json.dumps(data)\n",
    "\n",
    "# Send the post request to the relay server\n",
    "# Verify=False is temporary since I use a self-signed certificate\n",
    "response = requests.post(url, data=data_json, headers=headers, verify=False)\n",
    "\n",
    "# Print inference response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the inference gateway (needs to end with forward slash /)\n",
    "#url = \"https://data-portal-dev-vmw-01.cels.anl.gov/resource_server/polaris/\"\n",
    "url = \"http://localhost:8000/resource_server/polaris/llama-cpp/completions/\"\n",
    "# Add access token to the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Prepare the inference request\n",
    "data = {\n",
    "        \"model\": \"meta-llama-3-70b-instruct\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 150,\n",
    "        \"prompt\": \"List all proteins that interact with RAD51\"    \n",
    "}\n",
    "\n",
    "# Convert data into Json51633660\n",
    "\n",
    "data_json = json.dumps(data)\n",
    "\n",
    "# Send the post request to the relay server\n",
    "# Verify=False is temporary since I use a self-signed certificate\n",
    "response = requests.post(url, data=data_json, headers=headers, verify=False)\n",
    "\n",
    "# Print inference response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10-llama-cpp-env",
   "language": "python",
   "name": "llama-cpp-python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
