import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.ticker import FuncFormatter, LogLocator, ScalarFormatter, LogFormatterMathtext
import os

# ==============================================================================
# Configuration Constants
# ==============================================================================

# --- Plotting Style ---
plt.style.use('seaborn-v0_8-paper')
sns.set_context("paper", font_scale=1.2)

# --- Colors ---
COLORS = {
    'FIRST': '#1f77b4',       # Blue
    'vLLM Direct': '#ff7f0e', # Orange
    'OpenAI API': '#2ca02c',  # Green
    'FIRST (1 Instance)': '#1f77b4', # Blue (for scaling base)
    'FIRST (2 Instances)': '#d62728', # Red
    'FIRST (3 Instances)': '#9467bd', # Purple
    'FIRST (4 Instances)': '#8c564b', # Brown
}

# --- Standard Font Sizes ---
PLOT_TITLE_FS = 18
PLOT_AXIS_LABEL_FS = 16
PLOT_TICK_LABEL_FS = 14
PLOT_LEGEND_FS = 14
PLOT_BAR_LABEL_FS = 13

# --- Model Mappings ---
MODEL_SIZES_MAP = {
    'Llama 3.1-8B': 8,
    'Llama 3.3-70B': 70,
    'GPT-4o-mini': 8,
}

MODEL_CONFIGS_MAP = {
    'Llama 3.1-8B': '4 GPUs, TP=4',
    'Llama 3.3-70B': '8 GPUs, TP=8',
    'GPT-4o-mini': 'Unknown',
}

# --- Metrics ---
METRICS_TO_PLOT_BARS = {
    'request_throughput': 'Request TP (req/s)',
    'output_throughput': 'Output TP (tok/s)',
    'median_e2e_latency_s': 'Median Latency (s)',
    'duration': 'Duration (s)'
}

METRICS_TO_PLOT_LINES = {
    'req_tp': 'Request Throughput (req/s)',
    'out_tp': 'Output Token Throughput (tok/s)',
    'med_lat_s': 'Median E2E Latency (s)',
    'avg_duration': 'Avg. Benchmark Duration (s)'
}

# --- Data ---

# Direct vLLM 70B data points (used if loading from external source fails)
VLLM_70B_REQ_TP_DEFAULT = 5.671305779088315
VLLM_70B_OUT_TP_DEFAULT = 1059.0426272696145
VLLM_70B_MEDIAN_LAT_MS_DEFAULT = 77366.89644446597
VLLM_70B_MEAN_LAT_MS_DEFAULT = 81216.03193316469
VLLM_70B_P99_LAT_MS_DEFAULT = 147133.43299815897

# Benchmark data (vLLM Direct + FIRST + OpenAI + FIRST Scaling)
# Removed 405B and GPT-4o entries
BENCHMARK_DATA = [
    # vLLM Direct - Llama 3.1-8B
    {'model': 'Llama 3.1-8B', 'approach': 'vLLM Direct', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 130628, 'request_throughput': 19.1553, 'output_throughput': 2502.7224, 'mean_e2e_latency_ms': 24236.5392, 'median_e2e_latency_ms': 24365.2347, 'p99_e2e_latency_ms': 43260.5075, 'concurrency': 462.3320, 'duration': 52.3364},
    # vLLM Direct - Llama 3.3-70B
    {'model': 'Llama 3.3-70B', 'approach': 'vLLM Direct', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 186737, 'request_throughput': VLLM_70B_REQ_TP_DEFAULT, 'output_throughput': VLLM_70B_OUT_TP_DEFAULT, 'mean_e2e_latency_ms': VLLM_70B_MEAN_LAT_MS_DEFAULT, 'median_e2e_latency_ms': VLLM_70B_MEDIAN_LAT_MS_DEFAULT, 'p99_e2e_latency_ms': VLLM_70B_P99_LAT_MS_DEFAULT, 'duration': 176.32623578282073},
    # FIRST - Llama 3.1-8B
    {'model': 'Llama 3.1-8B', 'approach': 'FIRST', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 130826, 'request_throughput': 25.0958, 'output_throughput': 3282.8795, 'mean_e2e_latency_ms': 16433.4670, 'median_e2e_latency_ms': 16259.3126, 'p99_e2e_latency_ms': 33693.2355, 'concurrency': 405.3132, 'duration': 40.2809},
    # FIRST - Llama 3.3-70B (Single Instance - Base for Scaling)
    {'model': 'Llama 3.3-70B', 'approach': 'FIRST', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': (172497 + 172811 + 172417) / 3, 'request_throughput': 8.2950, 'output_throughput': 1431.5623, 'mean_e2e_latency_ms': 55522.4392, 'median_e2e_latency_ms': 54504.7951, 'p99_e2e_latency_ms': 112323.0454, 'concurrency': 460.2720, 'duration': 120.6668},
    # FIRST - Llama 3.3-70B (2 Instances)
    {'model': 'Llama 3.3-70B', 'approach': 'FIRST (2 Instances)', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 172668.33333333334, 'request_throughput': 14.573098812301948, 'output_throughput': 2516.340291109982, 'mean_e2e_latency_ms': 30160.809862608478, 'median_e2e_latency_ms': 30141.87261151771, 'p99_e2e_latency_ms': 62105.06021719231, 'concurrency': 438.6690397633557, 'duration': 68.75972741364967},
    # FIRST - Llama 3.3-70B (3 Instances)
    {'model': 'Llama 3.3-70B', 'approach': 'FIRST (3 Instances)', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 172827, 'request_throughput': 20.918326112665973, 'output_throughput': 3615.251547073722, 'mean_e2e_latency_ms': 19470.86226827395, 'median_e2e_latency_ms': 18778.019371908158, 'p99_e2e_latency_ms': 43768.45521016511, 'concurrency': 407.29784662255764, 'duration': 47.80497228191234},
    # FIRST - Llama 3.3-70B (4 Instances)
    {'model': 'Llama 3.3-70B', 'approach': 'FIRST (4 Instances)', 'request_rate': float('inf'), 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 296523, 'total_output_tokens': 173007, 'request_throughput': 23.87841979057548, 'output_throughput': 4131.133772708092, 'mean_e2e_latency_ms': 16169.353013928048, 'median_e2e_latency_ms': 16049.96080591809, 'p99_e2e_latency_ms': 36689.87011950463, 'concurrency': 386.0985990085808, 'duration': 41.878818145021796},
    # OpenAI API - GPT-4o-mini
    {'model': 'GPT-4o-mini', 'approach': 'OpenAI API', 'request_rate': 7.0, 'max_concurrency': None, 'completed': 1000, 'total_input_tokens': 297156, 'total_output_tokens': 180238, 'request_throughput': 6.65452212953161, 'output_throughput': 1199.3977595825183, 'mean_e2e_latency_ms': 2912.5246543023386, 'median_e2e_latency_ms': 1967.2636660106946, 'p99_e2e_latency_ms': 12415.200222365089, 'concurrency': 19.381459764861315, 'duration': 150.27375077200122},
]

RATE_COMPARISON_DATA = [
    {'approach': 'vLLM Direct', 'request_rate': '1', 'req_tp': 0.99, 'out_tp': 180.61, 'med_lat_ms': 2985.40, 'avg_duration': 1010.27},
    {'approach': 'vLLM Direct', 'request_rate': '5', 'req_tp': 4.68, 'out_tp': 849.54, 'med_lat_ms': 5517.47, 'avg_duration': 213.94},
    {'approach': 'vLLM Direct', 'request_rate': '10', 'req_tp': 6.24, 'out_tp': 1135.78, 'med_lat_ms': 36970.39, 'avg_duration': 160.30},
    {'approach': 'vLLM Direct', 'request_rate': '20', 'req_tp': 6.28, 'out_tp': 1143.28, 'med_lat_ms': 63163.51, 'avg_duration': 159.15},
    {'approach': 'vLLM Direct', 'request_rate': 'inf', 'req_tp': 5.78, 'out_tp': 1054.27, 'med_lat_ms': 80171.55, 'avg_duration': 173.40},
    {'approach': 'FIRST', 'request_rate': '1', 'req_tp': 0.98, 'out_tp': 177.89, 'med_lat_ms': 9197.30, 'avg_duration': 1023.12},
    {'approach': 'FIRST', 'request_rate': '5', 'req_tp': 4.50, 'out_tp': 819.81, 'med_lat_ms': 12763.06, 'avg_duration': 222.54},
    {'approach': 'FIRST', 'request_rate': '10', 'req_tp': 6.07, 'out_tp': 1103.70, 'med_lat_ms': 39288.83, 'avg_duration': 164.79},
    {'approach': 'FIRST', 'request_rate': '20', 'req_tp': 7.56, 'out_tp': 1379.22, 'med_lat_ms': 49397.43, 'avg_duration': 138.11},
    {'approach': 'FIRST', 'request_rate': 'inf', 'req_tp': 9.22, 'out_tp': 1677.49, 'med_lat_ms': 46927.70, 'avg_duration': 108.48},
]

# --- Output Files ---
CSV_SUMMARY_FILE = 'benchmark_summary.csv'
LATEX_SUMMARY_FILE = 'benchmark_summary_latex.tex'
PLOT_FIRST_VLLM_FILE = 'first_vs_vllm_comparison.png'
PLOT_FIRST_OPENAI_FILE = 'first_vs_openai_comparison.png'
PLOT_RATE_COMPARISON_FILE = 'rate_comparison_70b.png'
PLOT_SCALING_FILE = 'first_scaling_70b_comparison.png'

# ==============================================================================
# Data Processing Function
# ==============================================================================

def preprocess_data(data):
    """Converts raw benchmark data into a processed Pandas DataFrame."""
    df = pd.DataFrame(data)

    # Convert latency columns to seconds
    latency_cols_ms = ['mean_e2e_latency_ms', 'median_e2e_latency_ms', 'p99_e2e_latency_ms']
    for col_ms in latency_cols_ms:
        col_s = col_ms.replace('_ms', '_s')
        if col_ms in df.columns:
            df[col_s] = df[col_ms] / 1000

    # Add model size and hardware config from maps
    if 'model' in df.columns:
        df['model_size'] = df['model'].map(MODEL_SIZES_MAP)
        df['hardware_config'] = df['model'].map(MODEL_CONFIGS_MAP)

    # Convert specific rate comparison columns
    if 'med_lat_ms' in df.columns:
        df['med_lat_s'] = df['med_lat_ms'] / 1000

    return df

# ==============================================================================
# Plotting Helper Functions
# ==============================================================================

def setup_log_axis(ax):
    """Applies standard log scale formatting to the Y axis."""
    ax.set_yscale('log')
    ax.yaxis.set_major_formatter(LogFormatterMathtext())
    ax.yaxis.set_minor_formatter(LogFormatterMathtext(minor_thresholds=(2, 0.5)))
    ax.grid(True, linestyle='--', alpha=0.6, axis='y')
    ax.margins(y=0.25) # Add margin for bar labels

def add_bar_labels(ax, rects_list, labels_list):
    """Adds rotated labels above bars."""
    label_formats = {'median_e2e_latency_s': '%.2f', 'default': '%.1f'}
    for i, rects in enumerate(rects_list):
        labels = labels_list[i]
        metric_keys = list(METRICS_TO_PLOT_BARS.keys()) # Assumes bar plots use these metrics
        formatted_labels = []
        for idx, metric_key in enumerate(metric_keys):
             fmt = label_formats.get(metric_key, label_formats['default'])
             try:
                 formatted_labels.append(fmt % labels[idx])
             except (TypeError, IndexError):
                 formatted_labels.append("N/A") # Handle cases where data might be missing

        ax.bar_label(rects, labels=formatted_labels, padding=3, rotation=90, size=PLOT_BAR_LABEL_FS)


def plot_comparison_bars(df_plot, metrics_to_plot, approaches, filename, fig_size, title_prefix=""):
    """Generates grouped bar plots comparing different approaches for various models."""
    num_models = df_plot['model_size'].nunique()
    model_sizes = sorted(df_plot['model_size'].unique())

    fig, axes = plt.subplots(1, num_models, figsize=fig_size, sharey=False)
    if num_models == 1:
        axes = [axes] # Make it iterable

    bar_width = 0.35
    metric_keys = list(metrics_to_plot.keys())
    x_indices = np.arange(len(metric_keys))

    for i, model_size in enumerate(model_sizes):
        ax = axes[i]
        model_data = df_plot[(df_plot['model_size'] == model_size) & (df_plot['approach'].isin(approaches))].set_index('approach')

        rects_list = []
        labels_list = []
        approach_labels = []

        # Plot bars for each approach
        for j, approach in enumerate(approaches):
             if approach in model_data.index:
                 approach_data = model_data.loc[approach]
                 values = [approach_data.get(key, np.nan) for key in metric_keys] # Use .get for safety
                 position = x_indices + (j - (len(approaches) - 1) / 2) * bar_width
                 rects = ax.bar(position, values, bar_width, label=approach, color=COLORS.get(approach, '#808080'))
                 rects_list.append(rects)
                 labels_list.append(values)
                 approach_labels.append(approach)
             else:
                 print(f"Warning: Data for approach '{approach}' not found for model size {model_size}.")


        if not rects_list: # Skip if no data was plotted for this model size
             ax.text(0.5, 0.5, 'Data unavailable', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
             ax.set_xticks([])
             ax.set_yticks([])
        else:
            # Add bar labels
            add_bar_labels(ax, rects_list, labels_list)

            # Configure axes
            ax.set_ylabel('Metric Value', fontsize=PLOT_AXIS_LABEL_FS)
            ax.set_title(f'{title_prefix}{model_size}B Model Comparison', fontsize=PLOT_TITLE_FS)
            ax.set_xticks(x_indices)
            ax.set_xticklabels([metrics_to_plot[key] for key in metric_keys], rotation=45, ha="right", fontsize=PLOT_TICK_LABEL_FS)
            ax.tick_params(axis='y', labelsize=PLOT_TICK_LABEL_FS)
            setup_log_axis(ax)

            # Add legend
            handles, labels = ax.get_legend_handles_labels()
            if handles:
                 ax.legend(handles, labels, loc='best', fontsize=PLOT_LEGEND_FS, frameon=True)

    plt.tight_layout(rect=[0, 0.02, 1, 0.95]) # Adjusted bottom margin slightly
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Plot saved as {filename}")


def plot_rate_comparison_lines(df_rate, metrics_to_plot, filename, fig_size):
    """Generates line plots comparing performance across different request rates."""
    rate_order = ['1', '5', '10', '20', 'inf']
    num_metrics = len(metrics_to_plot)
    fig, axes = plt.subplots(1, num_metrics, figsize=fig_size) # Removed sharex=True

    # Define larger font sizes specifically for this plot
    title_fs_rate = PLOT_TITLE_FS + 4
    axis_label_fs_rate = PLOT_AXIS_LABEL_FS + 4
    tick_label_fs_rate = PLOT_TICK_LABEL_FS + 4
    legend_fs_rate = PLOT_LEGEND_FS + 4

    for i, (metric_key, metric_name) in enumerate(metrics_to_plot.items()):
        ax = axes[i]
        sns.lineplot(data=df_rate, x='request_rate', y=metric_key, hue='approach', style='approach',
                     markers=True, dashes=False, ax=ax, palette=COLORS, sort=False)

        ax.set_title(metric_name, fontsize=title_fs_rate)
        ax.set_xlabel('Request Rate (req/s)', fontsize=axis_label_fs_rate)
        ax.set_ylabel(metric_name.split('(')[-1].split(')')[0].capitalize(), fontsize=axis_label_fs_rate) # Extract unit
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(fontsize=legend_fs_rate)
        ax.set_xticks(range(len(rate_order))) # Need to set ticks for each ax
        ax.set_xticklabels(rate_order, fontsize=tick_label_fs_rate)
        ax.tick_params(axis='y', labelsize=tick_label_fs_rate)
        setup_log_axis(ax)

    plt.tight_layout(rect=[0, 0.05, 1, 0.97]) # Increased bottom margin
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Plot saved as {filename}")

def plot_scaling_bars(df_plot, metrics_to_plot, model_to_plot, approaches, filename, fig_size):
    """Generates a grouped bar plot specifically for scaling comparison."""
    scaling_data = df_plot[
        (df_plot['model'] == model_to_plot) &
        (df_plot['approach'].isin(approaches))
    ].set_index('approach')

    # Rename 'FIRST' approach to 'FIRST (1 Instance)' for the legend
    scaling_data = scaling_data.rename(index={'FIRST': 'FIRST (1 Instance)'})
    approaches_renamed = ['FIRST (1 Instance)' if a == 'FIRST' else a for a in approaches]

    fig, ax = plt.subplots(1, 1, figsize=fig_size)

    bar_width_scaling = 0.20 # Specific bar width for scaling plot
    metric_keys = list(metrics_to_plot.keys())
    x_indices = np.arange(len(metric_keys))

    rects_list = []
    labels_list = []
    all_approaches_present = True

    # Plot bars for each scaling approach
    for j, approach in enumerate(approaches_renamed):
         if approach in scaling_data.index:
             approach_data = scaling_data.loc[approach]
             values = [approach_data.get(key, np.nan) for key in metric_keys]
             position = x_indices + (j - (len(approaches_renamed) - 1) / 2) * bar_width_scaling
             rects = ax.bar(position, values, bar_width_scaling, label=approach, color=COLORS.get(approach, '#808080'))
             rects_list.append(rects)
             labels_list.append(values)
         else:
             print(f"Warning: Data for scaling approach '{approach}' not found for model {model_to_plot}.")
             all_approaches_present = False

    if not all_approaches_present or not rects_list:
         ax.text(0.5, 0.5, 'Data unavailable', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
         ax.set_xticks([])
         ax.set_yticks([])
    else:
        # Add bar labels
        add_bar_labels(ax, rects_list, labels_list)

        # Configure axes
        ax.set_ylabel('Metric Value', fontsize=PLOT_AXIS_LABEL_FS)
        ax.set_title(f'FIRST Scaling Performance ({model_to_plot})', fontsize=PLOT_TITLE_FS)
        ax.set_xticks(x_indices)
        ax.set_xticklabels([metrics_to_plot[key] for key in metric_keys], rotation=45, ha="right", fontsize=PLOT_TICK_LABEL_FS)
        ax.tick_params(axis='y', labelsize=PLOT_TICK_LABEL_FS)
        setup_log_axis(ax)

        # Add legend
        handles, labels = ax.get_legend_handles_labels()
        if handles:
             ax.legend(handles, labels, loc='best', fontsize=PLOT_LEGEND_FS, frameon=True)

    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Plot saved as {filename}")


# ==============================================================================
# Table Generation Function
# ==============================================================================

def generate_summary_tables(df_summary, csv_filename, latex_filename):
    """Generates CSV and LaTeX summary tables."""
    # Exclude models/approaches if necessary (already done in data definition)
    summary_df_sorted = df_summary.sort_values(by=['model_size', 'approach'])

    summary_df_filtered = summary_df_sorted[[
        'model', 'approach', 'request_throughput', 'output_throughput',
        'median_e2e_latency_s', 'p99_e2e_latency_ms',
        'duration',
        'hardware_config'
    ]].copy() # Use copy to avoid SettingWithCopyWarning

    # --- CSV Output ---
    summary_df_filtered.to_csv(csv_filename, index=False, float_format='%.2f')
    print(f"CSV summary saved as {csv_filename}")

    # --- LaTeX Output ---
    header_map = {
        'model': 'Model',
        'approach': 'Approach',
        'request_throughput': 'Req TP (req/s)',
        'output_throughput': 'Output TP (tok/s)',
        'median_e2e_latency_s': 'Median Latency (s)',
        'p99_e2e_latency_ms': 'P99 Latency (ms)',
        'duration': 'Duration (s)',
        'hardware_config': 'Hardware Config'
    }
    latex_df = summary_df_filtered.rename(columns=header_map).copy()

    # Select only main approaches for the primary table
    latex_df_main = latex_df[latex_df['Approach'].isin(['FIRST', 'vLLM Direct'])].copy()

    # Format P99 Latency to integer ms
    latex_df_main['P99 Latency (ms)'] = latex_df_main['P99 Latency (ms)'].fillna(-1).astype(int)
    latex_df_main.loc[latex_df_main['P99 Latency (ms)'] == -1, 'P99 Latency (ms)'] = 'N/A'


    # Define column format and generate LaTeX string
    col_formats = 'llrrrrrL'
    try:
        latex_table = latex_df_main.to_latex(
            index=False,
            float_format='%.2f',
            header=True, # Use column names from dataframe
            escape=False,
            column_format=col_formats,
            caption='Benchmark Comparison: FIRST vs. vLLM Direct',
            label='tab:benchmark_summary',
            position='!htbp'
        )

        # Add booktabs package and midrules for grouped headers
        latex_table = latex_table.replace(r'\begin{tabular}', r'\usepackage{booktabs}' + '\n' + r'\begin{tabular}')
        lines = latex_table.splitlines()
        header_line_index = -1
        for idx, line in enumerate(lines):
            if r'\toprule' in line:
                header_line_index = idx + 1
                break

        if header_line_index != -1:
            # Create multi-column headers
            multicolumn_header = r" & & \multicolumn{2}{c}{Throughput} & \multicolumn{2}{c}{Latency} & & \\"
            cmidrule_header = r" \cmidrule(lr){3-4} \cmidrule(lr){5-6}"
            # Insert after the main header line derived from dataframe
            main_header_line = lines[header_line_index]
            lines.insert(header_line_index, multicolumn_header + cmidrule_header)
            # Make the original header span correctly
            # Need to count columns to generate the correct multicolumn spec
            # Example: Model & Approach & Req TP & Out TP & Med Lat & P99 Lat & Dur & HW Config \\
            # Becomes: \multicolumn{1}{l}{Model} & \multicolumn{1}{l}{Approach} & ... & \multicolumn{1}{l}{Hardware Config} \\
            original_headers = [h.strip() for h in main_header_line.split('&')]
            num_cols = len(original_headers)
            new_main_header_parts = []
            # Adjust based on actual headers and col_formats
            if len(col_formats) == num_cols:
                 col_align = list(col_formats)
                 new_main_header_parts = [f"\multicolumn{{1}}{{{align}}}{{{head}}}" for align, head in zip(col_align, original_headers)]
                 lines[header_line_index + 2] = " & ".join(new_main_header_parts) + r" \\" # Replace original header
            else:
                 print("Warning: Column format length mismatch, skipping header modification.")


            latex_table = "\n".join(lines)

        with open(latex_filename, 'w') as f:
            f.write(latex_table)
        print(f"LaTeX summary saved as {latex_filename}")

    except Exception as e:
        print(f"Error generating LaTeX table: {e}")

# ==============================================================================
# Main Execution
# ==============================================================================

def main():
    """Main function to process data, generate plots, and create tables."""
    print("Starting benchmark analysis...")

    # --- Preprocess Data ---
    df_processed = preprocess_data(BENCHMARK_DATA)
    df_rate_processed = preprocess_data(RATE_COMPARISON_DATA)

    # --- Generate Plots ---

    # Plot 1: FIRST vs vLLM Direct Comparison
    plot_comparison_bars(
        df_plot=df_processed,
        metrics_to_plot=METRICS_TO_PLOT_BARS,
        approaches=['vLLM Direct', 'FIRST'],
        filename=PLOT_FIRST_VLLM_FILE,
        fig_size=(12, 7),
        title_prefix=""
    )

    # Plot 2: FIRST vs OpenAI Comparison
    plot_comparison_bars(
        df_plot=df_processed[df_processed['model_size']==8], # Only 8B comparison left
        metrics_to_plot=METRICS_TO_PLOT_BARS,
        approaches=['FIRST', 'OpenAI API'],
        filename=PLOT_FIRST_OPENAI_FILE,
        fig_size=(10, 7), # Adjusted size
        title_prefix="~"
    )


    # Plot 3: Performance vs. Request Rate
    plot_rate_comparison_lines(
        df_rate=df_rate_processed,
        metrics_to_plot=METRICS_TO_PLOT_LINES,
        filename=PLOT_RATE_COMPARISON_FILE,
        fig_size=(24, 9) # Uses specific larger fonts inside function
    )

    # Plot 4: FIRST Scaling Comparison
    plot_scaling_bars(
        df_plot=df_processed,
        metrics_to_plot=METRICS_TO_PLOT_BARS,
        model_to_plot='Llama 3.3-70B',
        approaches=['FIRST', 'FIRST (2 Instances)', 'FIRST (3 Instances)', 'FIRST (4 Instances)'],
        filename=PLOT_SCALING_FILE,
        fig_size=(12, 9) # Adjusted size
    )

    # --- Generate Summary Tables ---
    generate_summary_tables(
        df_summary=df_processed,
        csv_filename=CSV_SUMMARY_FILE,
        latex_filename=LATEX_SUMMARY_FILE
    )

    print("\nBenchmark analysis complete.")

if __name__ == "__main__":
    main()

# --- End Scaling Plot --- 